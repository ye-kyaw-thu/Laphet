{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32187a14-3ffd-4e87-896b-945116ebefbb",
   "metadata": {},
   "source": [
    "# Laphet (Version 0.7) with myPOS Tags (fasttext embedding, no freeze)\n",
    "\n",
    "ဒီ notebook မှာတော့ myPOS ဒေတာရဲ့ tag တွေနဲ့ ဆောက်ထားတဲ့ fasttext embedding ကို သုံးပြီး --embedding_method fasttext_no_freeze ဆိုတဲ့ option နဲ့ training, text generation and testing/evaluation တွေကို လုပ်ပြသွားပါမယ်။   \n",
    "\n",
    "MLP, Bi-LSTM, Transformer, BERT, GPT မော်ဒယ် တစ်ခုချင်းစီအတွက် shell script ရဲ့ task line ကို comment on/off လုပ်ပြီး run ပြသွားပါမယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcd2dc-3f58-496c-a957-fb61288c4772",
   "metadata": {},
   "source": [
    "## Updated Bash Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d7ba11-f85f-46d9-9f93-6c31950c3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.nofz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_nofz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.nofz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_no_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_no_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_no_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_no_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d03f0-4aaf-4678-b162-34ab3d40ba6c",
   "metadata": {},
   "source": [
    "## Training, Text Generation and Testing with MLP LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27892de3-b5d8-442e-8dd0-65bf4b0494e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mlp language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 135.33it/s]\n",
      "Epoch 1, Training Loss: 0.8974\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 519.16it/s]\n",
      "Epoch 1, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 140.34it/s]\n",
      "Epoch 2, Training Loss: 0.5729\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 515.38it/s]\n",
      "Epoch 2, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 140.29it/s]\n",
      "Epoch 3, Training Loss: 0.5729\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 546.96it/s]\n",
      "Epoch 3, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 139.42it/s]\n",
      "Epoch 4, Training Loss: 0.5729\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 528.53it/s]\n",
      "Epoch 4, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 140.85it/s]\n",
      "Epoch 5, Training Loss: 0.5729\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 483.06it/s]\n",
      "Epoch 5, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 137.42it/s]\n",
      "Epoch 6, Training Loss: 0.5729\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 515.58it/s]\n",
      "Epoch 6, Validation Loss: 0.5729\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 138.80it/s]\n",
      "Epoch 7, Training Loss: 0.5729\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 514.10it/s]\n",
      "Epoch 7, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 141.77it/s]\n",
      "Epoch 8, Training Loss: 0.5729\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 535.77it/s]\n",
      "Epoch 8, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.nofz.model with validation loss: 0.5729\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 143.18it/s]\n",
      "Epoch 9, Training Loss: 0.5729\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 522.01it/s]\n",
      "Epoch 9, Validation Loss: 0.5729\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:08<00:00, 141.35it/s]\n",
      "Epoch 10, Training Loss: 0.5729\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 507.19it/s]\n",
      "Epoch 10, Validation Loss: 0.5729\n",
      "\n",
      "real\t1m32.824s\n",
      "user\t1m37.327s\n",
      "sys\t0m2.528s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n adj fw tn part int int v conj punc sb adv num punc v punc [PAD] fw n ppm tn num adv adj adj v [UNK] adv sb pron n [PAD] adj n v [PAD] conj fw abb pron n int pron sb fw [UNK] punc abb part v fw\n",
      "Generated Text 2: n tn [PAD] v ppm fw tn int adj sb num fw num part fw v adj v v punc pron n [PAD] [PAD] [UNK] ppm conj ppm tn pron sb n v fw tn abb punc [UNK] pron pron sb pron sb conj int n [PAD] adj fw adj sb\n",
      "Generated Text 3: n adv conj fw num part int part num adj part int n tn int adv ppm int part v ppm [PAD] int n [PAD] [UNK] ppm int [PAD] conj adj pron adv abb part pron sb abb pron pron n v abb abb num int ppm adj part part fw\n",
      "Generated Text 4: n punc adv part part v punc v ppm abb [PAD] conj abb ppm ppm part part v [UNK] pron abb n adj sb conj fw part pron [PAD] [PAD] [PAD] abb int sb n abb int [UNK] sb int [PAD] [PAD] v conj adj conj v ppm fw ppm sb\n",
      "Generated Text 5: n [UNK] fw adj tn int int int tn [UNK] ppm tn int [PAD] num conj n [UNK] pron punc ppm punc conj punc sb adj punc part sb abb int v pron int num abb punc adv adv fw [UNK] abb conj ppm ppm [UNK] sb pron [UNK] v ppm\n",
      "Generated Text 6: n adv n adv fw conj punc adj num part abb n n pron [PAD] conj conj n punc [PAD] tn sb adv [UNK] [PAD] punc tn [UNK] adv tn punc abb part num n punc v punc int [UNK] punc v punc tn tn [PAD] tn adj n part ppm\n",
      "Generated Text 7: n ppm [PAD] n n fw num part part int fw [UNK] fw int tn abb pron punc num v abb [UNK] [PAD] adv v int [UNK] adj fw tn int ppm sb pron punc fw fw tn adv n conj n adj [PAD] adj punc part fw v [UNK] pron\n",
      "Generated Text 8: n [UNK] fw fw part num conj tn [UNK] tn num part ppm sb [UNK] n abb fw tn tn [PAD] [PAD] fw [UNK] adv pron part pron part punc sb tn pron sb abb fw ppm sb punc tn punc abb fw sb abb int ppm num conj tn [PAD]\n",
      "Generated Text 9: n [UNK] int num [UNK] adj num [PAD] abb [UNK] sb abb fw sb adv num abb tn [PAD] n n sb fw sb pron n num int conj sb [UNK] tn conj adv n [PAD] v punc pron adv tn n adj tn conj conj n v fw tn punc\n",
      "Generated Text 10: n adj sb adj abb abb num abb num fw tn [UNK] conj adj conj ppm abb [UNK] conj tn adv abb pron ppm ppm num int int abb num abb adj tn n ppm n part sb part adv v [PAD] fw conj adj int v [PAD] v abb adj\n",
      "\n",
      "real\t0m2.386s\n",
      "user\t0m4.905s\n",
      "sys\t0m2.402s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/mlp_nofz_gen_texts.txt\n",
      "\n",
      "real\t0m1.875s\n",
      "user\t0m4.482s\n",
      "sys\t0m2.386s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 55.60it/s]\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "real\t0m1.775s\n",
      "user\t0m4.427s\n",
      "sys\t0m2.389s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe8606-0d46-489e-9d29-343e61a35c48",
   "metadata": {},
   "source": [
    "## GPU Usage During MLP LM Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d44291cf-93e6-4c8a-a1e8-3e60e9e1da72",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:24:09 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   58C    P2             154W / 480W |   1050MiB / 24564MiB |     54%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          197MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       55MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    165062      C   python                                      350MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e719bec-410e-4351-bacb-db486a03e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 246K Jan 28 20:24 ./model/tag/mlp.nofz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 20:23 ./model/tag/mlp.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/mlp.nofz.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e38cab9-4047-40ae-8aa8-98e54c3fce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671dafd-ac87-4b4b-a443-83ec0bc5e809",
   "metadata": {},
   "source": [
    "Generated output with above promt file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f50565ef-42c5-43a1-b2e3-5b3e0df2840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron int num\n",
      "pron v adv\n",
      "pron tn [PAD]\n",
      "pron sb sb\n",
      "pron v int\n",
      "n punc v\n",
      "n adj conj\n",
      "n pron [UNK]\n",
      "n int v\n",
      "n sb sb\n",
      "adj num v\n",
      "adj [UNK] adj\n",
      "adj [UNK] fw\n",
      "adj [UNK] conj\n",
      "adj adv [UNK]\n",
      "v ppm num\n",
      "v n ppm\n",
      "v adv v\n",
      "v conj abb\n",
      "v abb tn\n",
      "pron part part v\n",
      "pron part n tn\n",
      "pron part int adv\n",
      "pron part int abb\n",
      "pron part adv sb\n",
      "pron ppm fw v\n",
      "pron ppm part adj\n",
      "pron ppm conj abb\n",
      "pron ppm part pron\n",
      "pron ppm fw ppm\n",
      "n v fw adv\n",
      "n v adv n\n",
      "n v conj conj\n",
      "n v abb v\n",
      "n v num [UNK]\n",
      "n n ppm abb\n",
      "n n n pron\n",
      "n n punc v\n",
      "n n fw v\n",
      "n n v num\n",
      "v part [PAD] fw\n",
      "v part ppm [PAD]\n",
      "v part ppm conj\n",
      "v part pron int\n",
      "v part sb fw\n",
      "n part int abb\n",
      "n part sb abb\n",
      "n part adv abb\n",
      "n part adv v\n",
      "n part ppm sb\n",
      "pron pron part abb\n",
      "pron pron conj ppm\n",
      "pron pron adj conj\n",
      "pron pron tn int\n",
      "pron pron n pron\n",
      "pron ppm num ppm\n",
      "pron ppm [PAD] conj\n",
      "pron ppm [UNK] tn\n",
      "pron ppm part [PAD]\n",
      "pron ppm fw tn\n",
      "n tn n v\n",
      "n tn [UNK] num\n",
      "n tn conj [PAD]\n",
      "n tn [UNK] [PAD]\n",
      "n tn adj conj\n",
      "adj v part tn adv\n",
      "adj v part abb abb\n",
      "adj v part n tn\n",
      "adj v part int abb\n",
      "adj v part punc part\n",
      "n n n int sb\n",
      "n n n tn ppm\n",
      "n n n tn pron\n",
      "n n n [PAD] tn\n",
      "n n n pron adv\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/mlp_nofz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f48b1d-58b2-4908-9c6c-21a26f1b9870",
   "metadata": {},
   "source": [
    "## Training, Text Generation and Testing with Bi-LSTM LM\n",
    "\n",
    "Updated bash shell script is as follows:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c77ca9-203f-4481-be7a-7254a99d03ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.32it/s]\n",
      "Epoch 1, Training Loss: 0.1894\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 176.39it/s]\n",
      "Epoch 1, Validation Loss: 0.0019\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0019\n",
      "Epoch 2/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.67it/s]\n",
      "Epoch 2, Training Loss: 0.0012\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 172.26it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.42it/s]\n",
      "Epoch 3, Training Loss: 0.0002\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.79it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.17it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.26it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.20it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.89it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.30it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 178.79it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.17it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.93it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.nofz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.22it/s]\n",
      "Epoch 8, Training Loss: 0.0003\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 178.19it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.32it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 179.41it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.36it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.29it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "\n",
      "real\t3m35.739s\n",
      "user\t3m39.862s\n",
      "sys\t0m2.928s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: n num sb part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part\n",
      "Generated Text 2: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 3: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 4: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 5: n num sb part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part\n",
      "Generated Text 6: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 7: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 8: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "Generated Text 9: n num sb part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part\n",
      "Generated Text 10: n num sb num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num num\n",
      "\n",
      "real\t0m2.246s\n",
      "user\t0m4.886s\n",
      "sys\t0m2.342s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_nofz_gen_texts.txt\n",
      "\n",
      "real\t0m2.050s\n",
      "user\t0m4.696s\n",
      "sys\t0m2.434s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 42.40it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m2.044s\n",
      "user\t0m4.661s\n",
      "sys\t0m2.446s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d571892-73d2-4cc2-a72d-27dfdc6cf671",
   "metadata": {},
   "source": [
    "## GPU Usage of Bi-LSTM LM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4243b54-3ba2-4adc-bc1f-3181e5fc2dc6",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:28:18 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   64C    P2             349W / 480W |   1929MiB / 24564MiB |     94%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          205MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       47MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      158MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    165529      C   python                                     1232MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7774099-7e18-4022-90f5-de86ea481f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 82M Jan 28 20:30 ./model/tag/bilstm.nofz.model\n",
      "-rw-rw-r-- 1 ye ye 110 Jan 28 20:27 ./model/tag/bilstm.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/bilstm.nofz.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ce8690-a351-4247-a264-55ed30149b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron pron abb\n",
      "pron pron n\n",
      "pron pron pron\n",
      "pron ppm int\n",
      "pron ppm num\n",
      "n num sb\n",
      "n num sb\n",
      "n num sb\n",
      "n num sb\n",
      "n num sb\n",
      "adj num adv\n",
      "adj [UNK] int\n",
      "adj pron num\n",
      "adj pron n\n",
      "adj [PAD] adv\n",
      "v conj part\n",
      "v conj part\n",
      "v conj part\n",
      "v conj part\n",
      "v conj part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron ppm pron num\n",
      "pron ppm num num\n",
      "pron ppm num adj\n",
      "pron ppm num adj\n",
      "pron ppm fw num\n",
      "n v conj part\n",
      "n v conj part\n",
      "n v conj part\n",
      "n v conj part\n",
      "n v conj part\n",
      "n n num sb\n",
      "n n num sb\n",
      "n n num sb\n",
      "n n num sb\n",
      "n n num sb\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "pron pron n num\n",
      "pron pron n adj\n",
      "pron pron ppm ppm\n",
      "pron pron ppm ppm\n",
      "pron pron n num\n",
      "pron ppm fw num\n",
      "pron ppm num num\n",
      "pron ppm num adj\n",
      "pron ppm conj adj\n",
      "pron ppm num num\n",
      "n tn num conj\n",
      "n tn [UNK] n\n",
      "n tn [UNK] abb\n",
      "n tn [UNK] n\n",
      "n tn conj part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "n n n num sb\n",
      "n n n num sb\n",
      "n n n num sb\n",
      "n n n num sb\n",
      "n n n num sb\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/bilstm_nofz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a002c99-3026-4401-92fc-4d5e6c91631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/bilstm.nofz.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31084940-f12a-4dbc-9f5f-04907fcf44ac",
   "metadata": {},
   "source": [
    "## Training, Text Generation and Testing with Transformer LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc2d231-7041-40cc-80aa-9a43450853d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.nofz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_nofz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.nofz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_no_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_no_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_no_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_no_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ddfd42-86e5-4b42-baea-21d5b70eac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 227.35it/s]\n",
      "Epoch 1, Training Loss: 0.2948\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 538.45it/s]\n",
      "Epoch 1, Validation Loss: 0.0063\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0063\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 231.51it/s]\n",
      "Epoch 2, Training Loss: 0.0059\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 638.67it/s]\n",
      "Epoch 2, Validation Loss: 0.0013\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0013\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 237.81it/s]\n",
      "Epoch 3, Training Loss: 0.0016\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 579.99it/s]\n",
      "Epoch 3, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0004\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 238.91it/s]\n",
      "Epoch 4, Training Loss: 0.0006\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 591.74it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 241.33it/s]\n",
      "Epoch 5, Training Loss: 0.0003\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 641.08it/s]\n",
      "Epoch 5, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0001\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 245.07it/s]\n",
      "Epoch 6, Training Loss: 0.0001\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 620.51it/s]\n",
      "Epoch 6, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0001\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 239.85it/s]\n",
      "Epoch 7, Training Loss: 0.0001\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 622.46it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 243.36it/s]\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 605.86it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 236.60it/s]\n",
      "Epoch 9, Training Loss: 0.0001\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 572.34it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 238.73it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 620.72it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.nofz.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m55.847s\n",
      "user\t1m0.454s\n",
      "sys\t0m2.522s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n [UNK] tn sb adv v num ppm punc adv num part [PAD] [PAD] num [PAD] tn [UNK] sb part v [PAD] part adj n sb adj conj adj ppm pron punc part punc conj int int [PAD] n n adv n adj [UNK] num punc adj [PAD] num n v\n",
      "Generated Text 2: n adj fw [UNK] adv int conj adv int adj adv v adj [UNK] adv adj conj int v v part [PAD] int [PAD] num conj fw fw fw punc v part v [PAD] pron adj n num [PAD] punc conj punc num tn tn num punc adv adv pron part\n",
      "Generated Text 3: n conj fw adv fw n sb tn ppm ppm ppm abb ppm int adv adj punc adj v conj part fw fw tn adj [UNK] part adv v sb adv tn part ppm abb abb fw adj adv part pron n punc abb abb conj fw fw conj sb conj\n",
      "Generated Text 4: n [UNK] num ppm n adv part ppm [UNK] num punc conj num adv v conj [UNK] v abb adj pron abb abb num conj v tn punc pron fw [PAD] tn tn n adv punc pron [PAD] fw conj [PAD] [UNK] tn conj fw fw conj punc v punc n\n",
      "Generated Text 5: n adj fw part fw adv punc [UNK] v adv tn punc num adv ppm adj fw [PAD] adv punc adv conj n abb punc int v punc fw tn punc [PAD] adv punc tn n num sb fw int adv sb v [PAD] adj n num fw [PAD] n tn\n",
      "Generated Text 6: n tn punc fw adv int sb [UNK] v abb sb tn [UNK] [PAD] num adj pron v num punc sb ppm adj n abb adj [PAD] num n v tn part adj conj part n num part punc fw abb int n punc [UNK] ppm sb pron ppm [PAD] num\n",
      "Generated Text 7: n v pron n n sb pron tn punc adv v ppm conj [UNK] adv n v abb sb fw conj v punc punc tn abb sb [PAD] tn ppm num ppm int ppm adj punc punc adv tn fw pron abb sb conj punc n v conj adj conj [PAD]\n",
      "Generated Text 8: n sb abb adj v tn [PAD] num n tn abb conj v pron ppm v v conj fw n num adv sb abb num int tn sb conj [UNK] adv n n fw adj conj adv adj v int adv adv pron adv ppm v abb pron adv [UNK] num\n",
      "Generated Text 9: n fw abb sb sb conj fw sb adv [UNK] num punc adv int adv tn abb sb [PAD] n int v tn pron adj [UNK] num sb num part part abb sb num punc int abb num [PAD] num num ppm n sb v adv punc num [PAD] part punc\n",
      "Generated Text 10: n sb pron adj int int fw fw int punc fw adj abb n fw part num ppm part int ppm n sb punc n [PAD] tn fw adv adj n pron part fw [UNK] num sb sb pron pron fw v tn sb pron fw punc tn sb ppm [PAD]\n",
      "\n",
      "real\t0m2.547s\n",
      "user\t0m5.216s\n",
      "sys\t0m2.412s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/transformer_nofz_gen_texts.txt\n",
      "\n",
      "real\t0m2.043s\n",
      "user\t0m4.695s\n",
      "sys\t0m2.328s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 49.50it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.816s\n",
      "user\t0m4.508s\n",
      "sys\t0m2.358s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d5c1f-9ae7-40a1-949e-5b0e1b87ba3e",
   "metadata": {},
   "source": [
    "## GPU Usage of Transformer based LM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "548b3013-5901-4ed2-8dbf-d819b36a6779",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:39:39 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   57C    P2             232W / 480W |   1144MiB / 24564MiB |     58%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          197MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       51MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    166361      C   python                                      448MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d1038-b22d-40fa-b579-aad4c7560f5f",
   "metadata": {},
   "source": [
    "training လုပ်ပြီးထွက်လာတဲ့ model ဖိုင်ရဲ့ filesize ကို လေ့လာရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae20aeb-256f-4d86-930e-64d4ae2e8919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 19:55 ./model/tag/transformer.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 19:54 ./model/tag/transformer.ftfz.model.vocab\n",
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:22 ./model/tag/transformer.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 17:20 ./model/tag/transformer.model.vocab\n",
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:40 ./model/tag/transformer.nofz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 20:39 ./model/tag/transformer.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/transformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f75039f-2e1f-4b17-8a56-ef8cfae5c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron [PAD] n\n",
      "pron n punc\n",
      "pron ppm v\n",
      "pron part adj\n",
      "pron punc ppm\n",
      "n fw [UNK]\n",
      "n ppm v\n",
      "n num fw\n",
      "n n adj\n",
      "n [UNK] tn\n",
      "adj punc n\n",
      "adj [UNK] tn\n",
      "adj [UNK] tn\n",
      "adj v [UNK]\n",
      "adj abb abb\n",
      "v num int\n",
      "v ppm num\n",
      "v adv fw\n",
      "v v sb\n",
      "v punc sb\n",
      "pron part tn n\n",
      "pron part [UNK] num\n",
      "pron part adj part\n",
      "pron part adv num\n",
      "pron part part fw\n",
      "pron ppm [UNK] adj\n",
      "pron ppm num n\n",
      "pron ppm v adv\n",
      "pron ppm ppm fw\n",
      "pron ppm num [UNK]\n",
      "n v int [PAD]\n",
      "n v num pron\n",
      "n v abb fw\n",
      "n v punc ppm\n",
      "n v ppm adv\n",
      "n n ppm punc\n",
      "n n [PAD] num\n",
      "n n adv adj\n",
      "n n v fw\n",
      "n n n tn\n",
      "v part tn adv\n",
      "v part adv n\n",
      "v part tn adv\n",
      "v part adv num\n",
      "v part adv n\n",
      "n part tn pron\n",
      "n part abb [UNK]\n",
      "n part pron tn\n",
      "n part pron part\n",
      "n part pron ppm\n",
      "pron pron adv abb\n",
      "pron pron fw sb\n",
      "pron pron punc conj\n",
      "pron pron ppm sb\n",
      "pron pron sb abb\n",
      "pron ppm [PAD] num\n",
      "pron ppm adj v\n",
      "pron ppm num num\n",
      "pron ppm part [PAD]\n",
      "pron ppm pron adj\n",
      "n tn tn n\n",
      "n tn num tn\n",
      "n tn sb int\n",
      "n tn pron [PAD]\n",
      "n tn tn sb\n",
      "adj v part punc conj\n",
      "adj v part adj tn\n",
      "adj v part part [PAD]\n",
      "adj v part tn int\n",
      "adj v part num fw\n",
      "n n n v int\n",
      "n n n num n\n",
      "n n n ppm fw\n",
      "n n n n tn\n",
      "n n n adv num\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/transformer_nofz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe94de0-4d76-4e2e-a9f6-e126238a027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/transformer.nofz.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58de0f-db8a-4e04-8427-e0423da46ce0",
   "metadata": {},
   "source": [
    "## Training, Text Generation and Testing with BERT LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb7b640-5e3f-49b0-838b-c0b712de3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.nofz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_nofz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.nofz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_no_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_no_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_no_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_no_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d620f0-e861-4492-9f5f-8115ed430808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 223.16it/s]\n",
      "Epoch 1, Training Loss: 0.3094\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 659.63it/s]\n",
      "Epoch 1, Validation Loss: 0.0069\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0069\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 235.57it/s]\n",
      "Epoch 2, Training Loss: 0.0063\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 664.03it/s]\n",
      "Epoch 2, Validation Loss: 0.0013\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0013\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 234.99it/s]\n",
      "Epoch 3, Training Loss: 0.0017\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 584.63it/s]\n",
      "Epoch 3, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0004\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 247.46it/s]\n",
      "Epoch 4, Training Loss: 0.0006\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 683.20it/s]\n",
      "Epoch 4, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0002\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 244.75it/s]\n",
      "Epoch 5, Training Loss: 0.0003\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 650.26it/s]\n",
      "Epoch 5, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0001\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 228.98it/s]\n",
      "Epoch 6, Training Loss: 0.0002\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 619.62it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 232.64it/s]\n",
      "Epoch 7, Training Loss: 0.0001\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 648.98it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 240.44it/s]\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 654.18it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 241.97it/s]\n",
      "Epoch 9, Training Loss: 0.0001\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 671.95it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 234.25it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 623.65it/s]\n",
      "Epoch 10, Validation Loss: 0.0001\n",
      "\n",
      "real\t0m56.185s\n",
      "user\t1m0.755s\n",
      "sys\t0m2.494s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n n sb ppm int int tn punc sb n fw tn abb v n num abb conj conj pron abb ppm conj [UNK] conj abb int fw num num fw adv n adv [PAD] ppm adj abb pron ppm sb pron conj part sb fw int adv sb fw adj\n",
      "Generated Text 2: n fw tn n punc n [UNK] conj abb n [UNK] abb abb part sb part [UNK] punc conj num num punc n tn tn part punc conj fw pron part [UNK] int fw tn [UNK] pron abb part adv pron [UNK] abb int v adv conj tn adv adv part\n",
      "Generated Text 3: n part n tn tn sb v [UNK] conj v tn tn punc fw part [UNK] adv abb v conj adv num conj tn pron adj fw n int int n sb punc tn punc [UNK] conj adj v [UNK] adv n v abb adj pron punc [PAD] [PAD] [PAD] sb\n",
      "Generated Text 4: n punc n n punc n [PAD] tn conj fw adv punc punc adv part ppm tn [UNK] adv tn n [PAD] tn abb conj sb [PAD] int [PAD] n adv adj ppm n fw [UNK] v part v num ppm sb punc sb adv v pron punc adv adv v\n",
      "Generated Text 5: n abb n sb [UNK] num [UNK] conj n sb int [PAD] fw ppm int [UNK] adv n pron [PAD] conj adv [PAD] adv ppm pron punc v n int int int sb punc abb adv part ppm num fw int abb sb [UNK] pron v n int sb part n\n",
      "Generated Text 6: n v part fw abb tn abb abb abb sb ppm num num fw num [PAD] sb int sb sb n v ppm fw fw pron [UNK] conj tn n fw sb tn fw sb pron conj conj sb adj [PAD] tn punc [PAD] adv adj int adv part n fw\n",
      "Generated Text 7: n num part num num adj conj conj adv part adj [PAD] conj adj v num [PAD] adv pron pron pron adv num [UNK] conj fw adj conj abb [UNK] [PAD] tn fw adj n punc [UNK] tn adv adv [UNK] num [PAD] ppm v sb adv adv num fw [UNK]\n",
      "Generated Text 8: n part [PAD] pron abb int adj part pron num [PAD] adj n ppm v [PAD] conj [PAD] fw n tn n num int v adv fw adv num num sb num [PAD] adv abb [UNK] int part adj sb ppm v tn v pron punc [PAD] v tn fw [UNK]\n",
      "Generated Text 9: n punc punc [UNK] num adv pron [UNK] adv num sb sb num num fw int num num punc adv adv num int fw abb [UNK] abb abb [UNK] conj num conj fw part tn n int int sb [PAD] fw ppm ppm abb num part [PAD] adv [UNK] n int\n",
      "Generated Text 10: n int fw v abb [PAD] abb ppm conj [PAD] punc adv adv n v v [UNK] adv fw ppm [UNK] conj v ppm tn n v conj pron punc tn n [PAD] punc adj tn adj num num punc pron [PAD] int fw abb pron n punc int conj sb\n",
      "\n",
      "real\t0m2.547s\n",
      "user\t0m5.211s\n",
      "sys\t0m2.385s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/bert_nofz_gen_texts.txt\n",
      "\n",
      "real\t0m2.051s\n",
      "user\t0m4.703s\n",
      "sys\t0m2.415s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 51.07it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.809s\n",
      "user\t0m4.461s\n",
      "sys\t0m2.318s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47771b3d-dd14-4dfd-9249-3aca78532b41",
   "metadata": {},
   "source": [
    "## GPU Usage of BERT LM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2a6d816-a54d-469d-a4a2-30f639dfca92",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:45:37 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   62C    P2             239W / 480W |   1143MiB / 24564MiB |     57%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          197MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       50MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    167121      C   python                                      448MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:45:46 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   63C    P2             245W / 480W |   1143MiB / 24564MiB |     68%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          197MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       50MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    167121      C   python                                      448MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe7b9ede-55aa-42ae-aa06-18d82c6141a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 19:59 ./model/tag/bert.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 19:58 ./model/tag/bert.ftfz.model.vocab\n",
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:37 ./model/tag/bert.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 17:36 ./model/tag/bert.model.vocab\n",
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:46 ./model/tag/bert.nofz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 20:45 ./model/tag/bert.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/bert*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46542835-5ef3-47a7-b7ef-174f8f81e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron v ppm\n",
      "pron ppm int\n",
      "pron conj pron\n",
      "pron n int\n",
      "pron adj adv\n",
      "n [PAD] num\n",
      "n v [UNK]\n",
      "n adj adj\n",
      "n part tn\n",
      "n adj [PAD]\n",
      "adj conj [PAD]\n",
      "adj num punc\n",
      "adj pron int\n",
      "adj int pron\n",
      "adj num v\n",
      "v conj n\n",
      "v int abb\n",
      "v [PAD] fw\n",
      "v adj ppm\n",
      "v adj part\n",
      "pron part conj n\n",
      "pron part tn int\n",
      "pron part [PAD] sb\n",
      "pron part part int\n",
      "pron part int ppm\n",
      "pron ppm conj adv\n",
      "pron ppm [PAD] ppm\n",
      "pron ppm [PAD] num\n",
      "pron ppm tn [PAD]\n",
      "pron ppm adv tn\n",
      "n v v n\n",
      "n v conj tn\n",
      "n v sb [UNK]\n",
      "n v [PAD] abb\n",
      "n v ppm [PAD]\n",
      "n n n num\n",
      "n n punc part\n",
      "n n abb adv\n",
      "n n pron [PAD]\n",
      "n n [UNK] adj\n",
      "v part v adv\n",
      "v part conj adv\n",
      "v part fw pron\n",
      "v part v v\n",
      "v part punc conj\n",
      "n part v abb\n",
      "n part pron abb\n",
      "n part punc adv\n",
      "n part adj n\n",
      "n part [PAD] tn\n",
      "pron pron pron part\n",
      "pron pron part part\n",
      "pron pron abb sb\n",
      "pron pron [PAD] punc\n",
      "pron pron [PAD] tn\n",
      "pron ppm punc [UNK]\n",
      "pron ppm pron punc\n",
      "pron ppm part adv\n",
      "pron ppm n tn\n",
      "pron ppm num sb\n",
      "n tn [UNK] [PAD]\n",
      "n tn conj v\n",
      "n tn punc fw\n",
      "n tn abb [PAD]\n",
      "n tn sb abb\n",
      "adj v part tn punc\n",
      "adj v part num ppm\n",
      "adj v part part fw\n",
      "adj v part num [PAD]\n",
      "adj v part v [UNK]\n",
      "n n n sb pron\n",
      "n n n part num\n",
      "n n n abb adv\n",
      "n n n pron abb\n",
      "n n n v adv\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/bert_nofz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "602a707c-44eb-4133-a933-5d560a434430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1, Training Loss: 0.3094\n",
      "Epoch 1, Validation Loss: 0.0069\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0069\n",
      "Epoch 2, Training Loss: 0.0063\n",
      "Epoch 2, Validation Loss: 0.0013\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0013\n",
      "Epoch 3, Training Loss: 0.0017\n",
      "Epoch 3, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0004\n",
      "Epoch 4, Training Loss: 0.0006\n",
      "Epoch 4, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0002\n",
      "Epoch 5, Training Loss: 0.0003\n",
      "Epoch 5, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0001\n",
      "Epoch 6, Training Loss: 0.0002\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 7, Training Loss: 0.0001\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 9, Training Loss: 0.0001\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.nofz.model with validation loss: 0.0000\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10, Validation Loss: 0.0001\n",
      "Text generation:\n",
      "Generated Text 1: n n sb ppm int int tn punc sb n fw tn abb v n num abb conj conj pron abb ppm conj [UNK] conj abb int fw num num fw adv n adv [PAD] ppm adj abb pron ppm sb pron conj part sb fw int adv sb fw adj\n",
      "Generated Text 2: n fw tn n punc n [UNK] conj abb n [UNK] abb abb part sb part [UNK] punc conj num num punc n tn tn part punc conj fw pron part [UNK] int fw tn [UNK] pron abb part adv pron [UNK] abb int v adv conj tn adv adv part\n",
      "Generated Text 3: n part n tn tn sb v [UNK] conj v tn tn punc fw part [UNK] adv abb v conj adv num conj tn pron adj fw n int int n sb punc tn punc [UNK] conj adj v [UNK] adv n v abb adj pron punc [PAD] [PAD] [PAD] sb\n",
      "Generated Text 4: n punc n n punc n [PAD] tn conj fw adv punc punc adv part ppm tn [UNK] adv tn n [PAD] tn abb conj sb [PAD] int [PAD] n adv adj ppm n fw [UNK] v part v num ppm sb punc sb adv v pron punc adv adv v\n",
      "Generated Text 5: n abb n sb [UNK] num [UNK] conj n sb int [PAD] fw ppm int [UNK] adv n pron [PAD] conj adv [PAD] adv ppm pron punc v n int int int sb punc abb adv part ppm num fw int abb sb [UNK] pron v n int sb part n\n",
      "Generated Text 6: n v part fw abb tn abb abb abb sb ppm num num fw num [PAD] sb int sb sb n v ppm fw fw pron [UNK] conj tn n fw sb tn fw sb pron conj conj sb adj [PAD] tn punc [PAD] adv adj int adv part n fw\n",
      "Generated Text 7: n num part num num adj conj conj adv part adj [PAD] conj adj v num [PAD] adv pron pron pron adv num [UNK] conj fw adj conj abb [UNK] [PAD] tn fw adj n punc [UNK] tn adv adv [UNK] num [PAD] ppm v sb adv adv num fw [UNK]\n",
      "Generated Text 8: n part [PAD] pron abb int adj part pron num [PAD] adj n ppm v [PAD] conj [PAD] fw n tn n num int v adv fw adv num num sb num [PAD] adv abb [UNK] int part adj sb ppm v tn v pron punc [PAD] v tn fw [UNK]\n",
      "Generated Text 9: n punc punc [UNK] num adv pron [UNK] adv num sb sb num num fw int num num punc adv adv num int fw abb [UNK] abb abb [UNK] conj num conj fw part tn n int int sb [PAD] fw ppm ppm abb num part [PAD] adv [UNK] n int\n",
      "Generated Text 10: n int fw v abb [PAD] abb ppm conj [PAD] punc adv adv n v v [UNK] adv fw ppm [UNK] conj v ppm tn n v conj pron punc tn n [PAD] punc adj tn adj num num punc pron [PAD] int fw abb pron n punc int conj sb\n",
      "Batch text generation from file:\n",
      "Generated texts saved to ./output/tag/bert_nofz_gen_texts.txt\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 51.07it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!cat ./log/tag/bert.nofz.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8689f-228b-4bc4-a46c-65a3ae05e677",
   "metadata": {},
   "source": [
    "## Training, Text Generation and Testing with GPT LM\n",
    "\n",
    "Updated bash shell script is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f39bcfd-d2fd-41e7-a44a-a97a424ecc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.nofz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_nofz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.nofz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_no_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_no_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_no_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_no_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d8b404f-a4aa-449c-8e69-2c184ecc0b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gpt language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 244.47it/s]\n",
      "Epoch 1, Training Loss: 0.1026\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 799.18it/s]\n",
      "Epoch 1, Validation Loss: 0.0018\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0018\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 270.74it/s]\n",
      "Epoch 2, Training Loss: 0.0013\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 803.94it/s]\n",
      "Epoch 2, Validation Loss: 0.0005\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0005\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 272.36it/s]\n",
      "Epoch 3, Training Loss: 0.0004\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 774.58it/s]\n",
      "Epoch 3, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0002\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 283.35it/s]\n",
      "Epoch 4, Training Loss: 0.0002\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 825.48it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 289.69it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 830.49it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 280.69it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 791.48it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 281.77it/s]\n",
      "Epoch 7, Training Loss: 0.0001\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 782.71it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 274.95it/s]\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 752.32it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 286.69it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 795.61it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:04<00:00, 288.34it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 819.31it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.nofz.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m48.233s\n",
      "user\t0m52.940s\n",
      "sys\t0m2.521s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n sb n conj part int pron punc ppm pron pron fw ppm [PAD] punc int [PAD] adj fw [UNK] n num fw abb [PAD] tn adv int conj adj fw [PAD] [UNK] int punc adj conj [UNK] fw adv v adj n [UNK] [PAD] adv part [UNK] sb [UNK] n\n",
      "Generated Text 2: n v part n [PAD] v adv punc part [PAD] num conj adv adv ppm sb conj punc n n int ppm fw pron adv adv abb pron [PAD] pron part [PAD] num sb int ppm pron part sb tn abb [UNK] ppm tn fw adj int [UNK] v ppm punc\n",
      "Generated Text 3: n fw [PAD] part sb [UNK] n ppm sb conj n [PAD] fw adj tn part fw adv int part sb int num tn conj [PAD] [UNK] sb int abb v part conj punc sb fw v int int v conj num pron ppm pron abb v sb adv int int\n",
      "Generated Text 4: n n n adv ppm conj n v int tn ppm num fw tn adj abb n sb [UNK] abb [PAD] [UNK] num adv adj num [UNK] [PAD] ppm int adv tn tn conj conj num fw v pron fw [UNK] adv adj ppm part sb part part num num n\n",
      "Generated Text 5: n tn adj v adv adj adj [PAD] ppm [PAD] [UNK] [PAD] int ppm int num int punc abb [PAD] punc num ppm sb punc conj [PAD] punc adj [UNK] sb pron tn v int abb adj fw abb ppm adj tn sb adv n punc [PAD] sb num num [UNK]\n",
      "Generated Text 6: n conj punc pron sb sb abb v fw pron fw adj conj num ppm int pron [UNK] pron v punc sb v abb [PAD] abb int v fw int sb adv punc fw pron n int [UNK] [UNK] n ppm punc ppm sb part pron adv pron pron conj [PAD]\n",
      "Generated Text 7: n tn [UNK] abb punc adj abb part v adv sb pron [UNK] punc abb fw [UNK] v n tn n int ppm n part v sb adj adj part sb ppm n sb [UNK] [UNK] part pron adj adv fw conj pron fw v adj part ppm int int v\n",
      "Generated Text 8: n conj pron tn fw tn punc adv ppm num tn pron ppm fw fw abb [PAD] tn fw adv n sb pron [PAD] int ppm adj int pron num sb part punc int conj adv [UNK] fw sb [UNK] int [PAD] abb sb num num adv num int pron abb\n",
      "Generated Text 9: n fw sb num part [UNK] n tn conj punc [PAD] [PAD] [UNK] num v n adv adj fw tn punc sb sb int [UNK] pron sb tn adv n sb part adv num adj adv num fw v conj [UNK] sb tn tn num pron tn [UNK] n [PAD] tn\n",
      "Generated Text 10: n abb sb conj conj adj part adv v ppm num conj [PAD] fw num fw fw pron tn pron [UNK] adj [PAD] [UNK] adj abb punc v part ppm [UNK] part v pron n adv adv [PAD] n abb fw [UNK] int num v adv [PAD] tn tn num adj\n",
      "\n",
      "real\t0m2.460s\n",
      "user\t0m5.081s\n",
      "sys\t0m2.424s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/gpt_nofz_gen_texts.txt\n",
      "\n",
      "real\t0m2.059s\n",
      "user\t0m4.726s\n",
      "sys\t0m2.399s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 54.06it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.786s\n",
      "user\t0m4.430s\n",
      "sys\t0m2.355s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_nofz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5f344-e1b8-4ec5-94b4-7676b231cf17",
   "metadata": {},
   "source": [
    "## GPU Usage of GPT based LM Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d06d522d-fa3c-42ed-af1f-1fb705a93c85",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:49:44 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   52C    P2             241W / 480W |   1119MiB / 24564MiB |     63%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          197MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       106MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       51MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    167598      C   python                                      422MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2dfd1-de9f-4d02-a5f3-3f77055d3448",
   "metadata": {},
   "source": [
    "trained လုပ်ပြီးထွက်လာတဲ့ model ဖိုင်ကိုလေ့လာကြည့်ရအောင်"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e18b3fbf-a475-46ff-be7d-e7c184d3aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:50 ./model/tag/gpt.nofz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 20:49 ./model/tag/gpt.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/gpt.nofz.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4337206b-4640-45c4-b6f1-5a4bfd9cd691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:02 ./model/tag/gpt.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:40 ./model/tag/gpt.model\n",
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:50 ./model/tag/gpt.nofz.model\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/gpt.*model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ba254-0800-4cb1-8b89-f13d5de4706b",
   "metadata": {},
   "source": [
    "အထက်ပါအတိုင်း fasttext embedding ကို သုံးပြီးဆောက်ခဲ့တဲ့ GPT based LM မော်ဒယ်နှစ်ခုက nn.Embedding နည်းနဲ့ ဆောက်ခဲ့တဲ့ မော်ဒယ်ထက် filesize သေးတယ် ဆိုတာကို တွေ့ရလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7103607-6710-45d8-a30c-c34555a93436",
   "metadata": {},
   "source": [
    "GPT based language model ရဲ့ tag generation ကိုလည်း လေ့လာကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bcb5bd5-9266-414a-a550-ffff460fca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron punc fw\n",
      "pron [PAD] pron\n",
      "pron adv [UNK]\n",
      "pron [PAD] v\n",
      "pron int tn\n",
      "n pron punc\n",
      "n n pron\n",
      "n int adj\n",
      "n [UNK] v\n",
      "n pron int\n",
      "adj sb v\n",
      "adj part abb\n",
      "adj fw adv\n",
      "adj part adv\n",
      "adj abb punc\n",
      "v fw fw\n",
      "v [UNK] tn\n",
      "v ppm ppm\n",
      "v abb tn\n",
      "v pron sb\n",
      "pron part num [PAD]\n",
      "pron part punc pron\n",
      "pron part int part\n",
      "pron part tn sb\n",
      "pron part pron ppm\n",
      "pron ppm v adj\n",
      "pron ppm ppm sb\n",
      "pron ppm punc conj\n",
      "pron ppm [PAD] abb\n",
      "pron ppm part sb\n",
      "n v conj fw\n",
      "n v [PAD] [PAD]\n",
      "n v [PAD] num\n",
      "n v n conj\n",
      "n v tn [PAD]\n",
      "n n adj abb\n",
      "n n adj tn\n",
      "n n punc int\n",
      "n n num n\n",
      "n n int part\n",
      "v part num sb\n",
      "v part adj ppm\n",
      "v part int v\n",
      "v part adj pron\n",
      "v part part [PAD]\n",
      "n part ppm [PAD]\n",
      "n part pron adj\n",
      "n part adj [UNK]\n",
      "n part fw fw\n",
      "n part punc adj\n",
      "pron pron n abb\n",
      "pron pron sb adj\n",
      "pron pron ppm v\n",
      "pron pron sb int\n",
      "pron pron abb sb\n",
      "pron ppm pron abb\n",
      "pron ppm [PAD] tn\n",
      "pron ppm [UNK] punc\n",
      "pron ppm [PAD] tn\n",
      "pron ppm v [UNK]\n",
      "n tn punc n\n",
      "n tn tn pron\n",
      "n tn sb sb\n",
      "n tn fw [PAD]\n",
      "n tn [PAD] adv\n",
      "adj v part part fw\n",
      "adj v part adj sb\n",
      "adj v part [PAD] part\n",
      "adj v part adv tn\n",
      "adj v part part abb\n",
      "n n n int conj\n",
      "n n n tn part\n",
      "n n n fw int\n",
      "n n n v v\n",
      "n n n v [UNK]\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/gpt_nofz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb28e0-963d-4990-97c2-11b2464c5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
