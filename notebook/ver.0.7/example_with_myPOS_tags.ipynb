{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c0062c-cc95-4aea-8ab2-5af5d6be5c76",
   "metadata": {},
   "source": [
    "# Laphet (Version 0.7) with myPOS Tags\n",
    "\n",
    "Language model ကို NLP task အမျိုးမျိုးအတွက် အသုံးပြုကြပါတယ်။ ဒီ notebook မှာတော့ Laphet LM Toolkit ကို သုံးပြီး မြန်မာစာကြောင်းတွေရဲ့ Part-of-Speech prediction/generation ကို လက်တွေ့ လုပ်ပြပါမယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d8e0a-1a65-4eec-a55d-f79b43042e1b",
   "metadata": {},
   "source": [
    "Version 0.7 မှာက embedding ကို fasttext နဲ့ပါ သုံးအောင် update လုပ်ထားတာပါ။ အဲဒါကြောင့် --embedding_method နဲ့ --fasttext_model ဆိုတဲ့ option တွေက အသစ်တွေပါ။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "675ac2bc-8a2b-4453-991f-d7a3fe4efda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: laphet.py [-h] --model_type {mlp,bilstm,transformer,bert,gpt} [--train]\n",
      "                 [--generate] [--test] [--data DATA] [--model MODEL]\n",
      "                 [--vocab VOCAB] [--dev_file DEV_FILE] [--test_file TEST_FILE]\n",
      "                 [--prompt PROMPT] [--input INPUT] [--seq_len SEQ_LEN]\n",
      "                 [--output OUTPUT] [--no_of_generation NO_OF_GENERATION]\n",
      "                 [--epochs EPOCHS] [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                 [--embed_dim EMBED_DIM] [--num_heads NUM_HEADS]\n",
      "                 [--num_layers NUM_LAYERS] [--hidden_dim HIDDEN_DIM]\n",
      "                 [--ff_dim FF_DIM] [--dropout DROPOUT]\n",
      "                 [--temperature TEMPERATURE] [--top_k TOP_K] [--top_p TOP_P]\n",
      "                 [--embedding_method {nn.Embedding,fasttext_freeze,fasttext_no_freeze}]\n",
      "                 [--fasttext_model FASTTEXT_MODEL]\n",
      "\n",
      "Laphet language model toolkit for Burmese.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_type {mlp,bilstm,transformer,bert,gpt}\n",
      "                        Type of model to use: mlp, bilstm, transformer, bert\n",
      "                        or gpt.\n",
      "  --train               Train the model.\n",
      "  --generate            Generate text using the trained model.\n",
      "  --test                Test the BERT model and evaluate perplexity.\n",
      "  --data DATA           Path to the dataset.\n",
      "  --model MODEL         Path to save/load the model.\n",
      "  --vocab VOCAB         Path to save/load the tokenizer vocabulary\n",
      "  --dev_file DEV_FILE   Path to the development or validation dataset.\n",
      "  --test_file TEST_FILE\n",
      "                        Path to the test dataset.\n",
      "  --prompt PROMPT       Prompt for text generation (default: None).\n",
      "  --input INPUT         File with starting words for line-by-line generation\n",
      "                        (default: None).\n",
      "  --seq_len SEQ_LEN     Sequence length (default: 30).\n",
      "  --output OUTPUT       File to save the generated text (default: None).\n",
      "  --no_of_generation NO_OF_GENERATION\n",
      "                        Number of sequences to generate (default: 1).\n",
      "  --epochs EPOCHS       Number of training epochs (default: 10).\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size (default: 32).\n",
      "  --lr LR               Learning rate (default: 0.0001).\n",
      "  --embed_dim EMBED_DIM\n",
      "                        Embedding dimension (default: 256).\n",
      "  --num_heads NUM_HEADS\n",
      "                        Number of attention heads (default: 4).\n",
      "  --num_layers NUM_LAYERS\n",
      "                        Number of layers (default: 4).\n",
      "  --hidden_dim HIDDEN_DIM\n",
      "                        Hidden dimension for LSTM (default: 512).\n",
      "  --ff_dim FF_DIM       Feedforward dimension for Transformer (default: 512).\n",
      "  --dropout DROPOUT     Dropout rate for LSTM (default: 0.5).\n",
      "  --temperature TEMPERATURE\n",
      "                        Sampling temperature (default: 1.0).\n",
      "  --top_k TOP_K         Top-k sampling (default: 10).\n",
      "  --top_p TOP_P         Top-p sampling (default: 0.9).\n",
      "  --embedding_method {nn.Embedding,fasttext_freeze,fasttext_no_freeze}\n",
      "                        Embedding method to use: nn.Embedding,\n",
      "                        fasttext_freeze, or fasttext_no_freeze.\n",
      "  --fasttext_model FASTTEXT_MODEL\n",
      "                        Path to the pretrained FastText model (required if\n",
      "                        embedding_method is fasttext_freeze or\n",
      "                        fasttext_no_freeze).\n"
     ]
    }
   ],
   "source": [
    "!python ./laphet.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a99ae-e036-4178-9527-c22a00f008d9",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "Link: [https://github.com/ye-kyaw-thu/myPOS](https://github.com/ye-kyaw-thu/myPOS)  \n",
    "\n",
    "Preprocessing အနေနဲ့ myPOS dataset ထဲက tag တွေကိုပဲ ဆွဲထုတ်ယူထားပါတယ်။  \n",
    "tag တွေ သို့မဟုတ် word တွေကို ဆွဲထုတ်ဖို့အတွက် သုံးခဲ့တဲ့ perl code က ဒီလင့်ကနေ ရယူနိုင်ပါတယ်။  \n",
    "\n",
    "[https://github.com/ye-kyaw-thu/myPOS/blob/master/corpus-draft-ver-1.0/mk-wordtag.pl](https://github.com/ye-kyaw-thu/myPOS/blob/master/corpus-draft-ver-1.0/mk-wordtag.pl)  \n",
    "\n",
    "Laphet repository မှာလည်း ကော်ပီကူးထည့်ပေးထားပါမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9ac6096-31c8-44f4-9359-03e5317f39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 103K Jan 26 18:03 ./data/myPOS/tag/dev_tag.txt\n",
      "-rw-rw-r-- 1 ye ye   95 Jan 24 18:42 ./data/myPOS/tag/start_tags.txt\n",
      "-rw-rw-r-- 1 ye ye  48K Jan 24 17:43 ./data/myPOS/tag/test_tag.txt\n",
      "-rw-rw-r-- 1 ye ye 1.9M Jan 26 18:02 ./data/myPOS/tag/train_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./data/myPOS/tag/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0a79b8e-9208-4aac-a656-fd0ffd9f6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40000  522381 1908736 ./data/myPOS/tag/train_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/train_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67119a87-addf-4893-a706-9a7977587db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2196  28668 104680 ./data/myPOS/tag/dev_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/dev_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "def2eb0c-f680-4962-8d10-e0aa0f19b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 13468 48966 ./data/myPOS/tag/test_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/test_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f0e4303-dbe9-44c7-82fc-c818f663d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 28 95 ./data/myPOS/tag/start_tags.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22463-547e-4fd7-bb67-cbd960d6dd2c",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca670b8c-1453-40cc-a4e4-dc8de7b60a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num n v n ppm n num part v ppm punc\n",
      "n part ppm v v part v part part n v n conj punc n conj n n ppm v part part n n part v part n conj n n v ppm punc\n",
      "adj n ppm v part n part v ppm punc\n",
      "n v part ppm v v part part n n part ppm v n part ppm n ppm adv v part v part part part punc\n",
      "pron ppm pron n part punc\n",
      "num n n n n punc num part punc n n punc n n punc\n",
      "pron n tn part v part part ppm punc\n",
      "pron part ppm pron n ppm n ppm v part v part ppm punc\n",
      "n v part adj n ppm pron part part punc\n",
      "adj n ppm n punc n punc n part n part v ppm punc\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myPOS/tag/train_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cad874dd-3a10-4123-a4a3-503bd9d7eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron tn part part v ppm punc\n",
      "n n n n part ppm adv v part part ppm punc\n",
      "conj n part v conj adv v part part punc\n",
      "pron pron v part n v part n v part part punc\n",
      "n v part ppm n v ppm punc\n",
      "v part ppm punc\n",
      "v v part v part punc\n",
      "pron ppm n n ppm v part part part part punc\n",
      "pron pron part ppm n tn part part ppm n v part v part punc\n",
      "pron part ppm adj n part punc\n"
     ]
    }
   ],
   "source": [
    "!tail ./data/myPOS/tag/dev_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32d93d71-71b1-48cb-8212-ba61e7d0e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn n ppm n tn part punc\n",
      "n ppm pron pron ppm v part ppm punc\n",
      "pron n v v part punc\n",
      "n v part v ppm part punc\n",
      "n adv v part punc tn tn n part v part conj v part ppm part punc\n",
      "v part conj n ppm v part v ppm punc\n",
      "n ppm v ppm punc\n",
      "n v conj n v part part part punc\n",
      "pron v part ppm tn part part v part ppm punc\n",
      "n part ppm v conj n part ppm n n ppm n v ppm punc\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myPOS/tag/test_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3507386b-d237-4d92-826e-5f151d8ab459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4133f-0f4e-45eb-84c1-278357af1824",
   "metadata": {},
   "source": [
    "start_tags.txt ဖိုင်မှာ start word (တကယ်ကတော့ ဒီနေရာမှာ start tag ပါ) အနေနဲ့ ပေးတဲ့အခါမှာ တစ်လုံးထက်မက ပိုပေးထားတာကို ဂရုပြုပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dc5d2-0a99-4b13-aa20-cfc22e0b1218",
   "metadata": {},
   "source": [
    "## Using --nn.Embedding\n",
    "\n",
    "ဒီ notebook မှာက အရင် version 0.6 မှာလိုပဲ simple embedding ဖြစ်တဲ့ nn.Embedding နဲ့ပဲ သွားပါမယ်။   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05150a-b986-40de-9215-0f0e49bbfed4",
   "metadata": {},
   "source": [
    "## Bash Shell Script\n",
    "\n",
    "Experiment အကုန်၊ ဆိုလိုတာက MLP, Bi-LSTM, Transformer, BERT, GPT မော်ဒယ်အကုန်နဲ့ experiment လုပ်မယ် ဆိုရင်တော့ အောက်ပါ shel script နဲ့ run ပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f90235c-fadc-4b1c-9b55-3196c258f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;      \n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 \\ \n",
      "      --embedding_method nn.Embedding --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "task bilstm\n",
      "task transformer\n",
      "task bert\n",
      "task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a38641-d5bd-4d4f-bb5d-b17651ad0995",
   "metadata": {},
   "source": [
    "ဒီ notebook မှာ အကုန်လုံးကို တန်းစီပြီး တခါတည်း run ချလိုက်ရင် လိုက်ကြည့်ရခက်နေမှာစိုးလို့ မော်ဒယ် တစ်ခုချင်းစီကို သပ်သပ်စီ run ပြသွားပါမယ်။ အဲဒီလို လုပ်ဖို့အတွက် ဆိုရင် task line တွေကို comment အပိတ်အဖွင့်လုပ်ပြီး ကစားပေးရပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391264e6-403d-4d4c-bf67-717f9162c783",
   "metadata": {},
   "source": [
    "## Bash Script for MLP  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd3e367c-8c74-4454-b4c3-de7c544e8aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;      \n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 \\ \n",
      "      --embedding_method nn.Embedding --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a884e-014a-4bc4-bf79-37b784bbf813",
   "metadata": {},
   "source": [
    "## MLP based LM Training, Text Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4899e390-58c3-428d-876e-a7b37fa12b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mlp language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 136.94it/s]\n",
      "Epoch 1, Training Loss: 0.6107\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 507.72it/s]\n",
      "Epoch 1, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 139.86it/s]\n",
      "Epoch 2, Training Loss: 0.5729\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 521.82it/s]\n",
      "Epoch 2, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 138.85it/s]\n",
      "Epoch 3, Training Loss: 0.5729\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 482.32it/s]\n",
      "Epoch 3, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 140.91it/s]\n",
      "Epoch 4, Training Loss: 0.5729\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 536.71it/s]\n",
      "Epoch 4, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 143.73it/s]\n",
      "Epoch 5, Training Loss: 0.5729\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 536.78it/s]\n",
      "Epoch 5, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 144.44it/s]\n",
      "Epoch 6, Training Loss: 0.5729\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 536.65it/s]\n",
      "Epoch 6, Validation Loss: 0.5729\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 143.91it/s]\n",
      "Epoch 7, Training Loss: 0.5729\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 529.35it/s]\n",
      "Epoch 7, Validation Loss: 0.5729\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 144.07it/s]\n",
      "Epoch 8, Training Loss: 0.5729\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 503.70it/s]\n",
      "Epoch 8, Validation Loss: 0.5729\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 139.51it/s]\n",
      "Epoch 9, Training Loss: 0.5729\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 469.70it/s]\n",
      "Epoch 9, Validation Loss: 0.5729\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:08<00:00, 142.70it/s]\n",
      "Epoch 10, Training Loss: 0.5729\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 491.43it/s]\n",
      "Epoch 10, Validation Loss: 0.5729\n",
      "\n",
      "real\t1m31.639s\n",
      "user\t1m36.463s\n",
      "sys\t0m2.321s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n part int punc adj part [UNK] fw part [PAD] fw abb conj int [UNK] adv conj abb n conj [PAD] ppm ppm num n tn v adj [PAD] sb ppm tn part punc [UNK] int punc [UNK] abb ppm punc abb adj int int v adv [UNK] adv [PAD] [UNK]\n",
      "Generated Text 2: n n n sb punc n [UNK] part [UNK] abb v [UNK] tn adj adj v pron int adj num fw [PAD] [PAD] tn n [UNK] [UNK] sb ppm sb v adv adj v n part n sb v [UNK] conj adj sb pron part num num fw part punc pron\n",
      "Generated Text 3: n sb abb adj part sb ppm int pron tn adj part conj abb fw punc int [UNK] abb adj ppm pron adv tn adv v int n adj part tn punc adv fw v sb ppm tn sb part v abb ppm adj tn adv adj v punc adj adj\n",
      "Generated Text 4: n ppm part v int num punc fw conj fw ppm adj adj adj sb v tn adj adj pron adv ppm pron tn [PAD] num n num sb pron pron [UNK] fw part punc v pron fw abb v [UNK] sb [PAD] sb pron part ppm adv adj conj ppm\n",
      "Generated Text 5: n conj int ppm ppm v fw conj int adj part num punc pron [UNK] [UNK] ppm pron num v [UNK] adv pron pron sb ppm conj conj [PAD] ppm adj int conj sb pron int sb part [PAD] tn abb adj pron part v part conj pron num int punc\n",
      "Generated Text 6: n [UNK] [PAD] adv num pron sb adv sb [PAD] part fw conj v pron adj ppm v int conj part n ppm fw num [UNK] v [UNK] sb ppm v conj [UNK] part fw int punc ppm [UNK] conj abb n abb [PAD] sb n adv adv punc tn pron\n",
      "Generated Text 7: n fw fw adj num pron [UNK] fw pron [PAD] pron num [PAD] [UNK] ppm abb ppm pron adv n fw adj v punc [PAD] punc v fw punc adj v punc pron conj abb part abb part [PAD] ppm pron ppm fw [UNK] v punc [PAD] conj [UNK] v int\n",
      "Generated Text 8: n adv n part adv conj adv fw adj num adj [PAD] adv n sb pron conj sb part num int part part num adv conj pron v sb [UNK] int conj part ppm int [UNK] num adj punc [UNK] part tn adv [PAD] num int int conj abb v n\n",
      "Generated Text 9: n abb abb part conj v conj fw fw part [PAD] sb conj num conj punc pron num adv conj [PAD] v fw adv tn ppm adj sb abb fw adj sb num v pron num conj tn conj conj [UNK] ppm punc sb sb [UNK] [PAD] punc [PAD] punc [UNK]\n",
      "Generated Text 10: n adj int int punc pron int punc abb abb num sb ppm pron conj adv adj int sb adv abb num pron num tn int pron sb ppm fw ppm ppm sb int v n conj adv conj sb [UNK] abb n v int ppm fw num adv part [PAD]\n",
      "\n",
      "real\t0m2.145s\n",
      "user\t0m5.028s\n",
      "sys\t0m2.204s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/mlp_gen_texts.txt\n",
      "\n",
      "real\t0m1.657s\n",
      "user\t0m4.500s\n",
      "sys\t0m2.218s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 53.35it/s]\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "real\t0m1.558s\n",
      "user\t0m4.381s\n",
      "sys\t0m2.109s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669b284-9730-4efe-b029-f1bad43fe83d",
   "metadata": {},
   "source": [
    "## GPU Usage for MLP-based Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c52eeb0-5f6c-4e2d-b0b0-4f4a9cb57dad",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 14:53:04 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   56C    P2             168W / 480W |   1019MiB / 24564MiB |     56%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       33MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      163MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                21MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            23MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    145141      C   python                                      370MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b8420-4148-4811-af8f-69ebc2bb0a81",
   "metadata": {},
   "source": [
    "## Checking Model, Output and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6768e6f0-0c95-4ea9-85b8-dad484e3e2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── mlp.model\n",
      "└── mlp.model.vocab\n",
      "\n",
      "1 directory, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a916e89-8547-4111-a936-f75796dfe2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 568K Jan 28 14:57 ./model/tag/mlp.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 14:56 ./model/tag/mlp.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d931e682-6fdf-4948-9697-8fe279dde845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17  34 110 ./model/tag/mlp.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!wc ./model/tag/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3314b552-4802-40d6-a8c8-e92084a301e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\t0\n",
      "abb\t1\n",
      "ppm\t2\n",
      "v\t3\n",
      "part\t4\n",
      "[UNK]\t5\n",
      "sb\t6\n",
      "[PAD]\t7\n",
      "conj\t8\n",
      "pron\t9\n",
      "fw\t10\n",
      "int\t11\n",
      "adv\t12\n",
      "adj\t13\n",
      "n\t14\n",
      "tn\t15\n",
      "punc\t16\n"
     ]
    }
   ],
   "source": [
    "!cat ./model/tag/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c06f6-ae3e-48ac-936d-dd9ccbeaf8ed",
   "metadata": {},
   "source": [
    "## Tag Generation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3c8956e1-cb28-4f0f-9e66-906d2bccef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318459eb-f8a4-419e-8f81-1122eb871323",
   "metadata": {},
   "source": [
    "အထက်ပါ input ဖိုင်ထဲက start tag တွေကို အခြေခံပြီး MLP-based LM က generated လုပ်ပြီးထွက်လာတဲ့ output က အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea57522f-115c-4d2f-b549-fa239e4fff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron sb [UNK]\n",
      "     2\tpron fw [UNK]\n",
      "     3\tpron v fw\n",
      "     4\tpron v [PAD]\n",
      "     5\tpron ppm int\n",
      "     6\tn adv punc\n",
      "     7\tn pron pron\n",
      "     8\tn conj sb\n",
      "     9\tn ppm num\n",
      "    10\tn adj pron\n",
      "    11\tadj ppm [UNK]\n",
      "    12\tadj ppm [PAD]\n",
      "    13\tadj part [PAD]\n",
      "    14\tadj v ppm\n",
      "    15\tadj n v\n",
      "    16\tv [PAD] num\n",
      "    17\tv int sb\n",
      "    18\tv abb fw\n",
      "    19\tv tn n\n",
      "    20\tv ppm fw\n",
      "    21\tpron part v ppm\n",
      "    22\tpron part [PAD] [PAD]\n",
      "    23\tpron part int [PAD]\n",
      "    24\tpron part adj v\n",
      "    25\tpron part sb [PAD]\n",
      "    26\tpron ppm part tn\n",
      "    27\tpron ppm n sb\n",
      "    28\tpron ppm ppm adv\n",
      "    29\tpron ppm punc int\n",
      "    30\tpron ppm int n\n",
      "    31\tn v [PAD] pron\n",
      "    32\tn v adj adv\n",
      "    33\tn v [PAD] fw\n",
      "    34\tn v abb sb\n",
      "    35\tn v adj part\n",
      "    36\tn n [UNK] punc\n",
      "    37\tn n conj int\n",
      "    38\tn n ppm n\n",
      "    39\tn n num n\n",
      "    40\tn n tn ppm\n",
      "    41\tv part pron sb\n",
      "    42\tv part sb fw\n",
      "    43\tv part [PAD] n\n",
      "    44\tv part n num\n",
      "    45\tv part conj adj\n",
      "    46\tn part int conj\n",
      "    47\tn part part abb\n",
      "    48\tn part tn num\n",
      "    49\tn part [UNK] abb\n",
      "    50\tn part conj num\n",
      "    51\tpron pron v tn\n",
      "    52\tpron pron punc sb\n",
      "    53\tpron pron adj fw\n",
      "    54\tpron pron tn tn\n",
      "    55\tpron pron tn sb\n",
      "    56\tpron ppm tn num\n",
      "    57\tpron ppm tn part\n",
      "    58\tpron ppm n conj\n",
      "    59\tpron ppm part fw\n",
      "    60\tpron ppm adv abb\n",
      "    61\tn tn [PAD] n\n",
      "    62\tn tn num num\n",
      "    63\tn tn n sb\n",
      "    64\tn tn abb [PAD]\n",
      "    65\tn tn ppm adj\n",
      "    66\tadj v part abb conj\n",
      "    67\tadj v part pron [PAD]\n",
      "    68\tadj v part [UNK] adj\n",
      "    69\tadj v part part adj\n",
      "    70\tadj v part adv fw\n",
      "    71\tn n n num tn\n",
      "    72\tn n n punc adv\n",
      "    73\tn n n punc [PAD]\n",
      "    74\tn n n [UNK] punc\n",
      "    75\tn n n tn n\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/mlp_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d35ec2-8b86-423f-8937-3832db3a76c0",
   "metadata": {},
   "source": [
    "tag generation ကို ဥပမာ အလုံး ၃၀ ထားကြည့်ပြီး run ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "022656ef-3280-44d9-9c05-3073997ef709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: pron conj num part [UNK] num pron int punc num v num n adv num abb adj int punc punc adj [UNK] num adv tn adj num num part abb tn\n",
      "Generated Text 2: n abb conj [PAD] adv int abb fw sb [UNK] tn sb sb adv adj adv num punc [UNK] abb pron [UNK] fw tn pron sb part pron adv tn n\n",
      "Generated Text 3: adj n conj ppm adj [UNK] pron conj sb conj int conj [UNK] num n conj [PAD] adj abb tn [PAD] adj adj v abb sb ppm pron pron v tn\n",
      "Generated Text 4: v adv n n num adj ppm part num tn adv tn tn ppm adv fw adj punc num num adj fw pron n part fw tn abb fw [UNK] v\n",
      "Generated Text 5: pron part conj tn adv adv tn [UNK] v adv tn part punc tn pron punc n pron punc conj v v int v num fw n [UNK] int conj v punc\n",
      "Generated Text 6: pron ppm adj v [PAD] abb conj part n abb conj [UNK] [UNK] adj part punc sb part [UNK] adv v tn [PAD] fw conj sb v [PAD] n part [UNK] abb\n",
      "Generated Text 7: n v fw adv adj sb part num sb n ppm ppm fw num tn conj tn n adv part n int tn tn adv adj abb part abb conj [PAD] conj\n",
      "Generated Text 8: n n part [PAD] tn tn [PAD] n sb tn adv v sb num v int [PAD] n punc fw v [UNK] v punc punc conj [PAD] fw int [PAD] part punc\n",
      "Generated Text 9: v part int v adv num sb v punc n punc adj [PAD] v conj conj adv punc fw int int v conj v pron tn fw v v punc v num\n",
      "Generated Text 10: n part v [UNK] abb [PAD] num int pron conj conj v adj int n [UNK] adj [UNK] n part num n punc conj [PAD] fw sb fw [UNK] pron conj sb\n",
      "Generated Text 11: pron pron [UNK] n [UNK] abb abb v pron abb [PAD] tn abb fw part n [PAD] adj v punc conj adv part part adj [UNK] part [PAD] adv abb [PAD] adj\n",
      "Generated Text 12: pron ppm [PAD] sb fw n adj punc conj adj int punc n pron [UNK] n tn adv pron abb [UNK] v abb tn n part [PAD] fw punc adj adv ppm\n",
      "Generated Text 13: n tn adv num sb part fw adv v part [PAD] fw adj sb sb punc adv int adj adj fw [UNK] punc fw sb fw v tn ppm ppm int [UNK]\n",
      "Generated Text 14: adj v part part int adj n abb num pron abb sb v num sb int n num fw [UNK] n fw punc v part adv [UNK] fw num part int punc ppm\n",
      "Generated Text 15: n n n [PAD] num num n [UNK] punc sb adj part v adj adv num tn adj conj sb int punc adv [PAD] [PAD] pron pron pron [UNK] adj sb fw int\n",
      "\n",
      "real\t0m1.990s\n",
      "user\t0m4.900s\n",
      "sys\t0m2.092s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type mlp --generate --model ./model/tag/mlp.model \\\n",
    "--seq_len 30 --input ./data/myPOS/tag/start_tags.txt --no_of_generation 1 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf77edd-a74b-471f-ad68-c85466fad41b",
   "metadata": {},
   "source": [
    "Test data နဲ့ evaluation လုပ်ထားတဲ့ ရလဒ်ကို ကြည့်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1dc2010d-f756-4115-a39c-6a3017121176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/mlp.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8dd2a-114b-415d-92c3-60fb9a36118f",
   "metadata": {},
   "source": [
    "## Bi-LSTM based LM Building, Tag Generation and Testing\n",
    "\n",
    "bash shell script ကို အောက်ပါအတိုင်း updated လုပ်ခဲ့။  \n",
    "\n",
    "# Run tasks for each model type in the specified order\n",
    "\\#task mlp  \n",
    "task bilstm  \n",
    "\\#task transformer  \n",
    "\\#task bert  \n",
    "\\#task gpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13d93d40-0856-461f-8c03-c64d1dc5210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.25it/s]\n",
      "Epoch 1, Training Loss: 0.1714\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.90it/s]\n",
      "Epoch 1, Validation Loss: 0.0013\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0013\n",
      "Epoch 2/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.17it/s]\n",
      "Epoch 2, Training Loss: 0.0011\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 166.91it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.62it/s]\n",
      "Epoch 3, Training Loss: 0.0003\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 168.17it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.22it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 167.40it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.35it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 167.64it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.62it/s]\n",
      "Epoch 6, Training Loss: 0.0001\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 166.11it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.75it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 173.36it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.61it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 176.41it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.19it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 168.44it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 59.56it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 166.18it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "\n",
      "real\t3m38.241s\n",
      "user\t3m42.390s\n",
      "sys\t0m2.785s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 2: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 3: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 4: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 5: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 6: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 7: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 8: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 9: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 10: n sb n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "\n",
      "real\t0m1.993s\n",
      "user\t0m4.809s\n",
      "sys\t0m2.153s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.806s\n",
      "user\t0m4.629s\n",
      "sys\t0m2.242s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 41.66it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.824s\n",
      "user\t0m4.724s\n",
      "sys\t0m2.107s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1974c6f-c277-45eb-8de2-1b1481361184",
   "metadata": {},
   "source": [
    "## Training/Testing with 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dccee6f4-6fa0-4a68-ad1a-9a43e8710a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 30 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;      \n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 \\\n",
      "      --embedding_method nn.Embedding --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat train_test_tag.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bc57fbe-0ec3-4b8b-b225-e2bc0685084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/30 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.75it/s]\n",
      "Epoch 1, Training Loss: 0.1783\n",
      "Epoch 1/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 169.08it/s]\n",
      "Epoch 1, Validation Loss: 0.0019\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0019\n",
      "Epoch 2/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.76it/s]\n",
      "Epoch 2, Training Loss: 0.0013\n",
      "Epoch 2/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 174.59it/s]\n",
      "Epoch 2, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0002\n",
      "Epoch 3/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.76it/s]\n",
      "Epoch 3, Training Loss: 0.0003\n",
      "Epoch 3/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 180.85it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 4/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.79it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 179.00it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 5/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.64it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 179.80it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 6/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.61it/s]\n",
      "Epoch 6, Training Loss: 0.0006\n",
      "Epoch 6/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 178.71it/s]\n",
      "Epoch 6, Validation Loss: 0.0001\n",
      "Epoch 7/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.66it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 180.01it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Epoch 8/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 179.09it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 9/30 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.60it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 178.97it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 10/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.60it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.98it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Epoch 11/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.64it/s]\n",
      "Epoch 11, Training Loss: 0.0000\n",
      "Epoch 11/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.27it/s]\n",
      "Epoch 11, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 12/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.63it/s]\n",
      "Epoch 12, Training Loss: 0.0000\n",
      "Epoch 12/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.71it/s]\n",
      "Epoch 12, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 13/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 13, Training Loss: 0.0000\n",
      "Epoch 13/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.99it/s]\n",
      "Epoch 13, Validation Loss: 0.0000\n",
      "Epoch 14/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.64it/s]\n",
      "Epoch 14, Training Loss: 0.0000\n",
      "Epoch 14/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.90it/s]\n",
      "Epoch 14, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 15/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.60it/s]\n",
      "Epoch 15, Training Loss: 0.0000\n",
      "Epoch 15/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.28it/s]\n",
      "Epoch 15, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 16/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.60it/s]\n",
      "Epoch 16, Training Loss: 0.0000\n",
      "Epoch 16/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.10it/s]\n",
      "Epoch 16, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 17/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.61it/s]\n",
      "Epoch 17, Training Loss: 0.0000\n",
      "Epoch 17/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.28it/s]\n",
      "Epoch 17, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 18/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 18, Training Loss: 0.0000\n",
      "Epoch 18/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.13it/s]\n",
      "Epoch 18, Validation Loss: 0.0000\n",
      "Epoch 19/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.63it/s]\n",
      "Epoch 19, Training Loss: 0.0000\n",
      "Epoch 19/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.08it/s]\n",
      "Epoch 19, Validation Loss: 0.0000\n",
      "Epoch 20/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 20, Training Loss: 0.0000\n",
      "Epoch 20/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.78it/s]\n",
      "Epoch 20, Validation Loss: 0.0000\n",
      "Epoch 21/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.64it/s]\n",
      "Epoch 21, Training Loss: 0.0000\n",
      "Epoch 21/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.93it/s]\n",
      "Epoch 21, Validation Loss: 0.0000\n",
      "Epoch 22/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.68it/s]\n",
      "Epoch 22, Training Loss: 0.0000\n",
      "Epoch 22/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.64it/s]\n",
      "Epoch 22, Validation Loss: 0.0000\n",
      "Epoch 23/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.66it/s]\n",
      "Epoch 23, Training Loss: 0.0000\n",
      "Epoch 23/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.10it/s]\n",
      "Epoch 23, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 24/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.71it/s]\n",
      "Epoch 24, Training Loss: 0.0000\n",
      "Epoch 24/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.93it/s]\n",
      "Epoch 24, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 25/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.69it/s]\n",
      "Epoch 25, Training Loss: 0.0000\n",
      "Epoch 25/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.73it/s]\n",
      "Epoch 25, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 26/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.66it/s]\n",
      "Epoch 26, Training Loss: 0.0000\n",
      "Epoch 26/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.97it/s]\n",
      "Epoch 26, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 27/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 27, Training Loss: 0.0000\n",
      "Epoch 27/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.83it/s]\n",
      "Epoch 27, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 28/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.75it/s]\n",
      "Epoch 28, Training Loss: 0.0000\n",
      "Epoch 28/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.20it/s]\n",
      "Epoch 28, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 29/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.66it/s]\n",
      "Epoch 29, Training Loss: 0.0000\n",
      "Epoch 29/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 180.39it/s]\n",
      "Epoch 29, Validation Loss: 0.0000\n",
      "Epoch 30/30 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 60.65it/s]\n",
      "Epoch 30, Training Loss: 0.0000\n",
      "Epoch 30/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 179.17it/s]\n",
      "Epoch 30, Validation Loss: 0.0000\n",
      "\n",
      "real\t10m34.678s\n",
      "user\t10m41.831s\n",
      "sys\t0m3.616s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 2: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 3: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 4: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 5: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 6: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 7: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 8: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 9: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 10: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "\n",
      "real\t0m1.972s\n",
      "user\t0m4.785s\n",
      "sys\t0m2.166s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.836s\n",
      "user\t0m4.634s\n",
      "sys\t0m2.289s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 42.86it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.800s\n",
      "user\t0m4.653s\n",
      "sys\t0m2.231s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60da73-f0d3-462f-aa99-1d2c0a11c5fb",
   "metadata": {},
   "source": [
    "## Updated the Shell Script\n",
    "\n",
    "Text generation, testing ပဲ လုပ်ဖို့နဲ့ sequence length ကို 5 ပဲ ထားပြီး run ကြည့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f65cbdb-c2d0-4e9b-852e-d3b71e67d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    #echo \"Training ${model_type^} language model:\";\n",
      "    #time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "    #  --dev_file $dev_data --model $model_file --seq_len 50 --epochs 30 --batch_size 32 \\\n",
      "    #  --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 5 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;      \n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 \\\n",
      "      --embedding_method nn.Embedding --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c862-f14e-450e-a06e-40f0b8b9e256",
   "metadata": {},
   "source": [
    "## Training Again for Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59dd0c06-fdbb-4e1f-be3d-a09ed60a11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: n n n n n n\n",
      "Generated Text 2: n n n n n n\n",
      "Generated Text 3: n n n n n n\n",
      "Generated Text 4: n n n n n n\n",
      "Generated Text 5: n n n n n n\n",
      "Generated Text 6: n n n n n n\n",
      "Generated Text 7: n n n n n n\n",
      "Generated Text 8: n n n n n n\n",
      "Generated Text 9: n n n n n n\n",
      "Generated Text 10: n n n n n n\n",
      "\n",
      "real\t0m1.795s\n",
      "user\t0m4.626s\n",
      "sys\t0m2.251s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.850s\n",
      "user\t0m4.662s\n",
      "sys\t0m2.248s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 42.96it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.821s\n",
      "user\t0m4.686s\n",
      "sys\t0m2.212s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c3478-720b-47fc-8699-91289c1ccd51",
   "metadata": {},
   "source": [
    "## Updated the Bash Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f8cd87db-b31d-4df6-b132-c385df49ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 30 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;      \n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 \\\n",
      "      --embedding_method nn.Embedding --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392aad-9f63-4349-84ef-9be68da5ce2a",
   "metadata": {},
   "source": [
    "## Transformer based LM Training, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ff4ccd8-97ec-46da-b89c-37bf68cbac34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer language model:\n",
      "Epoch 1/30 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 207.39it/s]\n",
      "Epoch 1, Training Loss: 0.0261\n",
      "Epoch 1/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 591.43it/s]\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0004\n",
      "Epoch 2/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 221.10it/s]\n",
      "Epoch 2, Training Loss: 0.0003\n",
      "Epoch 2/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 627.57it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0001\n",
      "Epoch 3/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 219.67it/s]\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 592.04it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 4/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 226.76it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 644.49it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 5/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 226.47it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 548.15it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 6/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 225.58it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 640.02it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 7/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 218.97it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 590.22it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 8/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 224.62it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 623.46it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 9/30 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 222.27it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/30 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 599.45it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 10/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 220.37it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 599.36it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 11/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 218.61it/s]\n",
      "Epoch 11, Training Loss: 0.0000\n",
      "Epoch 11/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 628.34it/s]\n",
      "Epoch 11, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 12/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 218.62it/s]\n",
      "Epoch 12, Training Loss: 0.0000\n",
      "Epoch 12/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 518.49it/s]\n",
      "Epoch 12, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 13/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 221.11it/s]\n",
      "Epoch 13, Training Loss: 0.0000\n",
      "Epoch 13/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 614.18it/s]\n",
      "Epoch 13, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 14/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 227.99it/s]\n",
      "Epoch 14, Training Loss: 0.0000\n",
      "Epoch 14/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 629.95it/s]\n",
      "Epoch 14, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 15/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 228.86it/s]\n",
      "Epoch 15, Training Loss: 0.0000\n",
      "Epoch 15/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 634.96it/s]\n",
      "Epoch 15, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 16/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 219.96it/s]\n",
      "Epoch 16, Training Loss: 0.0000\n",
      "Epoch 16/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 600.15it/s]\n",
      "Epoch 16, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 17/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 223.57it/s]\n",
      "Epoch 17, Training Loss: 0.0000\n",
      "Epoch 17/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 624.92it/s]\n",
      "Epoch 17, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 18/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 219.63it/s]\n",
      "Epoch 18, Training Loss: 0.0000\n",
      "Epoch 18/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 626.53it/s]\n",
      "Epoch 18, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 19/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 225.20it/s]\n",
      "Epoch 19, Training Loss: 0.0000\n",
      "Epoch 19/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 633.82it/s]\n",
      "Epoch 19, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 20/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 217.55it/s]\n",
      "Epoch 20, Training Loss: 0.0000\n",
      "Epoch 20/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 626.39it/s]\n",
      "Epoch 20, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 21/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 215.33it/s]\n",
      "Epoch 21, Training Loss: 0.0000\n",
      "Epoch 21/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 610.40it/s]\n",
      "Epoch 21, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 22/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 221.47it/s]\n",
      "Epoch 22, Training Loss: 0.0000\n",
      "Epoch 22/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 544.12it/s]\n",
      "Epoch 22, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 23/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 218.22it/s]\n",
      "Epoch 23, Training Loss: 0.0000\n",
      "Epoch 23/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 638.72it/s]\n",
      "Epoch 23, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 24/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 228.66it/s]\n",
      "Epoch 24, Training Loss: 0.0000\n",
      "Epoch 24/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 615.45it/s]\n",
      "Epoch 24, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 25/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 228.18it/s]\n",
      "Epoch 25, Training Loss: 0.0000\n",
      "Epoch 25/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 626.69it/s]\n",
      "Epoch 25, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 26/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 228.20it/s]\n",
      "Epoch 26, Training Loss: 0.0000\n",
      "Epoch 26/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 626.52it/s]\n",
      "Epoch 26, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 27/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 227.92it/s]\n",
      "Epoch 27, Training Loss: 0.0000\n",
      "Epoch 27/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 621.77it/s]\n",
      "Epoch 27, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 28/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 226.84it/s]\n",
      "Epoch 28, Training Loss: 0.0000\n",
      "Epoch 28/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 610.56it/s]\n",
      "Epoch 28, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 29/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 226.69it/s]\n",
      "Epoch 29, Training Loss: 0.0000\n",
      "Epoch 29/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 610.34it/s]\n",
      "Epoch 29, Validation Loss: 0.0000\n",
      "Epoch 30/30 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 226.37it/s]\n",
      "Epoch 30, Training Loss: 0.0000\n",
      "Epoch 30/30 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 618.03it/s]\n",
      "Epoch 30, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "\n",
      "real\t2m54.137s\n",
      "user\t3m2.907s\n",
      "sys\t0m2.652s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n ppm adv part part int [PAD] adv tn ppm n pron v punc sb conj conj num punc abb conj adj v ppm n int part tn [UNK] adj [UNK] punc num tn punc conj abb part sb num abb [UNK] int adv num tn punc fw punc adv pron\n",
      "Generated Text 2: n pron part n n num abb sb [UNK] num num [PAD] [UNK] n pron abb [PAD] tn adj sb part ppm abb v adj v conj abb [PAD] v ppm n abb int int punc fw adv [PAD] sb [PAD] fw conj tn [UNK] conj num v conj v v\n",
      "Generated Text 3: n [UNK] ppm abb conj pron tn v tn int n [UNK] v ppm part sb tn punc abb adj v part abb v v n adj punc v conj [PAD] sb pron part tn part punc v ppm fw conj adj [UNK] adj fw [PAD] pron conj punc v n\n",
      "Generated Text 4: n num adj adv n pron pron int tn fw part n [PAD] [UNK] v adv fw sb n sb v sb v abb abb adv part v num [PAD] sb part adv [PAD] tn v [PAD] ppm v int num conj fw adj tn adv conj fw adv abb pron\n",
      "Generated Text 5: n num int n ppm adv sb ppm pron n adj int fw n num adv tn pron part tn ppm abb abb adj conj adj num [UNK] v [PAD] part fw [UNK] v v n int v fw n ppm int pron tn adv tn n conj [UNK] adv adv\n",
      "Generated Text 6: n punc conj num sb part n pron adv conj fw n pron adv conj abb part [PAD] v [UNK] punc v part adv part [UNK] v tn tn ppm conj punc abb ppm tn sb ppm [UNK] v ppm conj int num fw num [UNK] v sb pron num tn\n",
      "Generated Text 7: n abb v fw adv pron num sb ppm fw v part conj ppm ppm conj conj [PAD] [UNK] v abb punc v punc num pron int num pron conj [PAD] part sb int abb pron [UNK] conj adv abb pron punc int fw adv num punc adj pron int tn\n",
      "Generated Text 8: n num num int ppm int abb conj adv adv tn num ppm [UNK] v fw adj [UNK] adj adj [UNK] v num conj n part n fw part [PAD] v part v num ppm [UNK] adj tn abb num punc adv punc part fw [UNK] v ppm punc pron adv\n",
      "Generated Text 9: n adv ppm part pron [PAD] fw [UNK] conj ppm pron n int int int fw sb fw abb sb part part int sb abb sb tn conj num n fw int ppm [UNK] v num [UNK] sb pron [UNK] v v num part ppm sb v adv adv fw conj\n",
      "Generated Text 10: n pron num punc [PAD] int adj part n adj punc fw pron fw pron [PAD] [UNK] tn n tn adj int conj [PAD] conj part pron fw fw int n [PAD] int [UNK] v ppm tn abb pron punc tn num tn [PAD] ppm n conj punc ppm tn abb\n",
      "\n",
      "real\t0m2.187s\n",
      "user\t0m5.066s\n",
      "sys\t0m2.185s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/transformer_gen_texts.txt\n",
      "\n",
      "real\t0m1.729s\n",
      "user\t0m4.595s\n",
      "sys\t0m2.198s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.11it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.539s\n",
      "user\t0m4.510s\n",
      "sys\t0m2.119s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0cb4e-2887-47a2-82b7-6828c98622fc",
   "metadata": {},
   "source": [
    "## GPU Usage of Transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55d341c4-0fdd-4091-abed-692a8ecbe79e",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:20:25 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   64C    P2             330W / 480W |   1202MiB / 24564MiB |     86%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         71MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       48MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    152148      C   python                                      486MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:20:30 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 33%   65C    P2             339W / 480W |   1202MiB / 24564MiB |     69%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         71MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       48MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    152148      C   python                                      486MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c4920-5f10-4ddc-9f4b-616f4b133c8d",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69896d13-97cb-4329-89a5-0d78826d590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3449f69-2711-418b-a146-4649f57c33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:22 ./model/tag/transformer.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 17:20 ./model/tag/transformer.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/transformer.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b1cfaab-14be-4cbd-9ef6-cf0636fff851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./output/tag/\u001b[0m\n",
      "├── bilstm_gen_texts.txt\n",
      "├── mlp_gen_texts.txt\n",
      "└── transformer_gen_texts.txt\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./output/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df2f4cad-0083-4ed4-bcd1-de5b980cbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron pron tn\n",
      "     2\tpron [UNK] ppm\n",
      "     3\tpron int v\n",
      "     4\tpron ppm conj\n",
      "     5\tpron part conj\n",
      "     6\tn int ppm\n",
      "     7\tn [PAD] n\n",
      "     8\tn ppm v\n",
      "     9\tn fw ppm\n",
      "    10\tn adv adj\n",
      "    11\tadj ppm int\n",
      "    12\tadj sb int\n",
      "    13\tadj adj [PAD]\n",
      "    14\tadj fw num\n",
      "    15\tadj v adv\n",
      "    16\tv abb num\n",
      "    17\tv v tn\n",
      "    18\tv v v\n",
      "    19\tv sb sb\n",
      "    20\tv part [UNK]\n",
      "    21\tpron part [UNK] adj\n",
      "    22\tpron part [UNK] adj\n",
      "    23\tpron part punc fw\n",
      "    24\tpron part pron pron\n",
      "    25\tpron part ppm pron\n",
      "    26\tpron ppm adv [UNK]\n",
      "    27\tpron ppm [UNK] adj\n",
      "    28\tpron ppm n conj\n",
      "    29\tpron ppm v fw\n",
      "    30\tpron ppm sb adj\n",
      "    31\tn v tn adv\n",
      "    32\tn v adv num\n",
      "    33\tn v adv punc\n",
      "    34\tn v adv fw\n",
      "    35\tn v sb n\n",
      "    36\tn n adv tn\n",
      "    37\tn n [PAD] pron\n",
      "    38\tn n fw part\n",
      "    39\tn n pron fw\n",
      "    40\tn n punc [PAD]\n",
      "    41\tv part tn fw\n",
      "    42\tv part [PAD] pron\n",
      "    43\tv part ppm num\n",
      "    44\tv part sb int\n",
      "    45\tv part [PAD] [PAD]\n",
      "    46\tn part [UNK] adj\n",
      "    47\tn part pron adj\n",
      "    48\tn part v [PAD]\n",
      "    49\tn part conj n\n",
      "    50\tn part sb v\n",
      "    51\tpron pron abb adv\n",
      "    52\tpron pron ppm ppm\n",
      "    53\tpron pron n conj\n",
      "    54\tpron pron abb punc\n",
      "    55\tpron pron fw conj\n",
      "    56\tpron ppm fw fw\n",
      "    57\tpron ppm ppm adj\n",
      "    58\tpron ppm [UNK] adj\n",
      "    59\tpron ppm fw adv\n",
      "    60\tpron ppm fw abb\n",
      "    61\tn tn conj n\n",
      "    62\tn tn int num\n",
      "    63\tn tn ppm pron\n",
      "    64\tn tn pron [UNK]\n",
      "    65\tn tn sb conj\n",
      "    66\tadj v part tn abb\n",
      "    67\tadj v part punc v\n",
      "    68\tadj v part n v\n",
      "    69\tadj v part [UNK] [PAD]\n",
      "    70\tadj v part adv conj\n",
      "    71\tn n n abb v\n",
      "    72\tn n n abb ppm\n",
      "    73\tn n n v adv\n",
      "    74\tn n n n tn\n",
      "    75\tn n n punc int\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/transformer_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "682e7e46-c302-417c-a932-f1d7b9b338b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/transformer.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558d18e-3570-40a3-9ce8-defa115ef2db",
   "metadata": {},
   "source": [
    "## Updated Bash Shell Script\n",
    "\n",
    "\\#task mlp  \n",
    "\\#task bilstm  \n",
    "\\#task transformer  \n",
    "task bert  \n",
    "\\#task gpt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf4764-aa6f-4605-adde-3adfdbb3b4fb",
   "metadata": {},
   "source": [
    "## BERT based LM Building, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17572252-1f42-4c0f-9d2a-7fa32e5f5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 214.68it/s]\n",
      "Epoch 1, Training Loss: 0.0303\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 629.24it/s]\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0004\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 222.20it/s]\n",
      "Epoch 2, Training Loss: 0.0004\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 628.19it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 223.03it/s]\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 628.81it/s]\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 224.33it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 622.00it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 226.20it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 620.04it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 227.08it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 627.92it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 225.80it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 624.39it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 229.44it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 636.65it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 228.41it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 620.76it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 222.80it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 610.12it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m58.827s\n",
      "user\t1m3.629s\n",
      "sys\t0m2.270s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n conj v [UNK] sb adj fw v sb n punc punc [PAD] int [PAD] num pron [PAD] ppm adv abb part ppm fw ppm v sb tn abb punc n [UNK] adj tn adv n [UNK] [PAD] [UNK] ppm [PAD] fw n adv adj n int sb adv pron adv\n",
      "Generated Text 2: n sb n int v fw adv adj tn punc int pron [UNK] adj ppm abb conj [PAD] [UNK] pron part n fw punc fw tn conj adv n num adj punc adj tn tn fw adj [PAD] tn tn sb adj sb [PAD] sb num punc ppm part tn [UNK]\n",
      "Generated Text 3: n adv part num part int fw int n sb v [PAD] fw part adj v part pron sb ppm adv tn v v int n [UNK] ppm pron conj tn n fw adv conj tn adv adv ppm fw tn part abb abb fw n [UNK] fw punc fw num\n",
      "Generated Text 4: n adv int n sb [PAD] ppm [UNK] adj ppm pron int ppm [UNK] adv pron punc ppm v fw v conj [UNK] sb ppm n part punc fw [UNK] conj v [PAD] n ppm num int conj [UNK] ppm int num [PAD] punc adj num v num fw adj num\n",
      "Generated Text 5: n int n adv adv pron sb abb pron tn part num part tn ppm num conj ppm adv [UNK] [UNK] sb int punc abb pron n sb abb int n [PAD] num pron adj num sb adj adv int adj adv fw part punc num fw int ppm punc tn\n",
      "Generated Text 6: n sb [UNK] pron part [PAD] fw part adv punc fw pron v adj sb v part fw ppm [UNK] adj punc conj ppm fw tn tn [PAD] ppm int pron part [UNK] int sb [UNK] ppm fw num n adv num pron pron adv int abb v [PAD] [PAD] [UNK]\n",
      "Generated Text 7: n [PAD] int adj v int conj pron abb v sb pron pron tn punc conj adv punc conj adv int n part adv adv part punc [PAD] n sb adv n fw tn adj num conj part int conj int [PAD] fw int tn ppm punc ppm int n ppm\n",
      "Generated Text 8: n punc sb tn pron part num v int pron punc v conj pron abb pron n v tn [PAD] fw adv num tn tn abb adv [UNK] pron adv int adj [PAD] int punc part conj part adv n ppm n tn part num v n ppm conj sb tn\n",
      "Generated Text 9: n tn num pron adj part adv num [PAD] sb fw abb conj adv n int v punc ppm [PAD] fw punc punc tn part conj pron [UNK] v part punc abb abb n num v [UNK] [PAD] [PAD] num [PAD] v conj sb ppm ppm num abb v part [UNK]\n",
      "Generated Text 10: n v int ppm adj fw abb [UNK] adj part part conj int punc v ppm n sb adv fw [UNK] adj n abb [PAD] ppm [PAD] adj adv sb part adv pron n abb num int pron v ppm pron part adv fw ppm adj tn [PAD] [PAD] fw part\n",
      "\n",
      "real\t0m2.182s\n",
      "user\t0m5.043s\n",
      "sys\t0m2.121s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/bert_gen_texts.txt\n",
      "\n",
      "real\t0m1.764s\n",
      "user\t0m4.662s\n",
      "sys\t0m2.161s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 50.13it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.561s\n",
      "user\t0m4.490s\n",
      "sys\t0m2.138s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c92b9-bfb4-4df9-b1a5-f580d4795cad",
   "metadata": {},
   "source": [
    "## GPU Usage of BERT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5091b5d-f51e-45b8-9847-3c6edb50e94d",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:37:19 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 34%   66C    P2             344W / 480W |   1211MiB / 24564MiB |     87%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        120MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       62MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    153166      C   python                                      486MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:37:30 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 37%   68C    P2             344W / 480W |   1211MiB / 24564MiB |     86%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        120MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       62MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    153166      C   python                                      486MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e2bbb-b038-472f-b55d-90a804b08ecb",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags and Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bbde7245-f57b-4491-8b90-3387ffb70237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bert.model\n",
      "├── bert.model.vocab\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82c62115-f57e-454b-9655-58916c04b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:37 ./model/tag/bert.model\n",
      "-rw-rw-r-- 1 ye ye  85M Jan 28 15:32 ./model/tag/bilstm.model\n",
      "-rw-rw-r-- 1 ye ye 568K Jan 28 14:57 ./model/tag/mlp.model\n",
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:22 ./model/tag/transformer.model\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/*.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62c7d45a-6efe-4eb0-8001-e78f965ec7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron abb tn\n",
      "     2\tpron abb part\n",
      "     3\tpron adv tn\n",
      "     4\tpron adj ppm\n",
      "     5\tpron n part\n",
      "     6\tn conj fw\n",
      "     7\tn part adv\n",
      "     8\tn abb v\n",
      "     9\tn pron pron\n",
      "    10\tn ppm tn\n",
      "    11\tadj [UNK] ppm\n",
      "    12\tadj n [UNK]\n",
      "    13\tadj [PAD] [PAD]\n",
      "    14\tadj adv adv\n",
      "    15\tadj int adv\n",
      "    16\tv conj punc\n",
      "    17\tv fw adv\n",
      "    18\tv punc [UNK]\n",
      "    19\tv sb abb\n",
      "    20\tv [PAD] conj\n",
      "    21\tpron part [PAD] adv\n",
      "    22\tpron part punc adv\n",
      "    23\tpron part ppm sb\n",
      "    24\tpron part fw sb\n",
      "    25\tpron part sb abb\n",
      "    26\tpron ppm conj ppm\n",
      "    27\tpron ppm adj punc\n",
      "    28\tpron ppm adj conj\n",
      "    29\tpron ppm part int\n",
      "    30\tpron ppm fw adj\n",
      "    31\tn v pron conj\n",
      "    32\tn v num punc\n",
      "    33\tn v abb conj\n",
      "    34\tn v [PAD] sb\n",
      "    35\tn v adj pron\n",
      "    36\tn n adj ppm\n",
      "    37\tn n conj pron\n",
      "    38\tn n part int\n",
      "    39\tn n [PAD] punc\n",
      "    40\tn n conj ppm\n",
      "    41\tv part ppm num\n",
      "    42\tv part adj part\n",
      "    43\tv part fw punc\n",
      "    44\tv part tn pron\n",
      "    45\tv part conj adj\n",
      "    46\tn part punc sb\n",
      "    47\tn part part n\n",
      "    48\tn part tn int\n",
      "    49\tn part num pron\n",
      "    50\tn part adv pron\n",
      "    51\tpron pron ppm sb\n",
      "    52\tpron pron punc v\n",
      "    53\tpron pron conj ppm\n",
      "    54\tpron pron ppm sb\n",
      "    55\tpron pron punc abb\n",
      "    56\tpron ppm part conj\n",
      "    57\tpron ppm punc v\n",
      "    58\tpron ppm [PAD] sb\n",
      "    59\tpron ppm int num\n",
      "    60\tpron ppm conj n\n",
      "    61\tn tn pron [UNK]\n",
      "    62\tn tn v ppm\n",
      "    63\tn tn conj adv\n",
      "    64\tn tn pron [PAD]\n",
      "    65\tn tn adj part\n",
      "    66\tadj v part abb punc\n",
      "    67\tadj v part n adv\n",
      "    68\tadj v part part sb\n",
      "    69\tadj v part tn conj\n",
      "    70\tadj v part [UNK] sb\n",
      "    71\tn n n adj tn\n",
      "    72\tn n n num part\n",
      "    73\tn n n punc adv\n",
      "    74\tn n n int adj\n",
      "    75\tn n n part conj\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/bert_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f775901d-aecb-4754-bfaf-920fa13b0bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1, Training Loss: 0.0303\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0004\n",
      "Epoch 2, Training Loss: 0.0004\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Text generation:\n",
      "Generated Text 1: n conj v [UNK] sb adj fw v sb n punc punc [PAD] int [PAD] num pron [PAD] ppm adv abb part ppm fw ppm v sb tn abb punc n [UNK] adj tn adv n [UNK] [PAD] [UNK] ppm [PAD] fw n adv adj n int sb adv pron adv\n",
      "Generated Text 2: n sb n int v fw adv adj tn punc int pron [UNK] adj ppm abb conj [PAD] [UNK] pron part n fw punc fw tn conj adv n num adj punc adj tn tn fw adj [PAD] tn tn sb adj sb [PAD] sb num punc ppm part tn [UNK]\n",
      "Generated Text 3: n adv part num part int fw int n sb v [PAD] fw part adj v part pron sb ppm adv tn v v int n [UNK] ppm pron conj tn n fw adv conj tn adv adv ppm fw tn part abb abb fw n [UNK] fw punc fw num\n",
      "Generated Text 4: n adv int n sb [PAD] ppm [UNK] adj ppm pron int ppm [UNK] adv pron punc ppm v fw v conj [UNK] sb ppm n part punc fw [UNK] conj v [PAD] n ppm num int conj [UNK] ppm int num [PAD] punc adj num v num fw adj num\n",
      "Generated Text 5: n int n adv adv pron sb abb pron tn part num part tn ppm num conj ppm adv [UNK] [UNK] sb int punc abb pron n sb abb int n [PAD] num pron adj num sb adj adv int adj adv fw part punc num fw int ppm punc tn\n",
      "Generated Text 6: n sb [UNK] pron part [PAD] fw part adv punc fw pron v adj sb v part fw ppm [UNK] adj punc conj ppm fw tn tn [PAD] ppm int pron part [UNK] int sb [UNK] ppm fw num n adv num pron pron adv int abb v [PAD] [PAD] [UNK]\n",
      "Generated Text 7: n [PAD] int adj v int conj pron abb v sb pron pron tn punc conj adv punc conj adv int n part adv adv part punc [PAD] n sb adv n fw tn adj num conj part int conj int [PAD] fw int tn ppm punc ppm int n ppm\n",
      "Generated Text 8: n punc sb tn pron part num v int pron punc v conj pron abb pron n v tn [PAD] fw adv num tn tn abb adv [UNK] pron adv int adj [PAD] int punc part conj part adv n ppm n tn part num v n ppm conj sb tn\n",
      "Generated Text 9: n tn num pron adj part adv num [PAD] sb fw abb conj adv n int v punc ppm [PAD] fw punc punc tn part conj pron [UNK] v part punc abb abb n num v [UNK] [PAD] [PAD] num [PAD] v conj sb ppm ppm num abb v part [UNK]\n",
      "Generated Text 10: n v int ppm adj fw abb [UNK] adj part part conj int punc v ppm n sb adv fw [UNK] adj n abb [PAD] ppm [PAD] adj adv sb part adv pron n abb num int pron v ppm pron part adv fw ppm adj tn [PAD] [PAD] fw part\n",
      "Batch text generation from file:\n",
      "Generated texts saved to ./output/tag/bert_gen_texts.txt\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 50.13it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!cat ./log/tag/bert.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ff02c-a596-4b73-861c-23214b01e0c7",
   "metadata": {},
   "source": [
    "## Updated Bash Shell Script\n",
    "\n",
    "\\#task mlp  \n",
    "\\#task bilstm  \n",
    "\\#task transformer  \n",
    "\\#task bert  \n",
    "task gpt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f811-e859-471f-a25d-96a0e8e31ca1",
   "metadata": {},
   "source": [
    "## GPT based LM Training, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24e2ece1-76cd-4940-a54f-f629fbd84ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gpt language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 220.21it/s]\n",
      "Epoch 1, Training Loss: 0.0130\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 622.94it/s]\n",
      "Epoch 1, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0002\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 235.25it/s]\n",
      "Epoch 2, Training Loss: 0.0002\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 604.23it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 230.58it/s]\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 592.55it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 235.87it/s]\n",
      "Epoch 4, Training Loss: 0.0000\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 612.15it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 229.30it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 602.98it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 226.67it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 612.60it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 229.99it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 617.84it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 237.93it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 613.26it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 238.41it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 619.26it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 226.30it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 582.01it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m57.356s\n",
      "user\t1m2.007s\n",
      "sys\t0m2.250s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n adj [PAD] v [UNK] fw [UNK] adj num sb conj int tn adv conj [UNK] adv tn part [PAD] tn adj ppm int num num [UNK] part int adj tn tn [PAD] conj adj adv sb n part ppm tn tn int punc v v pron fw fw abb pron\n",
      "Generated Text 2: n pron v pron abb sb n punc adj adv n int pron sb v adv pron adv int n part n pron adv adv ppm abb int [PAD] num ppm sb [PAD] pron conj ppm adv n fw [UNK] conj [PAD] adv abb [UNK] num adj tn sb abb fw\n",
      "Generated Text 3: n int [UNK] num abb punc conj part [UNK] ppm fw fw conj [UNK] int fw part tn [UNK] int sb ppm adj num sb pron tn sb int v tn n fw adj n part pron v conj [PAD] adv v sb n adj part abb int part abb sb\n",
      "Generated Text 4: n punc [UNK] pron n fw part part v num sb sb v ppm conj int num abb punc pron pron adv adj [UNK] [UNK] conj [PAD] sb abb part sb tn n adv num n adv sb fw conj pron punc [PAD] n conj n punc [PAD] adv sb adv\n",
      "Generated Text 5: n adv fw tn int adv int part ppm pron int part abb int int adj sb n int num pron part num int num adv v int part sb abb pron sb abb adv [UNK] n abb [PAD] v [PAD] part conj tn adv int adj [PAD] adv sb [PAD]\n",
      "Generated Text 6: n conj num tn tn adj tn n pron adj int [PAD] punc ppm v v conj ppm [UNK] num tn adv n ppm sb [UNK] ppm sb ppm adj [UNK] punc [UNK] n pron n conj v part adv ppm abb [PAD] punc n part [UNK] v [UNK] int int\n",
      "Generated Text 7: n n v sb adv pron n [UNK] [PAD] punc fw int v num punc part punc [PAD] pron punc punc [UNK] [PAD] tn num v n fw pron pron ppm [PAD] sb adv part punc part abb conj fw fw part pron int adv int abb fw abb [PAD] ppm\n",
      "Generated Text 8: n [PAD] [UNK] int part adv part [UNK] n tn n fw punc adj conj [UNK] punc int [UNK] sb adv n pron [PAD] pron tn adj [PAD] conj [PAD] tn part [PAD] [UNK] int tn sb pron pron sb ppm sb part pron [PAD] v abb n [PAD] punc [PAD]\n",
      "Generated Text 9: n abb [PAD] [PAD] sb conj [PAD] fw ppm adj num adj punc num int [UNK] part num [UNK] [UNK] sb ppm part ppm adj adv int pron n [UNK] int punc num pron pron adv ppm abb adv abb pron n punc int conj pron adv tn int [PAD] part\n",
      "Generated Text 10: n conj sb v adj fw int n fw adj num abb adj sb ppm n punc ppm tn abb int adv [PAD] sb tn conj tn v num adj int [UNK] conj conj sb tn tn punc conj punc adv sb adj part v ppm int punc abb fw abb\n",
      "\n",
      "real\t0m2.207s\n",
      "user\t0m5.074s\n",
      "sys\t0m2.197s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/gpt_gen_texts.txt\n",
      "\n",
      "real\t0m1.824s\n",
      "user\t0m4.681s\n",
      "sys\t0m2.198s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 49.38it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.574s\n",
      "user\t0m4.483s\n",
      "sys\t0m2.156s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a41ed7-5102-4c97-95a2-aaef9c4fe1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03cc7140-f50b-4465-901b-470b5eb906c6",
   "metadata": {},
   "source": [
    "## GPU Usage of GPT LM Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "688e3229-3518-4b0b-95ed-97ba2bac014a",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:40:25 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   64C    P2             340W / 480W |   1209MiB / 24564MiB |     85%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        121MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       57MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    153600      C   python                                      488MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Tue Jan 28 17:40:41 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 36%   67C    P2             361W / 480W |   1209MiB / 24564MiB |     87%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        121MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        80MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       57MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      161MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    153600      C   python                                      488MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc6796-7af5-41be-b047-8e3b6d1eba0f",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "45a19d4c-a06c-4889-afa3-60df0a57fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bert.model\n",
      "├── bert.model.vocab\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── gpt.model\n",
      "├── gpt.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09ea5d18-604c-4f4e-848a-7dcc62c4558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 28 17:40 ./model/tag/gpt.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 17:40 ./model/tag/gpt.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/gpt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "540b70bc-65e5-4c71-9cc7-5160e4281d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron punc tn\n",
      "     2\tpron n abb\n",
      "     3\tpron v adj\n",
      "     4\tpron tn v\n",
      "     5\tpron adv [PAD]\n",
      "     6\tn fw abb\n",
      "     7\tn sb int\n",
      "     8\tn [UNK] punc\n",
      "     9\tn tn abb\n",
      "    10\tn [PAD] punc\n",
      "    11\tadj [PAD] conj\n",
      "    12\tadj v adj\n",
      "    13\tadj n ppm\n",
      "    14\tadj int sb\n",
      "    15\tadj num tn\n",
      "    16\tv v n\n",
      "    17\tv adv n\n",
      "    18\tv punc part\n",
      "    19\tv int int\n",
      "    20\tv int abb\n",
      "    21\tpron part pron int\n",
      "    22\tpron part [UNK] v\n",
      "    23\tpron part pron fw\n",
      "    24\tpron part n n\n",
      "    25\tpron part n ppm\n",
      "    26\tpron ppm v [UNK]\n",
      "    27\tpron ppm adj adj\n",
      "    28\tpron ppm int abb\n",
      "    29\tpron ppm abb adj\n",
      "    30\tpron ppm abb abb\n",
      "    31\tn v num abb\n",
      "    32\tn v punc [UNK]\n",
      "    33\tn v abb abb\n",
      "    34\tn v pron [UNK]\n",
      "    35\tn v pron part\n",
      "    36\tn n [UNK] n\n",
      "    37\tn n conj part\n",
      "    38\tn n [PAD] fw\n",
      "    39\tn n adv adv\n",
      "    40\tn n abb abb\n",
      "    41\tv part sb conj\n",
      "    42\tv part ppm abb\n",
      "    43\tv part part ppm\n",
      "    44\tv part sb fw\n",
      "    45\tv part n part\n",
      "    46\tn part num part\n",
      "    47\tn part pron n\n",
      "    48\tn part n conj\n",
      "    49\tn part int v\n",
      "    50\tn part tn ppm\n",
      "    51\tpron pron num pron\n",
      "    52\tpron pron fw ppm\n",
      "    53\tpron pron part int\n",
      "    54\tpron pron n adv\n",
      "    55\tpron pron conj conj\n",
      "    56\tpron ppm [PAD] pron\n",
      "    57\tpron ppm adj num\n",
      "    58\tpron ppm fw punc\n",
      "    59\tpron ppm [PAD] pron\n",
      "    60\tpron ppm abb adv\n",
      "    61\tn tn punc v\n",
      "    62\tn tn n [PAD]\n",
      "    63\tn tn int int\n",
      "    64\tn tn part num\n",
      "    65\tn tn sb part\n",
      "    66\tadj v part tn int\n",
      "    67\tadj v part n n\n",
      "    68\tadj v part pron tn\n",
      "    69\tadj v part [UNK] adj\n",
      "    70\tadj v part [UNK] fw\n",
      "    71\tn n n v punc\n",
      "    72\tn n n n tn\n",
      "    73\tn n n ppm ppm\n",
      "    74\tn n n n ppm\n",
      "    75\tn n n abb fw\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/gpt_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "74036318-7915-4fa4-94a8-2044b64bb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./log/tag/bert.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/bilstm.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/gpt.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/mlp.log <==\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "==> ./log/tag/transformer.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69fe68-f0d2-4dd8-a9eb-c17d2b9105ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
