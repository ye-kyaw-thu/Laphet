{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41a129b-1dcd-433b-830b-d1ce546f05e2",
   "metadata": {},
   "source": [
    "# Laphet (Version 0.7) with Myanmar Names Dataset\n",
    "## Using nn.Embedding   \n",
    "\n",
    "ဒီ Jupyter notebook က Laphet LM Toolkit ကို သုံးပြီး MLP, Bi-LSTM, Transformer, BERT, GPT အခြေခံတဲ့ language model (LM) တွေကို ဆောက်ကြည့်တာ၊ ဆောက်ထားပြီးသား LM ကိုသုံးပြီး text generation စမ်းလုပ်ပြထားတာပါ။  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f76d6-b828-4250-9234-00caf02d91d1",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "Dataset link: [https://github.com/ye-kyaw-thu/myRoman](https://github.com/ye-kyaw-thu/myRoman)  \n",
    "\n",
    "myRoman ဒေတာ format က အောက်ပါအတိုင်း ရှိတာမို့  \n",
    "\n",
    "1\tကကြိုးကြာ\tက ကြိုး ကြာ\tka kyoe kyar  \n",
    "2\tကချောင်လွမ်းနန်း\tက ချောင် လွမ်း နန်း\tka chaung lwan nan  \n",
    "3\tကချောင်လွမ်းနန်း\tက ချောင် လွမ်း နန်း\tka chaung lwan nang  \n",
    "\n",
    "ဝဏ္ဏဖြတ်ထားတဲ့ (i.e. syllable) တတိယ ကော်လံကို ယူသုံးထားပါတယ်။  \n",
    "ပြီးတော့ ‌ZWNJ (U+200C) နဲ့ SHY (U+00AD) သင်္ကေတတွေကို clean လုပ်ပြီး သုံးခဲ့ပါတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d6982d-00e8-4a4f-b809-13b139e91b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_name.txt  start_names.txt  test_name.txt  train_name.txt\n"
     ]
    }
   ],
   "source": [
    "ls ./data/myRoman/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816fc343-d013-4a92-bf0d-a8f1b0152f54",
   "metadata": {},
   "source": [
    "train_name.txt ဖိုင်က training လုပ်တဲ့အခါမှာ သုံးဖို့အတွက် ပြင်ထားတာ။  \n",
    "dev_name.txt ဖိုင်က development သို့မဟုတ် validation အတွက် သုံးဖို့ ပြင်ထားတာ။  \n",
    "test_name.txt ဖိုင်က testing/evaluation လုပ်ဖို့အတွက် ပြင်ထားတာ။  \n",
    "start_names.txt ဖိုင်က ဆောက်ထားတဲ့ language model တွေနဲ့ မြန်မာနာမည်တွေကို generate လုပ်ခိုင်းတဲ့အခါမှာ ကိုယ်က စစေချင်တဲ့ ဝဏ္ဏ (i.e. syllable) စာလုံးတွေကို ပြင်ထားတဲ့ ဖိုင်ပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301671e0-feeb-4d40-9f52-e4e0be7d9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1000    3193   37204 ./data/myRoman/dev_name.txt\n",
      "     15      18     205 ./data/myRoman/start_names.txt\n",
      "   1000    3246   37728 ./data/myRoman/test_name.txt\n",
      "  27246   88007 1028564 ./data/myRoman/train_name.txt\n",
      "  29261   94464 1103701 total\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myRoman/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebf178-7430-43f6-9a99-4e733a2fd55e",
   "metadata": {},
   "source": [
    "အထက်မှာ မြင်ရတဲ့အတိုင်း training data က စာကြောင်းရေ နှစ်သောင်းခုနှစ်ထောင်ကျော်ရှိပါတယ်။  \n",
    "development နဲ့ test ဒေတာကိုတော့ စာကြောင်းရေ တစ်ထောင်စီ ထားထားပါတယ်။\n",
    "\n",
    "File format က sentence တစ်ကြောင်းကို နာမည်တစ်ခုမို့လို့ တနည်းအားဖြင့် training ဒေတာက မြန်မာစာနဲ့ ရိုက်ထားတဲ့ နာမည်ပေါင်း နှစ်သောင်းခုနှစ်ထောင်ကျော်ရှိပါတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25faf117-c1fc-41df-a55f-1d79e9e9d0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "သူ ရ ဖြိုး မင်း\n",
      "ခန့် မင်း သူ\n",
      "ခိုင် ဇာ ဝင်း\n",
      "ကြည် အောင် မင်း\n",
      "အောင် ဇင် ခန့်\n",
      "ကျော် လင်း ဝေ\n",
      "ခတ္တာ ဇော်\n",
      "ကောင်း ခန့် လင်း\n",
      "အောင် ထူး\n",
      "အောင် သန်း ထွန်း\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myRoman/train_name.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12cd7a84-0f27-409e-92b4-f14a6c664d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကံ ကောင်း\n",
      "လဲ့ လဲ့\n",
      "ကျော် တိုး ဝေ\n",
      "သက် ဘုန်း နိုင်\n",
      "သိမ့် သူ သူ ကျော်\n",
      "သဲ အိန္ဒြေ အောင်\n",
      "ကျော် သန်း ထွန်း\n",
      "အိ မွန် သန့်\n",
      "အေး မြင့် မိုရ် ပိုင်\n",
      "အောင် ဖုန်း ပိုင်\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myRoman/dev_name.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67277981-f61e-416e-9052-dcd8372c3be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "အေး သိင်္ဂီ\n",
      "သာ စော\n",
      "ကျော့ ယ မင်း သွယ်\n",
      "မိုး ဆု ပန်\n",
      "ခေါ် ထန့်\n",
      "ခိုင် ခိုင် ဖြိုး\n",
      "ကြယ် စင် မင်း ခန့်\n",
      "သေး မယ်\n",
      "သိဒ္ဓိ ဆန်း\n",
      "က လျာ သန်း\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myRoman/test_name.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef1659e-22fa-4ad5-9b32-eb4f959f98e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော်\n",
      "မ မ\n",
      "အေး\n",
      "လှ လှ\n",
      "\n",
      "မြ အေး\n",
      "သ\n",
      "မောင်\n",
      "မြင့် မြင့်\n",
      "ရွှေ\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myRoman/start_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0144bd9-5a43-4d71-88d7-0ffad475d7b2",
   "metadata": {},
   "source": [
    "start_names.txt ဖိုင်ထဲမှာ blank line ကိုလည်း တမင်တကာ ထားပြီး စမ်းထားတာပါ။ ကျပန်း အစစာလုံးနဲ့ ပရိုဂရမ်က name generate လုပ်ပေးသလား စမ်းချင်လို့ပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46704e53-c2c5-4694-a504-93b17f53c810",
   "metadata": {},
   "source": [
    "## --help of Laphet (Version 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12ad27a2-453e-4134-a137-a8fc0041c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: laphet.py [-h] --model_type {mlp,bilstm,transformer,bert,gpt} [--train]\n",
      "                 [--generate] [--test] [--data DATA] [--model MODEL]\n",
      "                 [--vocab VOCAB] [--dev_file DEV_FILE] [--test_file TEST_FILE]\n",
      "                 [--prompt PROMPT] [--input INPUT] [--seq_len SEQ_LEN]\n",
      "                 [--output OUTPUT] [--no_of_generation NO_OF_GENERATION]\n",
      "                 [--epochs EPOCHS] [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                 [--embed_dim EMBED_DIM] [--num_heads NUM_HEADS]\n",
      "                 [--num_layers NUM_LAYERS] [--hidden_dim HIDDEN_DIM]\n",
      "                 [--ff_dim FF_DIM] [--dropout DROPOUT]\n",
      "                 [--temperature TEMPERATURE] [--top_k TOP_K] [--top_p TOP_P]\n",
      "                 [--embedding_method {nn.Embedding,fasttext_freeze,fasttext_no_freeze}]\n",
      "                 [--fasttext_model FASTTEXT_MODEL]\n",
      "\n",
      "Laphet language model toolkit for Burmese.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_type {mlp,bilstm,transformer,bert,gpt}\n",
      "                        Type of model to use: mlp, bilstm, transformer, bert\n",
      "                        or gpt.\n",
      "  --train               Train the model.\n",
      "  --generate            Generate text using the trained model.\n",
      "  --test                Test the BERT model and evaluate perplexity.\n",
      "  --data DATA           Path to the dataset.\n",
      "  --model MODEL         Path to save/load the model.\n",
      "  --vocab VOCAB         Path to save/load the tokenizer vocabulary\n",
      "  --dev_file DEV_FILE   Path to the development or validation dataset.\n",
      "  --test_file TEST_FILE\n",
      "                        Path to the test dataset.\n",
      "  --prompt PROMPT       Prompt for text generation (default: None).\n",
      "  --input INPUT         File with starting words for line-by-line generation\n",
      "                        (default: None).\n",
      "  --seq_len SEQ_LEN     Sequence length (default: 30).\n",
      "  --output OUTPUT       File to save the generated text (default: None).\n",
      "  --no_of_generation NO_OF_GENERATION\n",
      "                        Number of sequences to generate (default: 1).\n",
      "  --epochs EPOCHS       Number of training epochs (default: 10).\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size (default: 32).\n",
      "  --lr LR               Learning rate (default: 0.0001).\n",
      "  --embed_dim EMBED_DIM\n",
      "                        Embedding dimension (default: 256).\n",
      "  --num_heads NUM_HEADS\n",
      "                        Number of attention heads (default: 4).\n",
      "  --num_layers NUM_LAYERS\n",
      "                        Number of layers (default: 4).\n",
      "  --hidden_dim HIDDEN_DIM\n",
      "                        Hidden dimension for LSTM (default: 512).\n",
      "  --ff_dim FF_DIM       Feedforward dimension for Transformer (default: 512).\n",
      "  --dropout DROPOUT     Dropout rate for LSTM (default: 0.5).\n",
      "  --temperature TEMPERATURE\n",
      "                        Sampling temperature (default: 1.0).\n",
      "  --top_k TOP_K         Top-k sampling (default: 10).\n",
      "  --top_p TOP_P         Top-p sampling (default: 0.9).\n",
      "  --embedding_method {nn.Embedding,fasttext_freeze,fasttext_no_freeze}\n",
      "                        Embedding method to use: nn.Embedding,\n",
      "                        fasttext_freeze, or fasttext_no_freeze.\n",
      "  --fasttext_model FASTTEXT_MODEL\n",
      "                        Path to the pretrained FastText model (required if\n",
      "                        embedding_method is fasttext_freeze or\n",
      "                        fasttext_no_freeze).\n"
     ]
    }
   ],
   "source": [
    "!python ./laphet.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f50e4-3269-4067-938d-dbe6cbb468bb",
   "metadata": {},
   "source": [
    "ကိုယ်စက်ထဲမှာ pytorch အပါအဝင် လိုအပ်တဲ့ python library တွေကို မှန်မှန်ကန်ကန် install လုပ်ထားရင် အထက်ပါလိုမျိုး help screen ကို မြင်ရပါလိမ့်မယ်။   \n",
    "\n",
    "Version 0.7 မှာ အသစ်တိုးထားတဲ့ command line arguments တွေက  \n",
    "- --embedding_method\n",
    "- --fasttext_model\n",
    "\n",
    "Version 0.6 မှာတုန်းက simple embedding method ဖြစ်တဲ့ nn.Embedding ကိုပဲသုံးထားပြီ။ ခု version 0.7 မှာတော့ fasttext embedding feature နဲ့ run လို့ ရသွားပါပြီ။ အဲဒီမှာတင် freeze နဲ့ no_freeze (feature finetuning) ဆိုပြီး နှစ်မျိုး ထပ်ကွဲသွားပါလိမ့်မယ်။ ဒီ notebook မှာတော့ nn.Embedding နဲ့ပဲ run ပြပါမယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b68ee-7332-4094-9133-93d65b9cf11c",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) based Language Modeling\n",
    "\n",
    "အရင်ဆုံး MLP-based LM ကို ဆောက်မယ်။ ပြီးရင် text generation လုပ်ကြည့်မယ်။ ပြီးရင် test data ကိုသုံးပြီး evaluation လုပ်ကြည့်မယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ce127-8140-4785-b89c-fd921994538a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbd20ea0-1070-4788-87de-354d146ef9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 128.90it/s]\n",
      "Epoch 1, Training Loss: 1.3342\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 508.30it/s]\n",
      "Epoch 1, Validation Loss: 1.0547\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0547\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 133.98it/s]\n",
      "Epoch 2, Training Loss: 1.0531\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 497.58it/s]\n",
      "Epoch 2, Validation Loss: 1.0526\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0526\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 134.54it/s]\n",
      "Epoch 3, Training Loss: 1.0522\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 514.45it/s]\n",
      "Epoch 3, Validation Loss: 1.0522\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0522\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 133.04it/s]\n",
      "Epoch 4, Training Loss: 1.0520\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 511.41it/s]\n",
      "Epoch 4, Validation Loss: 1.0520\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0520\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 140.14it/s]\n",
      "Epoch 5, Training Loss: 1.0519\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 492.60it/s]\n",
      "Epoch 5, Validation Loss: 1.0520\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0520\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 139.41it/s]\n",
      "Epoch 6, Training Loss: 1.0519\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 513.18it/s]\n",
      "Epoch 6, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 139.80it/s]\n",
      "Epoch 7, Training Loss: 1.0519\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 488.93it/s]\n",
      "Epoch 7, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 137.92it/s]\n",
      "Epoch 8, Training Loss: 1.0518\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 504.13it/s]\n",
      "Epoch 8, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 139.13it/s]\n",
      "Epoch 9, Training Loss: 1.0518\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 499.25it/s]\n",
      "Epoch 9, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:06<00:00, 137.87it/s]\n",
      "Epoch 10, Training Loss: 1.0518\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 510.56it/s]\n",
      "Epoch 10, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "\n",
      "real\t1m5.005s\n",
      "user\t1m7.369s\n",
      "sys\t0m2.306s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type mlp --train --data ./data/myRoman/train_name.txt \\\n",
    "--dev_file ./data/myRoman/dev_name.txt --model ./model/name/mlp.model \\\n",
    "--seq_len 50 --epochs 10 --batch_size 32 --lr 0.0001 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce2fcf-606b-476f-be87-099e80b97bb2",
   "metadata": {},
   "source": [
    "## GPU Usage for MLP LM Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "574ed648-22e7-4216-9850-81e2bcecdf53",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 22:29:56 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   62C    P2             237W / 480W |   1171MiB / 24564MiB |     75%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          252MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       46MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    171431      C   python                                      418MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872326e-a819-45ac-b541-f70a7191f2a5",
   "metadata": {},
   "source": [
    "ထွက်လာတဲ့ vocab ဖိုင်နဲ့ model ဖိုင်ကို လေ့လာကြည့်ရအောင်..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f02cbd9-0d5d-428d-8a25-b6c7c4271d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 4.8M Jan 28 22:30 ./model/name/mlp.model\n",
      "-rw-rw-r-- 1 ye ye  24K Jan 28 22:29 ./model/name/mlp.model.vocab\n"
     ]
    }
   ],
   "source": [
    "ls -lh ./model/name/mlp.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b65769a-bf29-4494-9ea6-ad9b7291cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/name/mlp.model: Zip archive data, at least v0.0 to extract, compression method=store\n"
     ]
    }
   ],
   "source": [
    "!file ./model/name/mlp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65692360-8fcf-4d04-93e3-ade2e2fbe5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "လက္ခ\t0\n",
      "အား\t1\n",
      "ကျဲရ်\t2\n",
      "ငြိမ့်\t3\n",
      "ဒွတ်\t4\n",
      "နဲမ်\t5\n",
      "ဟင်\t6\n",
      "လွေ\t7\n",
      "ဟေ\t8\n",
      "ချွတ်\t9\n"
     ]
    }
   ],
   "source": [
    "!head ./model/name/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c972af4-6aae-4f43-8734-fbb191452c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ယော်\t1433\n",
      "လင်း\t1434\n",
      "ဂျက်ခ်\t1435\n",
      "လန်း\t1436\n",
      "ကယ်လ်\t1437\n",
      "မိန်း\t1438\n",
      "ဇိန်း\t1439\n",
      "အဉ္ဇ\t1440\n",
      "အန့်\t1441\n",
      "ကုန်\t1442\n"
     ]
    }
   ],
   "source": [
    "!tail ./model/name/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23190499-5d4d-4b91-9dd5-6870fd511793",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba855c51-44e9-461f-a119-b2af134b018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ဆူ ဟွမ် ကု ဇင်း စီး ခွေး ဇင်း ဝိုင် ဂျက်ခ် ပေး ဟို့စ် လိ ချောင်း စီ မြို့ ဖုဏ်း ကြွ ဉာဏ် သြ ဖွား ဘင်း ချုပ် ကွန်း မင် ဟီး နန်းဒ် အု ပေါ လှေ မက် ကပ် ရဲ ပြုံး ချိန် ကွာ ငုံ ဣန္ဒာ ရ အက်စ် ခါ ထွဋ် ထာရ် ယု ဗွန် ဗုံ ကျိန် မာ့ ဘက် ဇေါင်း ယ\n",
      "Generated Text 2: ရဲ ဂျွန်း ရှမ်း ဧည့် ငဲ ကောက် ဆပ် ဒေါင်း လဲစ် ဟြေ ကြံ့ ယိမ့် အိမ်း ကေ ခွား ဗို ဌေး ကြောင်း အန်ဒ် ဇန် ဘင် အက်စ် ဒိုး မြိုင် ရှိန်း ဂုန် ပွဲ ဥမ္မာ လက်စ် ခွမ်း ဝူ ဒု ပန်း ချင်း ချမ်း ဓ ကြည် ဂွိ ကိုင် နှော ဓိ သန္ဒာ ထွဋ် ရွိုင် အိန္နီ ကင် ဆုံး ဂေါင် လဲစ် ဒုလ် နှစ်\n",
      "Generated Text 3: ရဲ ခေါန် ပု ရွိုင် ဆွဲ အပ္ပ သင်္ခ ကိုင်း ကြော့ ပ မာ့ သိဉ္စည်း ဒင် ကျွယ် လုံး ဟဲ ဇိန်း နို သဒ္ဒါ ဆုန်း တွေး ဒါးလ် အဲမ် ချိန်း [PAD] ညှာ ပါယ် ရှာမ် ဂျက်ခ် ဇွန်း စည်း ထ သက္က မိ ငိုက် ဘူ ဂျူ ယဉ် ဝင့် နိုး ရိန်း ဝိုင် ဘဲန် ပါး ငု လင်္ကာ သျှံ လဲ့ ရင့် မိုး ဂဲလ်\n",
      "Generated Text 4: ရဲ သျှင်း ကျန်း ကောလ် ငိုင် ဘယ်လ် ရို ဆွေး သိပ္ပံ ရှာ မောင်း ဇီ တက္ခ ဟီး တား ကွန့် ထု ဖီး မိုး တင်း ဟံ အဂ္ဂါ ဒေါင့် စံ လာန် ပံ ပွန်း ဒေါင့် အဂ္ဂါ လန်း ဘာ ငုံ ရို လာဘ် ဂျော် ဗန်း ဇာရ် ဒိမ် ဂုံ ဖွယ် ဘန်း ဂျိုး ဏိ လှိုင်း လှေ ဆွမ့် ဟင် ဗျာလ် နှောင်း သုံး အော\n",
      "Generated Text 5: ရဲ ပွန်း တောင်း ထွေ ရယ် ဂဂ် သင်္ချာ လိမ်း ကျော မူး တုံး ကျုံး ဂုန် အဉ္ဇ ဝိုက် အေ လ ဟိမ်း ပြာ ရော် ဝူ နဲ့ ဝဏ္ဏ စိုင်း ကြူ ဘယ်လ် ဟ ကွေ ဝံ ခက် ရား ရွှန်း နှဲမ်း ပေါင် ကြွက် ဆမ် ဒေါသ် ခဂ္ဂါ သန္ဒြာ စေး ဆုံး လိန်း လျှား ခွါ ပါရ် ဖြာ ရေ ဆင် အုံ ဝန် ရယ်\n",
      "Generated Text 6: ရဲ လိ ဒိ ဟင်လ် စွန်း ဖွေး ခိုင် မျှော် ဆေး လည်း ဆုမ်း သဲ ဿန် ရုဏ် ကွယ် ပိ ပုံ ခွါလ် ရိမ် ခေါလ် ရော ရေး ရံ သိဉ္စည်း နယ် ရှဲလ် ရံ ဘဲန် န အာဖ် ဒေါင် လောင်ဝ် ထာရ် တွမ်း ပုံ့ နှိုင်း ရှိမ်း သဉ္စာ အင်း ယောင် ရုံး လွှင် တန် သစ် ချယ် ရွာ ကြား ပေး လဲမ့် ဟုန် ဂန္တ\n",
      "Generated Text 7: ရဲ ရန် ဂူး ပြောင်း ညီ သေ့ နွေ ရော့ဒ် နန် တုတ် လို ပီ သေ့ အိန္ဒြေ သေ့ ဆုန်း ဟြေ လောဒ် ပျံ့ ဂန့် ရှယ် ကိ ဘက် ချီ ကော် ဒါ ဝန် မြဉ္ဇူ သောင်း ပိုင် ယတ် သျှန္တီ ပိုက် ကျင် နှစ် ထယ် ကျုံ မြူ စွန် ဒါးလ် ယုန် ချစ် တိန့် ဇမ္ဗူ ဒေါင့် ဇယ် ပွင့်် ထည် မျှား ပြောင် ဘီလ်\n",
      "Generated Text 8: ရဲ ဘွယ် စင် ပြာ စိုး ဆိုင် တွယ် ထာသ် ပိ နယ် ဂေါ် ကူ ဒိမ် ကျော် ယယ် ရှာ ဖောင် ရား ထွတ် ကျန် အုတ် ဝန်း အိုက် ဆီး မျက် ဇ္ဈိ ထွာ ချော ကျန်း ဟေး ဒစ် ချူး လစ် ယျ စွမ်း ဆောင် မတ် ဂူး ကောင်း နျူး ပံ ကျေး ကား ရယ် ကျော် ရှိုင်း ကြို ကျူး ရေး ဟင်လ် ဘွား\n",
      "Generated Text 9: ရဲ မြန် လိန် ကိန္န ပါး သန္ဒာ ဟူး မှု ရှမ် သီ အဲလ် အာ မြေ့ မင်း တိန်း ဿန် ဝုန် ကင် ယူး သွေး ခေါ်လ် ဓမ္မာ ကောလ် လှန် ခြင်း ဂဂ္ဂါး ကိန်း ညှင်း တွေး အိုက် သိဉ္စည်း တာ ဟန် ဖုံး ဖျား ရိ ပူး သောက် ပွ မြင့် ငွေ့ ဖွား ဇယ် ဆွင် ကျူ မွေ ညိမ့် စူ မွန်း ပြည် အာဖ်\n",
      "Generated Text 10: ရဲ တိုက် ကြုံ လည်း ဗေ ဟတ် ထွယ် ဘီ နှဲမ် အိမ်း ပေး ဆွမ်း ညိမ့် ဥတ္တ အင် ဇုန် ဂျာလ် ထော ဆန် ငြိမ်း ကျူး ကြောင် ယောင် လမ်း ချူ မင် ဠု ဘွိုင် ယှဉ် ချင်း ယန်း ဝါ ရှိုင်း အိုမ် ချုံ ကျိန်း ဝန် ဇူ ခါး သော့ ရှင် ကိမ် ထောင် ဆေး ဂဲလ် ပဲန်း အောင် လိတ် ဒုံ ယို စုံ\n",
      "\n",
      "real\t0m2.194s\n",
      "user\t0m5.123s\n",
      "sys\t0m2.094s\n"
     ]
    }
   ],
   "source": [
    "!time python -u laphet.py --model_type mlp --generate --model ./model/name/mlp.model \\\n",
    "--seq_len 50 --prompt \"ရဲ\" --no_of_generation 10 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ecb66-0497-4cc5-a407-82ee6a828241",
   "metadata": {},
   "source": [
    "လူနာမည်ဆိုရင်တော့ ဝဏ္ဏအလုံး ၅၀ ဆိုတာက မဖြစ်နိုင်လို့ နှစ်လုံးလောက်ပဲ ထားပြီး text generate လုပ်ခိုင်းကြည့်ရအောင်။   \n",
    "sequence length ကို --seq_len 2 ဆိုတဲ့ setting နဲ့ run မယ်။ ရလဒ်က အောက်ပါအတိုင်းပါ။   \n",
    "တစ်ခုရှိတာက တစ်ခေါက် generate လုပ်ခိုင်းတိုင်း နာမည်အသစ်တွေကိုပဲ generate လုပ်မှာမို့ run တဲ့အခေါက်တိုင်းမှာ မတူတဲ့ syllable sequence တွေကိုပဲ ထုတ်ပေးပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d28d0d81-ede6-4381-b7e3-8d6b5d549f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ဿန်း ဆန့်\n",
      "Generated Text 2: ရဲ စိ ကြွယ်\n",
      "Generated Text 3: ရဲ ရင့် ဖီး\n",
      "Generated Text 4: ရဲ အုံ ဃာ\n",
      "Generated Text 5: ရဲ ရှိတ် ဒယ်လ်\n",
      "Generated Text 6: ရဲ လည်း ဖုံး\n",
      "Generated Text 7: ရဲ တို တိန်း\n",
      "Generated Text 8: ရဲ တေး ဒါ\n",
      "Generated Text 9: ရဲ ခင် ဘက်\n",
      "Generated Text 10: ရဲ နူး ထိုင်\n",
      "\n",
      "real\t0m1.577s\n",
      "user\t0m4.487s\n",
      "sys\t0m2.122s\n"
     ]
    }
   ],
   "source": [
    "!time python -u laphet.py --model_type mlp --generate --model ./model/name/mlp.model \\\n",
    "--seq_len 2 --prompt \"ရဲ\" --no_of_generation 10 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815325b-faca-42a7-8ca7-da7bc7672858",
   "metadata": {},
   "source": [
    "myRoman ရဲ့ ဒေတာမှာက တိုင်းရင်းသားနာမည်မျိုးစုံ ပါဝင်ပါတယ်။ အဲဒါကြောင့် ဗမာနာမည်အနေနဲ့ ကြည့်ရင် ထူးဆန်းတဲ့ နာမည်မျိုးတွေလည်း ထုတ်ပေးပါလိမ့်မယ်။ သေချာတာက MLP-based language model က အလုပ်လုပ်ပေးတာကိုတော့ တွေ့ရပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea270d8-beb8-4d9f-aaa2-579502a606d1",
   "metadata": {},
   "source": [
    "## Text Generation with Start-Word Input File\n",
    "\n",
    "အင်္ဂလိပ်လိုခေါင်းစဉ်မှာက start word လို့ပေးထားပေမဲ့ ကျွန်တော်တို့ လက်ရှိ training လုပ်ထားတာက syllable သို့မဟုတ် ဝဏ္ဏဖြတ်ပြီး လုပ်ထားတာမို့လို့ start word သတ်မှတ်ပေးထားတဲ့ ဖိုင်မှာလည်း syllable unit ဖြတ်ပေးထားမှ အဆင်ပြေပါလိမ့်မယ်။ word ဖြတ်ပြီး training လုပ်ထားတဲ့ မော်ဒယ်အတွက်ဆိုရင်တော့ word unit နဲ့ သွားပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12819b23-7bc8-4234-ad16-99a8e01b75c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ဂုံ\n",
      "Generated texts saved to ./output/name/mlp_gen_texts.txt\n",
      "\n",
      "real\t0m1.659s\n",
      "user\t0m4.534s\n",
      "sys\t0m2.167s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type mlp --generate --model ./model/name/mlp.model \\\n",
    "--seq_len 2 --input ./data/myRoman/start_names.txt --no_of_generation 5 \\\n",
    "--output ./output/name/mlp_gen_texts.txt --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2ee83-6c4b-4108-83fc-033153ccd57d",
   "metadata": {},
   "source": [
    "ထွက်လာတဲ့ output ဖိုင်ကို လေ့လာကြည့်ရအောင်။  \n",
    "--no_of_generation 5 ဆိုပြီး ထားခဲ့တာမို့လို့ input file ထဲက အစ စာလုံး တစ်ခုစီအတွက် ဥပမာ ငါးခုစီ generate လုပ်ပေးပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8f2b328-5240-4703-86b9-2061771435fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော် ပြန့် မန်\n",
      "ကျော် ဂျား ဟူး\n",
      "ကျော် ဩ ကိုက်\n",
      "ကျော် ဆိုက် ကြံ့\n",
      "ကျော် ထိုက် ချုံး\n",
      "မ မ ဆဲ တောင်း\n",
      "မ မ အိန္ဒြ မြေး\n",
      "မ မ ပုံ့ မိန်း\n",
      "မ မ ဟို့စ် ဗေး\n",
      "မ မ အဲန် ရှာမ်\n",
      "အေး ညှာ မြိုင်\n",
      "အေး တောင် တွယ်\n",
      "အေး ဇ ဆွင်\n",
      "အေး လျှံ ပျို\n",
      "အေး ရှိုင်းန်း ရှန်း\n",
      "လှ လှ ဆုမ်း လင်္ကာ\n",
      "လှ လှ ဘယ်လ် ဂျက်ခ်\n",
      "လှ လှ သန္တာ မျှား\n",
      "လှ လှ ထား ကျင့်\n",
      "လှ လှ ချင်း ခဂ္ဂါ\n",
      "ဂုံ ယု ဣန္တာ\n",
      "ဂုံ စိမ်း တိုး\n",
      "ဂုံ ညှင်း စုက္က\n",
      "ဂုံ စိုး စင်္ကြာ\n",
      "ဂုံ နောင့် ရှီ\n",
      "မြ အေး ဆွေ ဂိုး\n",
      "မြ အေး ယံ အား\n",
      "မြ အေး မျှော် ကီး\n",
      "မြ အေး ဒို အိန္ဒြ\n",
      "မြ အေး မြူ ဇမ်\n",
      "သ ထောင် စင်္ကြာ\n",
      "သ ယဉ်း ဒါ\n",
      "သ ဋေ ရှိုင်းန်း\n",
      "သ လွမ်း ငုံ\n",
      "သ အဏ္ဏ ရီ\n",
      "မောင် ဟို အောန်\n",
      "မောင် ပွင့်် အစ်\n",
      "မောင် အပ္ပ ထွာ\n",
      "မောင် မင်္ဂ ဇံ\n",
      "မောင် နဲမ်း အန်ဒ်\n",
      "မြင့် မြင့် လမ့် မြင့်\n",
      "မြင့် မြင့် ဋာ ဟို့စ်\n",
      "မြင့် မြင့် ချို လမ်\n",
      "မြင့် မြင့် ဏာ မ\n",
      "မြင့် မြင့် မ ကီး\n",
      "ရွှေ ဂျေ ဆယ်\n",
      "ရွှေ ဖား မြူ\n",
      "ရွှေ တွမ် ပြေ\n",
      "ရွှေ ရွမ်း ရိန်း\n",
      "ရွှေ နု ဟွမ်း\n",
      "အဂ္ဂ မှိုင် ခွန်\n",
      "အဂ္ဂ ရောင်းန် ဂဲလ်\n",
      "အဂ္ဂ သိဉ္စည်း သွဲ့\n",
      "အဂ္ဂ ဂိုး ပြာ\n",
      "အဂ္ဂ ခေါ ခိုင်\n",
      "ဥက္ကာ ဘဲလ် သျှမ်း\n",
      "ဥက္ကာ ဆုံး မီး\n",
      "ဥက္ကာ ဖျား နတ်\n",
      "ဥက္ကာ ဖိုက် ဇုန်\n",
      "ဥက္ကာ တွယ် ကီး\n",
      "သိင်္ဂီ နွမ် နိုး\n",
      "သိင်္ဂီ တုန်း ခြင်း\n",
      "သိင်္ဂီ ကြိုင်း ဏီ\n",
      "သိင်္ဂီ ရွတ် [UNK]\n",
      "သိင်္ဂီ သြ ရွမ်း\n",
      "မေ ထဲမ်း လော့\n",
      "မေ လီ ဝေး\n",
      "မေ သွား ဂျမ်\n",
      "မေ ကျူ ထား\n",
      "မေ ဇမ် ထော\n",
      "ခိုင် ထီး ဆောင်\n",
      "ခိုင် ကွီး ညု\n",
      "ခိုင် နည် နာ\n",
      "ခိုင် ခေ သဒ္ဒါ\n",
      "ခိုင် အန် လဒ်\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/name/mlp_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a074a9-41bf-4ee0-9606-214dc33f1698",
   "metadata": {},
   "source": [
    "input ဖိုင်ထဲမှာ ပါတဲ့ စာလုံးတွေကို မေ့နေနိုင်တာမို့ ပြန်ရိုက်ပြရရင် အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bb29c94-becf-4dfc-bb81-c9facfbf28a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော်\n",
      "မ မ\n",
      "အေး\n",
      "လှ လှ\n",
      "\n",
      "မြ အေး\n",
      "သ\n",
      "မောင်\n",
      "မြင့် မြင့်\n",
      "ရွှေ\n",
      "အဂ္ဂ\n",
      "ဥက္ကာ\n",
      "သိင်္ဂီ\n",
      "မေ\n",
      "ခိုင်\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myRoman/start_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a9f71-b1e0-4a20-8462-ee604886f5d4",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a502a-f4e8-41bd-baf0-1f3553cb6cbd",
   "metadata": {},
   "source": [
    "Testing လုပ်ချင်တာမို့လို့ --test ဆိုတဲ့ command option နဲ့ run ရပါလိမ့်မယ်။  \n",
    "test ဖိုင်ရှိတဲ့ path ကို ညွှန်းပေးရပါလိမ့်မယ်။  \n",
    "--test_file ./data/myRoman/test_name.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "921982f5-3cb7-46c9-91cd-66d80b9e42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████████████████████████████| 16/16 [00:00<00:00, 53.76it/s]\n",
      "Average Perplexity on Test Data: 1.1112\n",
      "Average Cross-Entropy on Test Data: 0.1055\n",
      "\n",
      "real\t0m1.561s\n",
      "user\t0m4.459s\n",
      "sys\t0m2.067s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type mlp --test --model ./model/name/mlp.model \\\n",
    "--test_file ./data/myRoman/test_name.txt --seq_len 50 --batch_size 64 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f4aaa-0865-4359-8093-80f5107109c1",
   "metadata": {},
   "source": [
    "Laphet LM Toolkit မှာ Perplexity ရော၊ Cross-Entropy နဲ့ရော evaluation လုပ်ကြည့်ထားပါတယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ef3f8-0277-491a-9fc9-e1bf3a84f375",
   "metadata": {},
   "source": [
    "## Bi-LSTM based Language Modeling\n",
    "\n",
    "ဒီတစ်ခါတော့ command တစ်ခုစီကို အသေးစိတ်မရှင်းတော့ဘူး။  \n",
    "--model_type ဆိုတဲ့ နေရာမှာ bilstm ကို setup လုပ်ရင်ရပါပြီ။  \n",
    "ထွက်လာမယ့် model ဖိုင်နာမည်ကိုတော့ မတူအောင်ပေးသင့်တာပေါ့။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a1136-af83-4ee3-8dad-90a9a19e7a05",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0148bf38-188b-4d12-93b1-ee46a53aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.37it/s]\n",
      "Epoch 1, Training Loss: 0.4839\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.22it/s]\n",
      "Epoch 1, Validation Loss: 0.3251\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3251\n",
      "Epoch 2/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 56.16it/s]\n",
      "Epoch 2, Training Loss: 0.3135\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.17it/s]\n",
      "Epoch 2, Validation Loss: 0.3098\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3098\n",
      "Epoch 3/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.37it/s]\n",
      "Epoch 3, Training Loss: 0.3058\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 166.79it/s]\n",
      "Epoch 3, Validation Loss: 0.3022\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3022\n",
      "Epoch 4/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 56.25it/s]\n",
      "Epoch 4, Training Loss: 0.2897\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 167.19it/s]\n",
      "Epoch 4, Validation Loss: 0.2608\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.2608\n",
      "Epoch 5/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.32it/s]\n",
      "Epoch 5, Training Loss: 0.2153\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.84it/s]\n",
      "Epoch 5, Validation Loss: 0.1764\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.1764\n",
      "Epoch 6/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.41it/s]\n",
      "Epoch 6, Training Loss: 0.1485\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 171.57it/s]\n",
      "Epoch 6, Validation Loss: 0.1105\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.1105\n",
      "Epoch 7/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 56.51it/s]\n",
      "Epoch 7, Training Loss: 0.0968\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 171.44it/s]\n",
      "Epoch 7, Validation Loss: 0.0771\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0771\n",
      "Epoch 8/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 56.37it/s]\n",
      "Epoch 8, Training Loss: 0.0698\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 171.48it/s]\n",
      "Epoch 8, Validation Loss: 0.0583\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0583\n",
      "Epoch 9/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.18it/s]\n",
      "Epoch 9, Training Loss: 0.0506\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 166.93it/s]\n",
      "Epoch 9, Validation Loss: 0.0436\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0436\n",
      "Epoch 10/10 (Training): 100%|█████████████████| 852/852 [00:15<00:00, 55.65it/s]\n",
      "Epoch 10, Training Loss: 0.0367\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 168.83it/s]\n",
      "Epoch 10, Validation Loss: 0.0349\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0349\n",
      "\n",
      "real\t2m37.717s\n",
      "user\t2m39.293s\n",
      "sys\t0m2.943s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bilstm --train --data ./data/myRoman/train_name.txt \\\n",
    "--dev_file ./data/myRoman/dev_name.txt --model ./model/name/bilstm.model --seq_len 50 \\\n",
    "--epochs 10 --batch_size 32 --lr 0.0001 \\\n",
    "--embedding_method nn.Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd6db9-564a-4483-813d-0c9829e69c55",
   "metadata": {},
   "source": [
    "## GPU Usage of Bi-LSTM LM Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d1e04d8-e198-49ab-8a1c-3b4de36ca388",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 22:38:40 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 45%   73C    P2             383W / 480W |   1957MiB / 24564MiB |     95%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         71MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       49MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    172213      C   python                                     1208MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f01d158-df75-4f62-9968-f0fed3b648a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 92M Jan 28 22:40 ./model/name/bilstm.model\n",
      "-rw-rw-r-- 1 ye ye 24K Jan 28 22:37 ./model/name/bilstm.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/name/bilstm.model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9a353-0b32-418d-b787-0f600f61227d",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "--prompt \"ရဲ\" ဆိုတဲ့ setting နဲ့ run မှာမို့လို့ \"ရဲ\" နဲ့ စတဲ့ နာမည်တွေကို generate လုပ်ပေးသွားပါလိမ့်မယ်။  \n",
    "sequence length က ၅၀ ထားထားတာမို့လို့ နာမည်အရှည်ကြီးတွေ ရိုက်ထုတ်ပေးပါလိမ့်မယ်။  \n",
    "ပြီးတော့ --no_of_generation ကို ၁၀ ထားထားတာမို့လို့ စုစုပေါင်း စာကြောင်း ၁၀ကြောင်းအနေနဲ့ generate လုပ်ပေးသွားပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a181e5c-7b17-4f6f-b6ee-ec71c4cf229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ခင် ထိန် ကူ ကြောင် ပဲန်း လန့် ဖီ ပျို ထိန် ကူ ထိန် ကြောင် ပဲန်း ဖီ ကူ ထိန် ကူ ကူ ပဲန်း ပဲန်း ပျို ပျို ကြောင် ဖီ ထိန် ကန်း ပျို ပျို ထိန် ဖီ ကူ ကြောင် ကူ ထိန် ပျို ထိန် ပဲန်း ဖီ ထိန် ထိန် ကန်း ပဲန်း ကူ ကူ ကြောင် ပျို ဖီ ဖီ ဖီ ကြောင် ကူ\n",
      "Generated Text 2: ခင် ဖီ ဖီ ဖီ ကြောင် ကန်း ကန်း ကန်း ဖီ ဖီ ကြောင် ဖီ ထိန် ထိန် ကန်း ကြောင် ဖီ ပဲန်း ထိန် ဖီ ကြောင် ထိန် ပျို ကန်း ကူ ကန်း ပျို ဖီ ကန်း ထိန် ထိန် ကြောင် ပဲန်း ပျို ထိန် ထိန် ဖီ ကြောင် ဖီ ကန်း ဖီ ပျို ပျို ကန်း ထိန် ထိန် ပဲန်း လန့် ဖီ ကြောင် ပျို\n",
      "Generated Text 3: ခင် ကန်း ဖီ ဖီ ကြောင် ကြောင် ကြောင် သျှမ်း ကန်း ဖီ ကန်း ထိန် ပျို ကူ ဖီ ကြောင် ထိန် ကြောင် လောင်း ပျို ဖီ ကြောင် ပျို ကန်း ဖီ ကန်း ဖီ ပဲန်း ထိန် ကြောင် သျှမ်း ကြောင် ဖီ ပဲန်း လန့် ထိန် ထိန် ဖီ ထိန် ထိန် ဖီ ကြောင် ထိန် ထိန် ထိန် ပဲန်း ကြောင် ကြောင် ဖီ ပျို ထိန်\n",
      "Generated Text 4: ခင် လန့် ပဲန်း ပဲန်း ထိန် ကြောင် ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ထိန် ကန်း ထိန် ကြောင် ဖီ ကြောင် ထိန် ထိန် ဖီ ကြောင် ကြောင် ကန်း ထိန် ပျို ပျို ပဲန်း ပဲန်း ကန်း ကြောင် နမ် ကန်း ဖီ ကန်း ဖီ ကန်း ဖီ ကူ ဖီ ပျို ဖီ ဖီ ဖီ ဖီ ကြောင် လန့် ဖီ ပဲန်း\n",
      "Generated Text 5: ခင် ကူ ဖီ ထိန် ကူ ကြောင် ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ကြောင် ဖီ ဖီ ကြောင် ကြောင် ထိန် ဖီ ကြောင် ကန်း ထိန် ကန်း ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ဖီ ပျို ထိန် ထိန် ပျို ပျို ထိန် ကူ ကူ ပျို ဖီ ဖီ ကူ ကူ ကြောင် ကြောင် ကန်း ဖီ ပဲန်း ကန်း ဖီ\n",
      "Generated Text 6: ခင် ဖီ ကန်း ပဲန်း ဖီ ပျို ပဲန်း ကြောင် ကြောင် ဖီ ပျို ကြောင် ဖီ ထိန် ပဲန်း ကူ ဖီ ထိန် ပဲန်း ထိန် ပျို ထိန် ကန်း ကန်း ကူ ဖီ ပဲန်း ပျို ဖီ ပျို ကူ ကြောင် ဖီ ဖီ ပဲန်း ကြောင် ထိန် ထိန် ပဲန်း ထိန် ဖီ ထိန် ပဲန်း ဖီ ကြောင် ဖီ ဖီ ထိန် လန့် ထိန် လန့်\n",
      "Generated Text 7: ခင် ထိန် ဖီ ကြောင် ကြောင် ထိန် ဖီ ကူ ပျို ပဲန်း ကြောင် ကန်း ပျို ဖီ ကန်း ကန်း ပဲန်း ပဲန်း ထိန် ဖီ လန့် ဖီ ကူ ထိန် ကြောင် လောင်း ဖီ ကြောင် လန့် ဖီ ပဲန်း ဖီ ထိန် ကူ လန့် ပဲန်း ဖီ ပဲန်း ဖီ ထိန် ပဲန်း ကန်း ထိန် ကန်း ဖီ ပျို ပျို ပဲန်း ပဲန်း ကန်း ကန်း\n",
      "Generated Text 8: ခင် ကန်း ဖီ ကြောင် လန့် ဖီ ပျို ပျို ဖီ ကန်း ဖီ ဖီ ကန်း ပဲန်း ကူ ကူ ဖီ ဖီ ဖီ ကြောင် ထိန် ကန်း ထိန် ပဲန်း လန့် လန့် ဖီ ပဲန်း ပျို လန့် ပျို ကန်း လန့် ပဲန်း ကြောင် ဖီ ဖီ ကန်း ဖီ ပဲန်း ကူ ပဲန်း ဖီ ကန်း ဖီ ထိန် ကန်း ပဲန်း ဖီ ပျို ဖီ\n",
      "Generated Text 9: ခင် ဖီ ထိန် ကူ ဖီ ကြောင် ကန်း ပျို ဖီ ထိန် ကြောင် ဖီ ကြောင် ထိန် ကန်း ဖီ ကြောင် ပဲန်း ကူ ပျို ကန်း ကြောင် ထိန် ဖီ ထိန် ဖီ ထိန် လန့် ထိန် ဖီ ထိန် ထိန် ပျို ဖီ ထိန် လန့် ဖီ ထိန် ကြောင် ဖီ ကန်း ထိန် ပဲန်း လန့် ထိန် ထိန် ထိန် ဖီ ဖီ ထိန် ကြောင်\n",
      "Generated Text 10: ခင် ပျို ထိန် ပျို ဖီ ဖီ ဖီ ကန်း ပျို ကြောင် ပျို ကူ ထိန် ဖီ ဖီ ဖီ ဖီ ဖီ ကြောင် ထိန် ကြောင် ခု ကြောင် ပျို ထိန် ဖီ ကြောင် ပဲန်း ပဲန်း ပဲန်း ကြောင် ထိန် ဖီ ထိန် ဖီ ပဲန်း ပဲန်း ပဲန်း ကန်း ပဲန်း ပျို ကြောင် ကန်း ကြောင် ထိန် ထိန် ထိန် ကန်း ကန်း ဖီ ကြောင်\n",
      "\n",
      "real\t0m2.010s\n",
      "user\t0m4.798s\n",
      "sys\t0m2.200s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bilstm --generate --model ./model/name/bilstm.model \\\n",
    "--seq_len 50 --prompt \"ခင်\" --no_of_generation 10 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02be845-1f91-46ab-a24b-278db20882ef",
   "metadata": {},
   "source": [
    "## Text Generation with Start-Word Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ff4474a-16cc-4954-8291-403e94c206fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Random Prompt Generated: ရော်\n",
      "Generated texts saved to ./output/name/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.877s\n",
      "user\t0m4.747s\n",
      "sys\t0m2.151s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bilstm --generate --model ./model/name/bilstm.model \\\n",
    "--seq_len 2 --input ./data/myRoman/start_names.txt --no_of_generation 5 \\\n",
    "--output ./output/name/bilstm_gen_texts.txt --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df0e072f-e626-4b36-8e0e-786a59e33891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော် ဖီ သူ\n",
      "ကျော် ဖီ သူ\n",
      "ကျော် ဖီ သူ\n",
      "ကျော် ဖီ သူ\n",
      "ကျော် ဖီ သူ\n",
      "မ မ ပဲန်း ကြောင်\n",
      "မ မ ပဲန်း ထိန်\n",
      "မ မ လန့် ကန်း\n",
      "မ မ ကန်း ကြောင်\n",
      "မ မ ကန်း ဖီ\n",
      "အေး ဖီ ထိန်\n",
      "အေး ထိန် ဖီ\n",
      "အေး ဖီ ထိန်\n",
      "အေး ဖီ ကြောင်\n",
      "အေး ဖီ ဖီ\n",
      "လှ လှ ပျို ထိန်\n",
      "လှ လှ လန့် ပဲန်း\n",
      "လှ လှ ကြောင် ထိန်\n",
      "လှ လှ ပဲန်း ကန်း\n",
      "လှ လှ ပျို ပဲန်း\n",
      "ရော် ကူ ကန်း\n",
      "ရော် ဖီ ကြောင်\n",
      "ရော် ပဲန်း ပဲန်း\n",
      "ရော် ပျို ကန်း\n",
      "ရော် ဖီ ပျို\n",
      "မြ အေး ဖီ ထိန်\n",
      "မြ အေး ထိန် ဖီ\n",
      "မြ အေး ဖီ ဖီ\n",
      "မြ အေး ပျို ပျို\n",
      "မြ အေး ဖီ ဖီ\n",
      "သ ကန်း ကူ\n",
      "သ ကြောင် ကူ\n",
      "သ ထိန် လန့်\n",
      "သ လန့် ထိန်\n",
      "သ ဖီ ထိန်\n",
      "မောင် ကန်း ထိန်\n",
      "မောင် ဖီ လန့်\n",
      "မောင် ဖီ ကန်း\n",
      "မောင် ဖီ ထိန်\n",
      "မောင် ထိန် ဖီ\n",
      "မြင့် မြင့် ဖီ ကန်း\n",
      "မြင့် မြင့် ဖီ ဖီ\n",
      "မြင့် မြင့် ထိန် ဖီ\n",
      "မြင့် မြင့် ပျို ပဲန်း\n",
      "မြင့် မြင့် ထိန် ဖီ\n",
      "ရွှေ ပျို ပျို\n",
      "ရွှေ ကြောင် ပဲန်း\n",
      "ရွှေ ပဲန်း ပဲန်း\n",
      "ရွှေ ပျို ဖီ\n",
      "ရွှေ ဖီ ဖီ\n",
      "အဂ္ဂ ထိန် ထိန်\n",
      "အဂ္ဂ လန့် ကူ\n",
      "အဂ္ဂ ဖီ ဖီ\n",
      "အဂ္ဂ ဖီ ကန်း\n",
      "အဂ္ဂ ဖီ လန့်\n",
      "ဥက္ကာ ကူ ကြောင်\n",
      "ဥက္ကာ ကူ ကြောင်\n",
      "ဥက္ကာ ကြောင် ထိန်\n",
      "ဥက္ကာ ထိန် ဖီ\n",
      "ဥက္ကာ လန့် ကူ\n",
      "သိင်္ဂီ ပဲန်း ပဲန်း\n",
      "သိင်္ဂီ ပျို ကန်း\n",
      "သိင်္ဂီ ကန်း ကူ\n",
      "သိင်္ဂီ ပဲန်း ကန်း\n",
      "သိင်္ဂီ ကန်း ဖီ\n",
      "မေ ဖီ ဖီ\n",
      "မေ ပဲန်း ကြောင်\n",
      "မေ ဖီ ကြောင်\n",
      "မေ ကန်း ပျို\n",
      "မေ ကန်း ဖီ\n",
      "ခိုင် ဖီ ဖီ\n",
      "ခိုင် ဖီ ဖီ\n",
      "ခိုင် ဖီ ဖီ\n",
      "ခိုင် ဖီ ဖီ\n",
      "ခိုင် ဖီ ဖီ\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/name/bilstm_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90b3e5fc-9af0-472a-b06f-219ea883e434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75  245 2756 ./output/name/bilstm_gen_texts.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./output/name/bilstm_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f9cf7-f17f-479c-bf3a-6f3dc341cba1",
   "metadata": {},
   "source": [
    "## Testing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf7084e2-528b-4b7d-9a22-befe1a5a8b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████████████████████████████| 16/16 [00:00<00:00, 41.67it/s]\n",
      "Average Perplexity on Test Data: 1.0311\n",
      "Average Cross-Entropy on Test Data: 0.0306\n",
      "\n",
      "real\t0m1.842s\n",
      "user\t0m4.742s\n",
      "sys\t0m2.124s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bilstm --test --model ./model/name/bilstm.model \\\n",
    "--test_file ./data/myRoman/test_name.txt --seq_len 50 --batch_size 64 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e762a-6f1c-4362-bc18-ce1688a3cce8",
   "metadata": {},
   "source": [
    "So far, so good! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6a616-596c-4ae1-8a88-65681aa81eb8",
   "metadata": {},
   "source": [
    "# Transformer based Language Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd9ff4-f462-4049-9dc2-e703cf046731",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8756e6df-f9c9-418d-95e8-ff55b66239a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 196.83it/s]\n",
      "Epoch 1, Training Loss: 0.2568\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 595.16it/s]\n",
      "Epoch 1, Validation Loss: 0.0440\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0440\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 210.87it/s]\n",
      "Epoch 2, Training Loss: 0.0294\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 598.45it/s]\n",
      "Epoch 2, Validation Loss: 0.0212\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0212\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 208.29it/s]\n",
      "Epoch 3, Training Loss: 0.0153\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 591.43it/s]\n",
      "Epoch 3, Validation Loss: 0.0135\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0135\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 197.80it/s]\n",
      "Epoch 4, Training Loss: 0.0097\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 603.11it/s]\n",
      "Epoch 4, Validation Loss: 0.0094\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0094\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 209.00it/s]\n",
      "Epoch 5, Training Loss: 0.0066\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.09it/s]\n",
      "Epoch 5, Validation Loss: 0.0070\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0070\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 206.42it/s]\n",
      "Epoch 6, Training Loss: 0.0046\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 572.16it/s]\n",
      "Epoch 6, Validation Loss: 0.0056\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0056\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.45it/s]\n",
      "Epoch 7, Training Loss: 0.0033\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.84it/s]\n",
      "Epoch 7, Validation Loss: 0.0047\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0047\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 211.58it/s]\n",
      "Epoch 8, Training Loss: 0.0024\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 570.27it/s]\n",
      "Epoch 8, Validation Loss: 0.0041\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0041\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 209.33it/s]\n",
      "Epoch 9, Training Loss: 0.0017\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 584.63it/s]\n",
      "Epoch 9, Validation Loss: 0.0038\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0038\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 207.80it/s]\n",
      "Epoch 10, Training Loss: 0.0012\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 604.37it/s]\n",
      "Epoch 10, Validation Loss: 0.0035\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0035\n",
      "\n",
      "real\t0m43.828s\n",
      "user\t0m46.035s\n",
      "sys\t0m2.305s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type transformer --train --data ./data/myRoman/train_name.txt \\\n",
    "--dev_file ./data/myRoman/dev_name.txt --model ./model/name/transformer.model --seq_len 50 \\\n",
    "--epochs 10 --batch_size 32 --lr 0.0001 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f5f56-f581-4fc7-84c4-4cf181fd4f14",
   "metadata": {},
   "source": [
    "## GPU Usage of Transformer based LM Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff33738b-414a-43ac-84fa-e51275a37109",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 22:45:32 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   65C    P2             346W / 480W |   1282MiB / 24564MiB |     88%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       41MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      158MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    173038      C   python                                      544MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245709b3-4251-44bd-b3bb-2d47739e1c8d",
   "metadata": {},
   "source": [
    "မော်ဒယ် အမျိုးအစားပေါ်ကို မူတည်ပြီးတော့ training လုပ်တဲ့အခါမှာ ကြာချိန်က တူမှာ မဟုတ်ပါဘူး။ လက်ရှိမှာ နာမည်ဒေတာမို့လို့ တကယ်က စာကြောင်းအနေနဲ့ တိုပါတယ်။ ပြီးတော့ သုံးထားတဲ့ GPU အမျိုးအစားနဲ့ memory အပေါ်မှာလည်း မူတည်ပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd74aa78-2b25-4d65-9781-78492f29f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 11M Jan 28 22:45 ./model/name/transformer.model\n",
      "-rw-rw-r-- 1 ye ye 24K Jan 28 22:45 ./model/name/transformer.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/name/transformer.model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cc0e7-cb02-4f27-aa0c-0d855dec84bf",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f61592e4-2e4e-41a7-a448-8de88d8263a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: အိ စန္ဒ ဟ ယှဉ် တိုင် အဲလ် အမ္မ အဲလ် ဆန်း ဗိုလ် ရင့် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် မြန် အဲလ် အဲလ် အဲလ် ထီး အဲလ် စက်ဖ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 2: အိ သင်္ကေ ဆိုင်း သဒ္ဒါ ကွီ ဂန္ဓ အဲလ် ရယ်လ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် ခွမ်း အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် အဲလ် ဆွင်း အဲလ် အဲလ် အဲလ် တိုင် ဆွင်း အဲလ် ဟေ ငါး မြန် အဲလ် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 3: အိ ဆန့် အဲလ် ဂျေ ဝ ထွဋ် ဗုတ် အဲလ် မြက် ဝန်း တွမ် အဲလ် တိုင် ဆိုင်း ဒိမ့် အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် မြန် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တန့် ခွမ်း တိုင် ဆွင်း အဲလ် အဲလ် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 4: အိ သော် ရို မျှော် နွမ်း ခန့် ဆော် အဲလ် လမ့် အဲလ် အိုက် အဲလ် အိုက် အဲလ် တိုင် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အိုက် တိုင် ဆွင်း ခွမ်း ဆွင်း တိုင် အဲလ် အိုက် အိုက် အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် ဆွင်း မြန် အဲလ် အဲလ် ဖူ တယ်လ် တိုင် ခွမ်း ခွမ်း အိုက် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 5: အိ ထူး သြ ကိုလ် အဲလ် ဇွန်း အဲလ် ပွင့် ကြည် ဒိတ် အိုက် ခွမ်း ဆွင်း အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် အိုက် အဲလ် အဲလ် အိုက် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် [PAD] အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် ဝန် ရှန် ထပ် အိုက် အဲလ် အဲလ် ဆွင်း အိုက် အဲလ် ဆွင်း အိုက်\n",
      "Generated Text 6: အိ ဗေး ချုံ စောမ် အဲလ် လှဲ ဆွင်း တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် မြန် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် ဆွင်း အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် ခွမ်း အိုက် အိုက် မြန် ဆွင်း အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အိုက်\n",
      "Generated Text 7: အိ ထယ် ကျော သဉ္ဇူ ဆိုင်း ည ရုဏ် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် [PAD] အဲလ် အဲလ် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် ဆိုင်း လတ် ကြိုင်း ဆိုင်း ပြန့် အိုက် ဆွင်း တိုင် အိုက် ခွမ်း အိုက် မြန် အဲလ် အိုက် အဲလ် အဲလ် အဲလ် ဿန်း အိုက် အဲလ် အဲလ် တိုင် အဲလ် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 8: အိ ဓု ခူ ချွန် ကျူး တိုင် အဲလ် ကပ် ပွင့် သဲ ဖြင့် ခွမ်း ခွမ်း ယျာ ဒေါင် အိုက် တိုင် အဲလ် တိုင် တု အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် တိုင် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် အဲလ် အဲလ် မိန် အိုက် အဲလ် အဲလ် အဲလ် တိုင် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 9: အိ ယူ ဝူ အဲလ် ဣန္ဒာ အိုက် အဲလ် ဆိုင် ငုံ အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် မြန် အဲလ် အဲလ် အဲလ် မြန် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် တိုင် အဲလ် အဲလ် အဲလ် ညှင်း သွန်း အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ်\n",
      "Generated Text 10: အိ ဂျီ ဇိ အိုက် ဥက္ကာ ချိ အဲလ် ဂျတ် ဆိုင်း ကျောက် ဆွင်း အဲလ် အိုက် အဲလ် အဲလ် [PAD] အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် အဲလ် [PAD] အဲလ် အဲလ် အဲလ် အဲလ် အိုက် အဲလ် တိုင် အိုက် ဆွင်း အဲလ် ရှိ အဲလ် အဲလ် အဲလ် အဲလ် ခွမ်း အဲလ်\n",
      "\n",
      "real\t0m2.261s\n",
      "user\t0m5.183s\n",
      "sys\t0m2.068s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type transformer --generate --model ./model/name/transformer.model \\\n",
    "--seq_len 50 --prompt \"အိ\" --no_of_generation 10 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afc8d0-bc9a-44fb-a889-35b6a3aba7bd",
   "metadata": {},
   "source": [
    "Transformer based LM ရဲ့ text generation ဥပမာတချို့ကတော့ အထက်မှာ မြင်ရတဲ့ အတိုင်းပါပဲ။  \n",
    "တကယ်က training ဒေတာက syllable ဖြတ်ထားတဲ့ မြန်မာနာမည်တွေမို့လို့ စာကြောင်းလိုမျိုး မရှည်ပါဘူး။  \n",
    "တမင်တကာ စာကြောင်းအရှည်ကြီး generate လုပ်ခိုင်းကြည့်တာပါ။   \n",
    "\n",
    "--seq_len ကို 2 ပဲ ထားပြီး၊ --no_of_generation ကိုလည်း 5 ပဲထားပြီး နောက်တစ်ခေါက် ထပ် run ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8d07a211-dff7-4c6d-91b4-2199c7345509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: အိ ဗေး ဖွေး\n",
      "Generated Text 2: အိ ဿဏ် အဲလ်\n",
      "Generated Text 3: အိ သိမ့် မိ\n",
      "Generated Text 4: အိ ဖောင်း အဲလ်\n",
      "Generated Text 5: အိ ဝ ဒု\n",
      "\n",
      "real\t0m1.629s\n",
      "user\t0m4.522s\n",
      "sys\t0m2.127s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type transformer --generate --model ./model/name/transformer.model \\\n",
    "--seq_len 2 --prompt \"အိ\" --no_of_generation 5 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa517816-ac28-45cb-bde9-53c5dd1464e7",
   "metadata": {},
   "source": [
    "နာမည်အသစ် generate လုပ်ကြည့်တာ မဆိုးဘူးလို့ ထင်ပါတယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979a166-e6a5-41cf-ba29-8ba736b5df71",
   "metadata": {},
   "source": [
    "## Text Generation with Start-Word Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "54738ee7-d9ba-4a7d-9aa2-f56c9f8d3417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ဟေး\n",
      "Generated texts saved to ./output/name/transformer_gen_texts.txt\n",
      "\n",
      "real\t0m1.771s\n",
      "user\t0m4.578s\n",
      "sys\t0m2.204s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type transformer --generate --model ./model/name/transformer.model \\\n",
    "--seq_len 2 --input ./data/myRoman/start_names.txt --no_of_generation 5 \\\n",
    "--output ./output/name/transformer_gen_texts.txt --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d288d-4e78-44c4-9fa7-a357a4cc052d",
   "metadata": {},
   "source": [
    "--output option သုံးပြီးတော့ ထွက်လာတဲ့ generated sentence တွေကို ဖိုင်ထဲမှာ သိမ်းခိုင်းထားတာမို့လို့ အဲဒီဖိုင်ကို cat command နဲ့ ရိုက်ထုတ်ပြခိုင်းပြီး ဘယ်လို စာကြောင်းတွေ ထွက်လာသလဲ ဆိုတာကို လေ့လာကြည့်ကြရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "acf0b272-e2ab-4dbb-a5cc-a8efbc1050c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော် သျှန် အိုး\n",
      "ကျော် ပါယ် ခေါမ်\n",
      "ကျော် ဆွန်း အဲလ်\n",
      "ကျော် ဟိမ်း အဲလ်\n",
      "ကျော် ကြယ် ခ\n",
      "မ မ ယျ လဲ\n",
      "မ မ လာန်း အိုက်\n",
      "မ မ သောင် အဲလ်\n",
      "မ မ မွန် ယဉ်\n",
      "မ မ ပါ သွဲ့\n",
      "အေး လွှမ်း ဘော်\n",
      "အေး ယန် မြန်\n",
      "အေး ခြိမ့် တိုင်\n",
      "အေး တေး အဲလ်\n",
      "အေး သိဉ္စည်း ဆွင်း\n",
      "လှ လှ တု မွင်း\n",
      "လှ လှ ဝိ [UNK]\n",
      "လှ လှ ရည် ဆောမ်း\n",
      "လှ လှ စူ မြော်\n",
      "လှ လှ လက်ျာ ချောင်း\n",
      "ဟေး ဆိုင်း ဒိုင်\n",
      "ဟေး ဆိုင်း ငုံ\n",
      "ဟေး အဲလ် ကတ္တီ\n",
      "ဟေး ဆိုင်း ခန်း\n",
      "ဟေး မြန် ထွတ်\n",
      "မြ အေး စန်း ကျီး\n",
      "မြ အေး ဓူ အဲလ်\n",
      "မြ အေး ကံ့ ကောင်း\n",
      "မြ အေး ဖေ ကုမ္မာ\n",
      "မြ အေး ခေါ ကျိမ်\n",
      "သ တယ်လ် မောင်း\n",
      "သ အိန္မာ ကြိုက်\n",
      "သ ပြန့် ထဲ\n",
      "သ တြာ အဲလ်\n",
      "သ လှန် ဆွင်း\n",
      "မောင် တည် တက္ခ\n",
      "မောင် ဝဏ် အဲလ်\n",
      "မောင် လမ့် ထော\n",
      "မောင် ရှာမ် နည်\n",
      "မောင် ကယ်လ် အဲလ်\n",
      "မြင့် မြင့် ချမ်း မာရ်\n",
      "မြင့် မြင့် စု ထင်\n",
      "မြင့် မြင့် ဂိမ် အဲလ်\n",
      "မြင့် မြင့် ကံ့ ပ\n",
      "မြင့် မြင့် ထ အိန်\n",
      "ရွှေ သဒ္ဓါ ဖုန်း\n",
      "ရွှေ ချေ ရက်\n",
      "ရွှေ တွန်း အဲလ်\n",
      "ရွှေ မီးရ် အဲလ်\n",
      "ရွှေ အမ် မြင့်\n",
      "အဂ္ဂ ဆုမ်း အဲလ်\n",
      "အဂ္ဂ ဘုဏ်း ဗန်\n",
      "အဂ္ဂ တီးလ် ခွမ်း\n",
      "အဂ္ဂ အို [PAD]\n",
      "အဂ္ဂ ဟြေ အဲလ်\n",
      "ဥက္ကာ ဖ ဘွား\n",
      "ဥက္ကာ ဒါ စက်\n",
      "ဥက္ကာ ကွန့် အိမ့်\n",
      "ဥက္ကာ ဒေါသ် လာန်း\n",
      "ဥက္ကာ ဆောင် မုန်း\n",
      "သိင်္ဂီ ဗြ တန့်\n",
      "သိင်္ဂီ ဂျမ် အိုက်\n",
      "သိင်္ဂီ တက် သိင်္ဂီ\n",
      "သိင်္ဂီ ခ ဘုန်း\n",
      "သိင်္ဂီ ခဲ ထက်\n",
      "မေ ကော အဲလ်\n",
      "မေ သတ် အဲလ်\n",
      "မေ ပြီ ဆွမ့်\n",
      "မေ ဘုတ် အဲလ်\n",
      "မေ ကော အဲလ်\n",
      "ခိုင် ညွှန်း အဲလ်\n",
      "ခိုင် မြက် ရက်\n",
      "ခိုင် ဖွား အဲလ်\n",
      "ခိုင် ရင်န် အဲလ်\n",
      "ခိုင် ဇက် လျှား\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/name/transformer_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101c6dd-e6b3-4df1-8e14-1b87867d3cb8",
   "metadata": {},
   "source": [
    "## Testing/Evaluation\n",
    "\n",
    "ဒီတခါတော့ test data ဖိုင်နဲ့ Transformer-based LM အသေးလေးကို evaluation လုပ်ခိုင်းကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ea2154e-7f50-41a6-bf8e-7b430d20de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████████████████████████████| 16/16 [00:00<00:00, 49.76it/s]\n",
      "Average Perplexity on Test Data: 1.0035\n",
      "Average Cross-Entropy on Test Data: 0.0035\n",
      "\n",
      "real\t0m1.628s\n",
      "user\t0m4.478s\n",
      "sys\t0m2.190s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type transformer --test --model ./model/name/transformer.model \\\n",
    "--test_file ./data/myRoman/test_name.txt --seq_len 50 --batch_size 64 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84c34f-0807-47a7-9c0d-cdd33213fc3c",
   "metadata": {},
   "source": [
    "## BERT based Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c855d88-d57f-4bc2-8d5e-4ef2af408b40",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e4f87f05-102c-423e-8f83-198d89482ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 189.67it/s]\n",
      "Epoch 1, Training Loss: 0.2508\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 597.03it/s]\n",
      "Epoch 1, Validation Loss: 0.0435\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0435\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.66it/s]\n",
      "Epoch 2, Training Loss: 0.0290\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 596.81it/s]\n",
      "Epoch 2, Validation Loss: 0.0212\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0212\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.33it/s]\n",
      "Epoch 3, Training Loss: 0.0152\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 604.70it/s]\n",
      "Epoch 3, Validation Loss: 0.0138\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0138\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.91it/s]\n",
      "Epoch 4, Training Loss: 0.0096\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 588.49it/s]\n",
      "Epoch 4, Validation Loss: 0.0099\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0099\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 206.82it/s]\n",
      "Epoch 5, Training Loss: 0.0066\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 567.92it/s]\n",
      "Epoch 5, Validation Loss: 0.0075\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0075\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 206.28it/s]\n",
      "Epoch 6, Training Loss: 0.0047\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 596.45it/s]\n",
      "Epoch 6, Validation Loss: 0.0059\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0059\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.81it/s]\n",
      "Epoch 7, Training Loss: 0.0033\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 581.13it/s]\n",
      "Epoch 7, Validation Loss: 0.0051\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0051\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 207.13it/s]\n",
      "Epoch 8, Training Loss: 0.0024\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 601.05it/s]\n",
      "Epoch 8, Validation Loss: 0.0045\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0045\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 204.74it/s]\n",
      "Epoch 9, Training Loss: 0.0017\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 582.15it/s]\n",
      "Epoch 9, Validation Loss: 0.0042\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0042\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 207.79it/s]\n",
      "Epoch 10, Training Loss: 0.0012\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 593.90it/s]\n",
      "Epoch 10, Validation Loss: 0.0040\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0040\n",
      "\n",
      "real\t0m43.782s\n",
      "user\t0m46.001s\n",
      "sys\t0m2.356s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --train --data ./data/myRoman/train_name.txt \\\n",
    "--dev_file ./data/myRoman/dev_name.txt --model ./model/name/bert.model --seq_len 50 \\\n",
    "--epochs 10 --batch_size 32 --lr 0.0001 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40181b-4524-4539-9149-078007cda753",
   "metadata": {},
   "source": [
    "## GPU Usage of BERT based LM Building\n",
    "\n",
    "BERT based language model ကို training လုပ်နေစဉ်မှာ nvidia-smi command ကို သုံးပြီး GPU အသုံးပြုတဲ့ အချက်အလက်တွေကို စစ်ကြည့်တော့ အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "174ac8a2-64c8-44e9-ba84-11df41671071",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 22:55:55 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   63C    P2             347W / 480W |   1286MiB / 24564MiB |     89%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       45MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      158MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    173783      C   python                                      544MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd333c50-a1ed-4c2f-aac5-126f6aa090c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 3.4M Jan 27 23:14 ./model/name/bert.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye  24K Jan 27 23:14 ./model/name/bert.ftfz.model.vocab\n",
      "-rw-rw-r-- 1 ye ye  11M Jan 28 22:56 ./model/name/bert.model\n",
      "-rw-rw-r-- 1 ye ye  24K Jan 28 22:55 ./model/name/bert.model.vocab\n",
      "-rw-rw-r-- 1 ye ye 3.4M Jan 27 23:20 ./model/name/bert.nofz.model\n",
      "-rw-rw-r-- 1 ye ye  24K Jan 27 23:20 ./model/name/bert.nofz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/name/bert*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5b40b-472d-4723-be32-b25511ce48e6",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc230982-4f76-4482-8843-99c1b5e4659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ကျက် သောက် ဇမ် အာရ် လွှင် ဇမ် သောက် ကြုံ ကြုံ သောက် ဇမ် ကြုံ အိ တီ နာ အုံ အိ ပြန် သောက် ကြုံ အိ ချိ သောက် သိဏ်း ရော့ဒ် အိ ကာလ် သိဏ်း သောက် ကျော သောက် နှော နှော အိ သွေး ပန် သောက် သိဏ်း သောက် သိဏ်း အိ သောက် သောက် သောက် ကြုံ ဇမ် ဇမ် သောက် ဇမ် နှော\n",
      "Generated Text 2: ရဲ ဂျွန်း သောက် အိ မျှား သောက် ကြုံ သောက် သောက် အိ ထောင်း သောက် သုံး သောက် ကြုံ သောက် နှော သောက် ကြုံ နှော သုံး သိဏ်း မုစ် သောက် အိ မု တေ သောက် အိ လာ အဲမ် နှော ကြုံ သောက် သောက် နှော သိဏ်း သောက် အိ ကောင် သိဏ်း သောက် သောက် သောက် အိ မြီ သောက် ဇမ် သောက် သောက် သောက်\n",
      "Generated Text 3: ရဲ အိန္ဒု အိ ဂဂ္ဂါး ဓိ စေး ကွက် သောက် ကြုံ အိ သန္တာ ခါး သောက် အိ လား ဟိုး အိ ခြယ် အိ ပိုး သောက် ဇမ် နှော နှော အိ သန်း ကွေး ကြုံ သုံး အိ ကီ သောက် ဇမ် သိဏ်း သောက် ဇမ် ဇမ် အိ ထော် သောက် သိဏ်း သောက် နှော သောက် အိ ခိ သောက် နှော သောက် အိ မြဲ\n",
      "Generated Text 4: ရဲ မီး ဆိုင် ဂတ် မိန် မှု သောက် သိဏ်း ဆို ကြိုး သုံး သိဏ်း သောက် ကျော သောက် ကြုံ သောက် ကြုံ သောက် ကြုံ ဇမ် သောက် အိ နစ် ကြုံ အိ ပျံ့ သောက် ဇမ် သောက် ဇမ် သောက် သိဏ်း သောက် နှော ကျော အိ ဘယ်လ် ကျူ ကြုံ ဇမ် ဇမ် နှော သိဏ်း သောက် သိဏ်း သောက် ကြုံ ကြုံ အိ ချို\n",
      "Generated Text 5: ရဲ ဇော် ဝိ တို ကျွယ် သိဏ်း ဇဉ် ဆွမ်း သောက် ကြုံ ကြုံ သောက် သုံး ကြုံ ကြုံ သိဏ်း သောက် သုံး သုံး သောက် အိ ဘုတ် သောက် သောက် သုံး သောက် အိ သိဉ္စည်း သောက် သောက် ကြုံ သုံး အိ စောမ် ဇမ် သောက် သောက် သိဏ်း ကွေ သောက် ကျော င သောက် သုံး သောက် ဇမ် သောက် ကြုံ သောက် သိဏ်း ဗေး\n",
      "Generated Text 6: ရဲ မို့ လုံ ဂိတ် အိ ကျား ထပ် အိ လျှမ်း သောက် နှော သောက် အိ သန္တာ ဆန့် သောက် ကျော သောက် ကြုံ ကြုံ သောက် သောက် သောက် ဇမ် ကြုံ နှော အိ အေး ဇန် သောက် အိ လှီး သုံး သုံး အိ ညွတ် ကြုံ သုံး သောက် ကျော သိဏ်း သောက် နှော သုံး သောက် နှော နှော သောက် သောက် သိဏ်း ထူး\n",
      "Generated Text 7: ရဲ အိုင်း သောက် သောက် အိ ပါယ် ချုံး ဇမ် သောက် နှော သောက် သောက် သုံး သောက် သုံး ဇမ် ဇမ် အိ နှဲမ်း ဒိမ် သောက် ဇမ် သုံး နှော သောက် သိဏ်း သောက် သောက် သောက် အိ ယိုင် အိ ချင်း အိ မယ် သောက် ကြုံ သိဏ်း ဥဂ္ဂါ သောက် သောက် သုံး သိဏ်း ခါး သောက် နှော ကြုံ ကြုံ နှော ဇမ် နှော\n",
      "Generated Text 8: ရဲ လီ ရှန်း ဂန္ဓာ သုံး နှော သောက် အိ တုန်း အိ ချိ သုံး အိ မုခ် ကြုံ သောက် အိ သာ ခါး သောက် ကြုံ သိဏ်း နှဲမ် သောက် ကျော သောက် သောက် ကျော အိ မြင့် ဆုန်း သောက် ကြုံ နှော ကြုံ ကြုံ ကျော ကြုံ သောက် သုံး သုံး သောက် သုံး ကျော သောက် သိဏ်း သောက် ဇမ် သောက် အိ ဟက်ခ်\n",
      "Generated Text 9: ရဲ ဂန္တ ဇမ် ပိုး ကွာ သောက် ဇမ် သောက် ဇမ် ကြုံ ကျော ကြုံ သောက် သောက် သုံး သိဏ်း သောက် ကြုံ ကြုံ သိဏ်း ဘယ်လ် သောက် ကြုံ သိဏ်း သောက် ဇမ် သောက် နှော သောက် သိဏ်း ထွေး ဋေ ဒုံ သိဏ်း သောက် ကျော သောက် သောက် သုံး ကြုံ ကြုံ အိ သောက် အိ ထာသ် ကြုံ ကြုံ သိဏ်း သောက် သောက် သိဏ်း\n",
      "Generated Text 10: ရဲ ဆိုင် ကြော့ သန္တာ လဲ့ ရံ အိုင်း နှော သောက် ကြုံ သောက် သိဏ်း သောက် သိဏ်း ဝိုင် သောက် ကြုံ ကျော သောက် နှော သုံး နှော သုံး ကြုံ ကြုံ နှော ကျော သောက် အိ ဝန်း သောက် သိဏ်း ရှီး သောက် နှော နှော သောက် သောက် သိဏ်း သောက် သိဏ်း သောက် သုံး သိဏ်း သောက် သောက် ကြုံ အိ ငိုင် သုံး သောက်\n",
      "\n",
      "real\t0m2.226s\n",
      "user\t0m5.088s\n",
      "sys\t0m2.155s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --generate --model ./model/name/bert.model \\\n",
    "--seq_len 50 --prompt \"ရဲ\" --no_of_generation 10 \\\n",
    "--embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48bbf4-71cc-44fd-9c9e-a7720ec04e2f",
   "metadata": {},
   "source": [
    "ဒီတခါတော့ \"ကျော်\" နဲ့ စတဲ့ နာမည်တွေကို generate လုပ်ခိုင်းကြည့်ရအောင်။ sequence length ကိုတော့ 5 ထားထားတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dff2dada-1674-4069-aeb3-dd52bf25f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ကျော် စုက္က သောက် သောက် နှော သုံး\n",
      "Generated Text 2: ကျော် ရိုင်းန် အိ အို မျိုး ကစ္စ\n",
      "Generated Text 3: ကျော် သင်း ဣန္ဒြေ သောက် သောက် နှော\n",
      "Generated Text 4: ကျော် ဟွမ် ဌေး ပြောင်း သောက် ကြုံ\n",
      "Generated Text 5: ကျော် သင့် မီ လေး ခြူး ကွယ်\n",
      "Generated Text 6: ကျော် ယ မွှန်း သောက် ကြုံ နှော\n",
      "Generated Text 7: ကျော် လောင် နွဲ့ ဒင်း သောက် သောက်\n",
      "Generated Text 8: ကျော် ထွေး အာဖ် သိဏ်း ဘော် ညွှန့်\n",
      "Generated Text 9: ကျော် အော နင် ကြုံ သောက် ဇမ်\n",
      "Generated Text 10: ကျော် ဝမ်း ချားလ် ဒို အိ ဧည့်\n",
      "\n",
      "real\t0m1.670s\n",
      "user\t0m4.541s\n",
      "sys\t0m2.150s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --generate --model ./model/name/bert.model \\\n",
    "--seq_len 5 --prompt \"ကျော်\" --no_of_generation 10 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143ea17-798f-43bc-9b17-30f53c777dba",
   "metadata": {},
   "source": [
    "ဒီတခါတော့ \"မြင့်\" နဲ့ စတဲ့ နာမည်တွေကို generate လုပ်ခိုင်းကြည့်ရအောင်။ sequence length ကိုတော့ 2 ပေးထားတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2ec6a0f3-31a1-4a8b-a6d0-da79c898307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: မြင့် ထဲ သောက်\n",
      "Generated Text 2: မြင့် ကံ့ ယဉ်း\n",
      "Generated Text 3: မြင့် မြက် ဂွိ\n",
      "Generated Text 4: မြင့် ဂူး သောက်\n",
      "Generated Text 5: မြင့် ညင် သောက်\n",
      "Generated Text 6: မြင့် လင် သော့ခ်\n",
      "Generated Text 7: မြင့် ဇင် လဲစ်\n",
      "Generated Text 8: မြင့် စင်္ကြာ ထိန်\n",
      "Generated Text 9: မြင့် မွေ ဘို\n",
      "Generated Text 10: မြင့် ပြည့် အိန္ဒု\n",
      "\n",
      "real\t0m1.659s\n",
      "user\t0m4.534s\n",
      "sys\t0m2.138s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --generate --model ./model/name/bert.model \\\n",
    "--seq_len 2 --prompt \"မြင့်\" --no_of_generation 10 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcdccb-053a-45c9-8fe0-5c4f975715f2",
   "metadata": {},
   "source": [
    "## Text Generation with Start-Word Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cce68f95-42be-4bdc-af3d-89a663446196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ကျဲရ်\n",
      "Generated texts saved to ./output/name/bert_gen_texts.txt\n",
      "\n",
      "real\t0m1.789s\n",
      "user\t0m4.697s\n",
      "sys\t0m2.152s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --generate --model ./model/name/bert.model \\\n",
    "--seq_len 2 --input ./data/myRoman/start_names.txt --no_of_generation 5 \\\n",
    "--output ./output/name/bert_gen_texts.txt --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e023e4e-09de-4853-bdb1-1c83bf594e15",
   "metadata": {},
   "source": [
    "သတ်မှတ်ပေးထားတဲ့ (prompt အနေနဲ့ စပေးထားတဲ့) နာမည်အစ စာလုံးတွေကို အခြေခံပြီး BERT LM model က အောက်ပါအတိုင်း နာမည်အသစ်တွေကို ထုတ်ပေးနိုင်ပါတယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37e8cf00-591a-4e7c-a074-6f1b6d006f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော် ဒန် ပွင့်\n",
      "ကျော် ပံ သောက်\n",
      "ကျော် သော့ခ် ခုံ\n",
      "ကျော် ဂျော် ဟေး\n",
      "ကျော် ရည် ထွေ\n",
      "မ မ ဟေး သောက်\n",
      "မ မ ဘောမ် ခြာ\n",
      "မ မ လှန် သောက်\n",
      "မ မ ချိတ် ဖွာ\n",
      "မ မ ကမ္ဘာ လုံ\n",
      "အေး ဘယ်လ် စက်ဖ်\n",
      "အေး ခို လွှင်\n",
      "အေး ကွန့် စိမ်\n",
      "အေး သျှန္တီ သောက်\n",
      "အေး အာရ် အိပ်\n",
      "လှ လှ ရည် ဖွာ\n",
      "လှ လှ ပါယ် နတ်\n",
      "လှ လှ ရွိုင် ကျော\n",
      "လှ လှ ဗုတ် ဟုန်\n",
      "လှ လှ ဝါး ကျွံ\n",
      "ကျဲရ် ယန် ထော\n",
      "ကျဲရ် မြို့ သျှန်\n",
      "ကျဲရ် ရွှန်း ခေတ်\n",
      "ကျဲရ် စင် ခယ်\n",
      "ကျဲရ် ကွတ် သောက်\n",
      "မြ အေး သောင် ဗွန်\n",
      "မြ အေး ပူး ကြုံ\n",
      "မြ အေး ဟွမ်း အိ\n",
      "မြ အေး ဒ ဆွေး\n",
      "မြ အေး ရံ ရွှန်း\n",
      "သ လင် စိမ်း\n",
      "သ ရောင် ဘူ\n",
      "သ ဘွိုင် မော\n",
      "သ အိန္မာ သောက်\n",
      "သ ကြေး ပေ\n",
      "မောင် သွေး ရှေး\n",
      "မောင် ဆုန်း မှုန်း\n",
      "မောင် ကန့် ကြုံ\n",
      "မောင် မွင်း လျင့်\n",
      "မောင် ခွေး အိ\n",
      "မြင့် မြင့် ယျန် သိဏ်း\n",
      "မြင့် မြင့် ခေါင်း ဖာ\n",
      "မြင့် မြင့် ဟောမ်း သောက်\n",
      "မြင့် မြင့် လစ် သတ်\n",
      "မြင့် မြင့် မှန် သောက်\n",
      "ရွှေ ခယ် သုံး\n",
      "ရွှေ ရောင်းန် သောက်\n",
      "ရွှေ ဆည်း နွံ\n",
      "ရွှေ ဒူး လျှန်\n",
      "ရွှေ စက္ကန့် သောက်\n",
      "အဂ္ဂ ဂျက်ခ် သောက်\n",
      "အဂ္ဂ အွမ် သင့်\n",
      "အဂ္ဂ ရွယ် ယဉ်း\n",
      "အဂ္ဂ တီး တူး\n",
      "အဂ္ဂ နီးလ် သောက်\n",
      "ဥက္ကာ ခ မြက်\n",
      "ဥက္ကာ ဂျမ် နှော\n",
      "ဥက္ကာ ဆွမ့် သိင်္ခ\n",
      "ဥက္ကာ လျှမ်း အူလ္လာ\n",
      "ဥက္ကာ ဇင်း ဖယ်\n",
      "သိင်္ဂီ ပိုက် ငိုက်\n",
      "သိင်္ဂီ ထိန် မိုင်း\n",
      "သိင်္ဂီ တိုး ခါ\n",
      "သိင်္ဂီ ဖယ် သောက်\n",
      "သိင်္ဂီ ဓ မှုန်\n",
      "မေ ညိမ့် သိဏ်း\n",
      "မေ ဩ ကိုး\n",
      "မေ ရမ်း သောက်\n",
      "မေ ကင် သန်း\n",
      "မေ တွာ ကြုံ\n",
      "ခိုင် ဘူး ပါယ်\n",
      "ခိုင် မဉ္ဇူ သျှ\n",
      "ခိုင် စံ ပီး\n",
      "ခိုင် အံ သောက်\n",
      "ခိုင် ကပ် လွင်\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/name/bert_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c643e-5a7f-4111-8a28-63fc6ee99327",
   "metadata": {},
   "source": [
    "## Testing/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "503e69a0-529d-4fcf-acbb-877bea53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████████████████████████████| 16/16 [00:00<00:00, 47.93it/s]\n",
      "Average Perplexity on Test Data: 1.0040\n",
      "Average Cross-Entropy on Test Data: 0.0040\n",
      "\n",
      "real\t0m1.630s\n",
      "user\t0m4.510s\n",
      "sys\t0m2.131s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type bert --test --model ./model/name/bert.model \\\n",
    "--test_file ./data/myRoman/test_name.txt --seq_len 50 --batch_size 64 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe44c23-4664-4316-a1b9-52c370438e4d",
   "metadata": {},
   "source": [
    "## GPT based Language Modeling\n",
    "\n",
    "ဒီတခါတော့ ChatGPT တို့လို GPT model ကို အခြေခံတဲ့ language model အသေးတစ်ခုကို ဆောက်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb4cdd-2a44-4842-81b2-fd7cacfeb668",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1a6cbd6-4b26-4f46-961a-4b8ce3ffaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 199.94it/s]\n",
      "Epoch 1, Training Loss: 0.0689\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 586.16it/s]\n",
      "Epoch 1, Validation Loss: 0.0014\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0014\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 209.32it/s]\n",
      "Epoch 2, Training Loss: 0.0010\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 593.09it/s]\n",
      "Epoch 2, Validation Loss: 0.0004\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0004\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 211.11it/s]\n",
      "Epoch 3, Training Loss: 0.0004\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 552.97it/s]\n",
      "Epoch 3, Validation Loss: 0.0002\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0002\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.10it/s]\n",
      "Epoch 4, Training Loss: 0.0002\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 570.72it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:03<00:00, 213.35it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.23it/s]\n",
      "Epoch 5, Validation Loss: 0.0001\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0001\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.34it/s]\n",
      "Epoch 6, Training Loss: 0.0001\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.65it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 209.46it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 575.26it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 211.07it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 588.34it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:03<00:00, 214.09it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 583.22it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 207.07it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 585.72it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m43.129s\n",
      "user\t0m45.435s\n",
      "sys\t0m2.343s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type gpt --train --data ./data/myRoman/train_name.txt \\\n",
    "--dev_file ./data/myRoman/dev_name.txt --model ./model/name/gpt.model --seq_len 50 \\\n",
    "--epochs 10 --batch_size 32 --lr 0.0001 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266eb7b1-93a8-477c-a83a-5f3b3a0f3169",
   "metadata": {},
   "source": [
    "## GPU Usage of GPT-based LM Building"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8da06968-60f2-439a-a3e6-cbd4f55539e8",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 23:01:09 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   64C    P2             361W / 480W |   1290MiB / 24564MiB |     89%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       50MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      158MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    174370      C   python                                      544MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e0ded0fc-0959-4b24-b5d5-76cf2631e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 11M Jan 28 23:01 ./model/name/gpt.model\n",
      "-rw-rw-r-- 1 ye ye 24K Jan 28 23:00 ./model/name/gpt.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/name/gpt.model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3306b1-d5f5-41b1-bc62-4516f5b44b10",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2e9d9395-f4da-48d7-96c5-52722609606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ငယ် ဆင်း ဘန့် ဆပ် ဟွေ ကီး ကျ အက်စ် လေး ယဉ်း နာမ် ဖန်း ဆိုင် ယု ဟုန်း စိမ် သုန် ကွယ် ရဲ မျာ ဆန္ဒ တံ့ ကဲ စောမ် အိန္ထ အောင်း ဇက် ကျိန် ခေါင် ဗို နည် သင်္ခ သန့် ဒတ် ရေ ဗဲလ် အိမ်း ရှိန်း ကင်း ဖီ ဒူး မိုရ် မေ ညု ဂိတ် ရာ ရွာ မံ ကြီး ခွါလ်\n",
      "Generated Text 2: ရဲ ယုန် တြီ ဂျော် သျှင် လွယ် လေ နွံ ကယ်လ် ကြိုင် ဆွမ့် ရွှေ မု ဗန်း နှော ရေ ခန်း အက်စ် ငွေ ဆုန် ဇန် မော လှ အုပ် ဂျို ဣ ဖီ ဟုန် သဒ္ဓါ ရ ယတ် ရစ် ညောင် ရစ် ဂိမ်း ဆွမ်း ဖူ ဒမ်း ဖိ ဗန် လက်ျာ ဿ ဖွေး စုက္က ဟက်ခ် ချိုင်း ကော့ ထော် လက် ဂန္တ ကေး\n",
      "Generated Text 3: ရဲ သဒ္ဒါ ဇိတ် လန်း တို အို စူး ပု ဣန္ဒြေ ခါး ပဲန် ရှိုး ကြောင် ထဲ ဇို မှူး ဒင် ချန်း ဥတ္တ ညှာ ဂျာ ရှည် ဆန်း ခေါ်လ် လော် ရင်း အင်း ဂျမ် ညောင် စွန်း ခြင်း ဒတ် သော တီး ထွဋ် ကွန်း ဓါန် ဆဲ လောဒ် ထဲမ်း အိန္ဒု ချင် မံ ချဲ့ ရစ္စ ခဲ လွဲ လာဒ် ဖျား နွမ် မြိုင်\n",
      "Generated Text 4: ရဲ ဘေ ကွာ လဲလ် ရီ လျော့ နေး ဆည်း မံ တုံ စီ မင်း နင်္ဂ ဟို့စ် ယျ ဒါ ဖန် မွမ်း ငန် ညို့ အိန္ဒြာ ဂိမ် ဒေါသ် ဒတ် အားလ် ဃာ သွယ် ကြွမ် ဂန် ညှင်း ရှိုင်းန်း အိပ် ကိ ကြွမ် နူး ခန် ဒိုင်း လော့ မာန် ဝဏ် ငြား ဆွန်း ဗိုင်း ငြိမ် ရိန် ခြူး လက် ရှေး တိုး ရှမ် ခမ့်\n",
      "Generated Text 5: ရဲ ဂျွန်း မှုံ ကိုင်း ဆား ဝိန် အုတ် စို ဘောက် သစ္စာ အိန္ဒာ ရား ကဉ္စ ဒူး ဗျော အဏ္ဏ လှိုင်း ယိန်း စာ ရမ် သီ ဆွတ် အိန္ဒြာ တုန်း ကျင်း ရှုံး ကြ ဇင့် ဝိုက် ချား ဖောင် ရှည် ထဲမ်း ရာ ဆိုင်း ဟ သော့ခ် ဒိတ် အဏ္ဏ ကြယ် ရပ် ဘောက် ခွန် ပြန့် နဲ စိုး သျှမ် ကိမ် မို ခေါမ်း ရိန်\n",
      "Generated Text 6: ရဲ ဘင် ရုဏ် မြဲ အိန္ဓု ဖိုက် လက်ျာ ကြော့ ပျံ့ ချယ် ဆောင် စင်း ချိတ် ရိမ် ဂိတ် ကီ နိုး ယယ် ကျင် ဆန် ရား ဟောမ်း ကွိဇ် နွေး ဒေါင်း လမ်း ကျိန်း အဂ္ဂ ဘိုင် မြဲ ယျ ကစ္စ ဇွန် ပိုး ဟမ်း အစ္စ ပေါ် လီ ဂူး ချိ လမ်း ကြို ယာန် မောင်း ကြံ ယိမ့် မှိုင်း သွယ် သိပ္ပံ တိုက် ဝူ\n",
      "Generated Text 7: ရဲ မာ နှီး ကျင် ဂိုးလ် ရော် ပါ ကွယ် ဗုတ် ဂတ် ကွန့် ဿဲ ဆမ် မှန် ဆေး ချွန် ကျူး ဖြူး ရန် ခ ရုပ် ဝံ ထိုင် ဒေါ် ဗွီ ဒစ် ဆည်း ဘိ ဆွေ နန် ထိုက် နည် ခေါက် ငွေ ငြိမ့် ရှီး စု ရစ္စ ဒင်း ယုန် စော် ဖု မှုန်း ကွေ့ အိမ် ပိုင် တည် ဠာ ဆွန်း သျှင် နွေ\n",
      "Generated Text 8: ရဲ သမ္ဘူ ဂီ ဝိုင် မန်း ဖြူး မိ မန်း ယုန် ကြောင်း အော ရော ဆားရ် ရု သျှား ဇူး ဒီ အဲန် လွင် သင့် ခွန် လွဲ သဲ ဇွန် ရှင်း တယ်လ် ကယ်လ် ရစ္စ ဆူ ဇော် တု ဆွေ ဝတ် ကိုက် မင်္ဂ ဆူ ပြုံ ငန် ဇီ ဆယ် လော့ ညင် ချိ ကျော့် တား နန္ဒာ လွိုင်း ဇုံ ဥမ္မာ အမ် ကျန်း\n",
      "Generated Text 9: ရဲ ဖွာ ကြား ကွန့် ဒီ ကြ ဟူ ခဲ မား မိုက် ချွန်း လျာ ကြော့ ညွန့် လော် ဖန့် ဓါန် သိဒ္ဓိ ရွှေ ပြည့် ဒုံ ဂျယ်လ် သင်္ခ စံ မွမ် တောင်း သန်း ဘင်း ရာ ယူး ဇေါင်း ဆုန်း ဘွမ် ပိ ညွန် အစ် ဌေး ကြောင် ဘောက် ဆ စစ် အိုင်း ရှယ် ကမ္ဘာ နာမ် ဆေး ရိုင် ဿီ ကယ် တီး ဒန်\n",
      "Generated Text 10: ရဲ မွန် ချိုင်း လဲလ် ဒစ် ငိုက် ပါ ဝတ် သုံး မှူး ဒွေး ညင် ဠု နှဲမ် ဆက် ဘယ်လ် စွန် ဆဲ လွန်း နဲမ်း ကျို လျာ ဇော ကျို တောင် နင် မှာ ရဲမ် ဣန္ဒာ တင့် ဗျက် စိမ်း တော် ဖယ် စိုင်း ပြုံး ကွေ့ ငြိမ်း ဒိ လီ ယဉ် နမ် ယယ် သျှင်း ယက် ပဲန်း အဉ္ဇူ ဖောင် ထူး ခြိမ့် ချော်\n",
      "\n",
      "real\t0m2.244s\n",
      "user\t0m5.069s\n",
      "sys\t0m2.193s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type gpt --generate --model ./model/name/gpt.model \\\n",
    "--seq_len 50 --prompt \"ရဲ\" --no_of_generation 10 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a8776-883b-43e6-97a8-98a524656bf8",
   "metadata": {},
   "source": [
    "ဒီတခါတော့ --prompt အေး ဆိုတာနဲ့ --seq_len 2 ပဲ ထားပြီး generate လုပ်ခိုင်းကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a859e36-6c50-468e-aba6-852fde9268ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: အေး ကိ ဗျက်\n",
      "Generated Text 2: အေး င ငုံ\n",
      "Generated Text 3: အေး ပြည် ဆွန်း\n",
      "Generated Text 4: အေး ငိုက် မံ\n",
      "Generated Text 5: အေး ဒင် ဆုံး\n",
      "Generated Text 6: အေး လှဲ ချစ်\n",
      "Generated Text 7: အေး မန် အံ့\n",
      "Generated Text 8: အေး အောန် ဆွိ\n",
      "Generated Text 9: အေး အော် ခြင်း\n",
      "Generated Text 10: အေး ထွေး ဂန္ဓ\n",
      "\n",
      "real\t0m1.673s\n",
      "user\t0m4.505s\n",
      "sys\t0m1.998s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type gpt --generate --model ./model/name/gpt.model \\\n",
    "--seq_len 2 --prompt \"အေး\" --no_of_generation 10 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66346331-a3d1-4dc6-a55f-5f7be3be799d",
   "metadata": {},
   "source": [
    "ဒီတစ်ခါတော့ \"မြင့်\" နဲ့ စတဲ့ နာမည်တွေကို generate လုပ်ခိုင်းကြည့်မယ်။  \n",
    "sequence length ကို 2 ထားမယ်။ ပြီးတော့ no of generation ကိုတော့ 5 ပဲ ထားမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d288bb12-9f5d-4864-bebd-1f8c8a5c147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: မြင့် စိမ် နှင်မ်\n",
      "Generated Text 2: မြင့် ရင် လော်\n",
      "Generated Text 3: မြင့် ဒ ပါ\n",
      "Generated Text 4: မြင့် စုက္က ကျွန်း\n",
      "Generated Text 5: မြင့် ပျံ့ ဆွန်း\n",
      "\n",
      "real\t0m1.685s\n",
      "user\t0m4.492s\n",
      "sys\t0m2.116s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type gpt --generate --model ./model/name/gpt.model \\\n",
    "--seq_len 2 --prompt \"မြင့်\" --no_of_generation 5 --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1694bb-cbea-4298-90ad-545eaa498878",
   "metadata": {},
   "source": [
    "## Text Generation with Start-Word Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5d391bf-f33d-478e-a10e-9cd256df5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ချို\n",
      "Generated texts saved to ./output/name/gpt_gen_texts.txt\n",
      "\n",
      "real\t0m1.862s\n",
      "user\t0m4.726s\n",
      "sys\t0m2.159s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type gpt --generate --model ./model/name/gpt.model \\\n",
    "--seq_len 2 --input ./data/myRoman/start_names.txt --no_of_generation 5 \\\n",
    "--output ./output/name/gpt_gen_texts.txt --embedding_method nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e444b-265c-40aa-a2a1-b03e605da31b",
   "metadata": {},
   "source": [
    "Save လုပ်ထားတဲ့ gpt_gen_texts.txt ဖိုင်ကို ကြည့်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a263ac2e-db03-46ac-a4e0-efab92429bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ကျော် ညင် ဆုန်\n",
      "ကျော် လာဘ် ဇောင်း\n",
      "ကျော် လုံး ကင်းမ်\n",
      "ကျော် လတ် ရွာ\n",
      "ကျော် ဆွတ် ဘု\n",
      "မ မ ဇီ ဂန္တ\n",
      "မ မ ဗွီ ထွန်း\n",
      "မ မ မြစ် ကုံး\n",
      "မ မ မြူး ဖုံး\n",
      "မ မ ရွှေ ဝင်\n",
      "အေး ကွဲ လှ\n",
      "အေး ခူး ဆွမ်း\n",
      "အေး နေ လျှမ်း\n",
      "အေး မွမ်း ဇွဲ\n",
      "အေး [UNK] သျှ\n",
      "လှ လှ မော ရီး\n",
      "လှ လှ ဆိုက် ယား\n",
      "လှ လှ ဣန္ဒာ ပေါ\n",
      "လှ လှ ရုံ ညိမ်း\n",
      "လှ လှ တောက် ဖေါ\n",
      "ချို ပိုက် မြန်\n",
      "ချို ကြယ် ဆီ\n",
      "ချို သော ခက်\n",
      "ချို ဆွန် ချောင်\n",
      "ချို တြီ မဲ့\n",
      "မြ အေး ပေ ငယ်\n",
      "မြ အေး စဲ စ\n",
      "မြ အေး ခိ အာ\n",
      "မြ အေး ခင်း စ\n",
      "မြ အေး လုံး အဲ့\n",
      "သ ခို ကိန်း\n",
      "သ ရှိမ်း လုတ်\n",
      "သ မြူး မှောင်\n",
      "သ အေး ကမ္မ\n",
      "သ အိန္ဓု ထူး\n",
      "မောင် အယ် ဇေါင်း\n",
      "မောင် စုက္က ကြင်\n",
      "မောင် ဆဲ ဟို\n",
      "မောင် အဲန် ကျန်း\n",
      "မောင် သို့ အောင်\n",
      "မြင့် မြင့် ချင်း ရော်\n",
      "မြင့် မြင့် ရွယ် တာ\n",
      "မြင့် မြင့် ကျုံ သင်္ခ\n",
      "မြင့် မြင့် ဘူး ခဲ\n",
      "မြင့် မြင့် လှေး ယိန်း\n",
      "ရွှေ စိုက် ကျူး\n",
      "ရွှေ မောင်း ခေါ\n",
      "ရွှေ လှိုင် ချိတ်\n",
      "ရွှေ ကောလ် သမ္ဘူ\n",
      "ရွှေ ခေါ လေ\n",
      "အဂ္ဂ ဆုန် စွယ်\n",
      "အဂ္ဂ စံ ဓမ္မာ\n",
      "အဂ္ဂ ဂျူး ဟင်လ်\n",
      "အဂ္ဂ စေ ဂျိုး\n",
      "အဂ္ဂ ဟုန် ရှု\n",
      "ဥက္ကာ ကစ္စ ကျား\n",
      "ဥက္ကာ စုံ အုပ်\n",
      "ဥက္ကာ ကျုံ ဟူ\n",
      "ဥက္ကာ ဇိ ဖောင်\n",
      "ဥက္ကာ ခေါ်လ် အိုး\n",
      "သိင်္ဂီ ခြူ ဆောမ်း\n",
      "သိင်္ဂီ ကြိုက် လွှင်\n",
      "သိင်္ဂီ ကိုင် ဖျား\n",
      "သိင်္ဂီ ချမ်း ကင်\n",
      "သိင်္ဂီ ယန် မှုန်\n",
      "မေ နှင်း ဖေ\n",
      "မေ လွေ မြေ့\n",
      "မေ ပန်း ဂျေ\n",
      "မေ ညှာ မြဉ္ဇူ\n",
      "မေ ညွှန်း နီးလ်\n",
      "ခိုင် ဗျာ ရွှေ\n",
      "ခိုင် နေ ပေါ်\n",
      "ခိုင် ချိတ် ခက်ခ်\n",
      "ခိုင် လွိုင်း လဲင်\n",
      "ခိုင် ကြုံ ပါ\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/name/gpt_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b39e0c-cc3e-4e73-97c8-f84e2fd65af1",
   "metadata": {},
   "source": [
    "## Bash Shell Script\n",
    "\n",
    "just for your knowledge ပါ။  \n",
    "အထက်မှာ ပြခဲ့တဲ့အတိုင်း မော်ဒယ် တစ်ခုစီကို coding လုပ်ရင်းနဲ့ စမ်းရတဲ့အခါမှာ အကြိမ်ကြိမ်အခါခါ run လိုက် coding ကို ဝင်ပြင်လိုက်လုပ်ရတာမို့လို့ command line argument တွေအတိုင်းအတာ တစ်ခုထိ သတ်မှတ်ပြီးသွားတဲ့အခါမှာ shell script ရေးလိုက်ပြီး run ပြီးမှ log တွေ၊ ထွက်လာတဲ့ output တွေကို manual ဝင်စစ်ကြည့်တာမျိုး လုပ်ခဲ့ရပါတယ်။ အဲဒီလို လုပ်မှလည်း command တွေကို ရိုက်ထည့်နေရတဲ့ အချိန်သက်သာပါတယ်။  \n",
    "\n",
    "လက်တွေ့ experiment တွေလုပ်တဲ့အခါမှာလည်း shel script ရေးတတ်ဖို့ လိုအပ်ပါတယ်။  \n",
    "\n",
    "Laphet LM Toolkit ကို coding လုပ်နေစဉ်မှာ သုံးခဲ့တဲ့ bash shell script က အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8472f0c1-9643-4b37-aae3-327db6fd453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 27 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/name/\n",
      "mkdir -p output/name/\n",
      "mkdir -p log/name/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/name/${model_type}.model\"\n",
      "  local output_file=\"./output/name/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/name/${model_type}.log\"\n",
      "  local train_data=\"./data/myRoman/train_name.txt\"\n",
      "  local dev_data=\"./data/myRoman/dev_name.txt\"\n",
      "  local test_data=\"./data/myRoman/test_name.txt\"\n",
      "  local start_name=\"./data/myRoman/start_names.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"ရဲ\" --no_of_generation 10 \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method nn.Embedding;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method nn.Embedding 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "task bilstm\n",
      "task transformer\n",
      "task bert\n",
      "task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_name.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a48326-0116-48e0-abe6-599aaeb52d90",
   "metadata": {},
   "source": [
    "## Running MLP, Bi-LSTM, Transformer, BERT and GPT Language Models Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c0629-7838-4ef2-95c2-efaaee4b49c3",
   "metadata": {},
   "source": [
    "ဒီတခါတော့ Myanmar name dataset ကိုသုံးပြီး မော်ဒယ် ၅မျိုးစလုံးရဲ့ training, text generation နဲ့ testing/evaluation အကုန် အစအဆုံးကို အထက်က shell script ကိုသုံးပြီး run ကြည့်ပါမယ်။   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b585bee2-ed89-4bf4-8287-c559ba47fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mlp language model:\n",
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 131.17it/s]\n",
      "Epoch 1, Training Loss: 1.3108\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 511.44it/s]\n",
      "Epoch 1, Validation Loss: 1.0544\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0544\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 141.19it/s]\n",
      "Epoch 2, Training Loss: 1.0530\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 506.03it/s]\n",
      "Epoch 2, Validation Loss: 1.0524\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0524\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 137.38it/s]\n",
      "Epoch 3, Training Loss: 1.0522\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 515.66it/s]\n",
      "Epoch 3, Validation Loss: 1.0520\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0520\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 137.34it/s]\n",
      "Epoch 4, Training Loss: 1.0520\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 512.93it/s]\n",
      "Epoch 4, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 135.62it/s]\n",
      "Epoch 5, Training Loss: 1.0519\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 515.43it/s]\n",
      "Epoch 5, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 136.26it/s]\n",
      "Epoch 6, Training Loss: 1.0519\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 507.49it/s]\n",
      "Epoch 6, Validation Loss: 1.0519\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0519\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 138.55it/s]\n",
      "Epoch 7, Training Loss: 1.0519\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 517.10it/s]\n",
      "Epoch 7, Validation Loss: 1.0518\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0518\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 137.69it/s]\n",
      "Epoch 8, Training Loss: 1.0518\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 518.26it/s]\n",
      "Epoch 8, Validation Loss: 1.0518\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0518\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:06<00:00, 137.82it/s]\n",
      "Epoch 9, Training Loss: 1.0518\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 494.36it/s]\n",
      "Epoch 9, Validation Loss: 1.0518\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0518\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:06<00:00, 141.30it/s]\n",
      "Epoch 10, Training Loss: 1.0518\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 521.16it/s]\n",
      "Epoch 10, Validation Loss: 1.0518\n",
      "Best model saved at ./model/name/mlp.model with validation loss: 1.0518\n",
      "\n",
      "real\t1m4.546s\n",
      "user\t1m6.807s\n",
      "sys\t0m2.270s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ယာ ပါယ် မံ ညှင်း မဲ့ ဿ ရုံး ဂွိ ငန် အဂ္ဂ ဖုန်း ယူး စဉ် ပေး နီးလ် ထပ် ငဲ နမ် မုခ် ဦး ရော ဆားရ် ဆိုင်း သြ ပပ်ဖ် ဝေး ဒူး ရှဲမ်း ထာရ် စင်္ကြာ သိင်္ဂါ ဩ ဟတ် ဂဂ္ဂါး ရှမ်း သို ဿန်း ဝါး မာရ် လ ငယ် ဒွန်း ကြား ရှဲလ် တ ဆို ဝမ် ဂုံ ကောလ် ရေ\n",
      "Generated Text 2: ရဲ ဖုဏ်း ကြား သိင်္ဂီ သိမ့် ဖြင့် ဖေါ စိုး ရှီ ဗွီ ရှိန် သျွှန်း မွန် ယက် သိမ်း ဇာ တက္ခ င လျန်း လစ် ကျုံး မြန် ကယ် နောင့် ယျန် ရွှန်း ဖိ ကီ ကြိုင် ထွန်း သင်္ကြန် ဂျူး နောင် ရိုင်းန် ဆပ် ပဲန်း အိန္မာ ကျဲရ် မွှေး မန့် မုခ် ရှမ်း ယိုင် သျှန္တီ ဒူ မြူ သိဉ္စည်း ဂျ ချင်း လောင် ခေါမ်\n",
      "Generated Text 3: ရဲ ဖြိုး ဒေ့ဗ် ဇဉ် မာ့ ကု ပစ် ဟူ ဂျာလ် သွန်း မှုန်း နှစ် ဒေါင်း ဇွန်း တိန်း ဘို့ ဝမ်း ဘွား သိုက် ဂန် အိပ် ဂျန် ဖုံး ထိ ထာရ် ဝိုင် သင်္ခ သံ ဇွန် ရိုး မဉ္ဇူ ရော့ဒ် ပြီ သည်း လှဲန် သံ ရော လန်း အိန္ထ ရိ ညို မစ် အီး ရှေး ကွဲ ဂွိ ဖောင်း ကာ ဖွား ဂျွန် ရန်\n",
      "Generated Text 4: ရဲ ခီ ပြာ မုန် ဂျော် ယတ် မု ရာဇ် ကား ဆမ် ကာလ် တော မန့် ဒို့ဗ် ဆူး ဘယ်လ် ဘီလ် ခမ့် ကြံ ဟေ သင်္ခ နံ့ နာ ဒိတ် လျှန် ခြူး ရေ့ဒ်ဗ် သိမ့် ကျီး လျှမ်း မိ ယူး ဗီ မောရ် ခူ လျှပ် ကိုလ် ဗုံ တြာ ကျန်း တင်း ဂျယ်လ် က လျှပ် ရေ ဘာ မွေး လုံး ကွယ် ပြန်း အက်\n",
      "Generated Text 5: ရဲ သျှင်း ကျန်း ဆုန် ဒေါ် လု ပြည် ဝယ် လာမ် ယိန်း စဉ် ဿန် ကစ် ဝဏ် မြူ အယ်လ် ခြင်း ဓူ တုန် ချူ ခမ့် စိန် ချွတ် ဒ လင်္ကာ အဲ ယော ထည် ကျေး လိ ဦး ဘု ယုန် လိမ္မာ ပြား ဗိုလ် စိုး စက် ဟိုး ကာလ် ယောင် ငြား တိမ်း ယိုင် မှာ ပု မို့ ဘယ် ဇက် ခြည် ယန်\n",
      "Generated Text 6: ရဲ ဧ ကြိုး ရှုံး လျှို ကြီး ဖေါ ကန် ကျောက် သန္ဒြာ စင်း သန် ဆွတ် အောင် ခြင်း ဘုန်း ဣ စောမ် ဖယ် ကြုံ ငြား ဖူး ပြည် ရွှန်း ဗိုင်း မွေ့ ကျင် ဆိုး နှောင်း မ ပြီး ဣန္တာ သစ် ကန် နွန် အိင်္ခ ကွန့် ရှာမ် ငြား ဇူး မုန်း ဂျက်ခ် တီလ် ဿန် နို အံ့ ကျွံ ရွိုင် ရှင် ပမ် အိန္တ\n",
      "Generated Text 7: ရဲ ဣန္ဒြေ နာမ် မြို့ လျှပ် ချောင်း တော ဿဲ နဲမ်း ရွာ သင်း သော့ခ် ထန် ဆိန်း တီလ် မယ် ဂဂ် ဒေါင့် ဆွန် မိုး ခြယ် အဲမ် ပန်း ဒင်း သေး ခေါ်လ် မှူး စင်္ကြာ လောင်ဝ် သျှံ ဝမ် ခင် ဂျယ်လ် နတ် မို သျှ နှင်း သေး သမ္မာ ဘိ စွမ်း ဟမ်း ပေါက် ရို ထွဏ်း မြင့့်် ရှိတ် ဝဏ္ဏ လျော့ စွန်း သေး\n",
      "Generated Text 8: ရဲ ဒတ် ဆင်း ထော မွမ် ရန်း ခန်း ဖျား အောန် ရှာမ် ဖွာ ရာ ပေါက် လွယ် ဂျတ် တိုင်း ဌေး ခြင်း တုတ် ဖိန် ဒြာ ထွန်း ဇာရ် ခေါမ် ချီ ကြွယ် ဗျက် သိုင်း သောက် ကို နှစ် လျင့် ချူး လောင်း ပွန်း ဂင့် လဲ့ ဗြ နဲမ်း ဒို့ဗ် ဒါး ဝိုင်း အန့် အဉ္စ ဟော ရင်း လဲမ့် ဆိန်း ခယ် လိုင်း ဇွန်း\n",
      "Generated Text 9: ရဲ ထန်း ညား မြင့့်် ဟင်း ဂင့် ဟမ်း ဂုန် အိုး ထော ရွှေ့ ဖြူ ဂန် ကြုတ် ခေါ် ယိမ်း ဆန့် ကျက် လို ခွန်း ဂျွန်း ဟမ် မြဲ ဟမ်း ညိမ့် ရုန်း မှုန် ဇွန်း နန္ဒီ ထော ပြန် ဉာဏ် စန်း ဟီး ကိုင် မွေ့ တင်း ကျူး ညင် ဓ ကျော် ကျစ် သွေး လဲမ့် မော ပါ့ မဲန်း သီ ကိစ္စ ရဲ ကု\n",
      "Generated Text 10: ရဲ ညား ကြွယ် ဇုန် ခန့် ရှိန်း ကျင်း ဒြာ သိန်း ဇန္ဒား ဥမ္မာ စင်း လှောင် ဗိုလ် ကွယ် ဇမ္ဗူ ရူ မို့ ဇို ဗဲလ် နန်းဒ် ဝယ် ယယ် ကျား လှဲန် ရှမ်း ကေ အိုင် ဝါလ် ဘိုင် သိင်္ခ ပုံ့ ရှုံး ခြာ လွမ်း လိမ္မာ လွန်း ဘိုင် ကိုင်း အေ ဝင် ဟွ မ လူး ကြင် ဒေါသ် ကုမ္မာ ရိ ဝံ စိန် လှောင်\n",
      "\n",
      "real\t0m2.164s\n",
      "user\t0m4.983s\n",
      "sys\t0m2.198s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: သန်\n",
      "Generated texts saved to ./output/name/mlp_gen_texts.txt\n",
      "\n",
      "real\t0m1.655s\n",
      "user\t0m4.474s\n",
      "sys\t0m2.261s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 51.95it/s]\n",
      "Average Perplexity on Test Data: 1.1110\n",
      "Average Cross-Entropy on Test Data: 0.1052\n",
      "\n",
      "real\t0m1.590s\n",
      "user\t0m4.492s\n",
      "sys\t0m2.158s\n",
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.09it/s]\n",
      "Epoch 1, Training Loss: 0.4843\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.82it/s]\n",
      "Epoch 1, Validation Loss: 0.3187\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3187\n",
      "Epoch 2/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.49it/s]\n",
      "Epoch 2, Training Loss: 0.3128\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.93it/s]\n",
      "Epoch 2, Validation Loss: 0.3079\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3079\n",
      "Epoch 3/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.60it/s]\n",
      "Epoch 3, Training Loss: 0.3053\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 166.23it/s]\n",
      "Epoch 3, Validation Loss: 0.3006\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.3006\n",
      "Epoch 4/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.71it/s]\n",
      "Epoch 4, Training Loss: 0.2867\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.15it/s]\n",
      "Epoch 4, Validation Loss: 0.2585\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.2585\n",
      "Epoch 5/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.60it/s]\n",
      "Epoch 5, Training Loss: 0.2172\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 158.61it/s]\n",
      "Epoch 5, Validation Loss: 0.1781\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.1781\n",
      "Epoch 6/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.14it/s]\n",
      "Epoch 6, Training Loss: 0.1598\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 166.69it/s]\n",
      "Epoch 6, Validation Loss: 0.1305\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.1305\n",
      "Epoch 7/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.35it/s]\n",
      "Epoch 7, Training Loss: 0.1107\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 169.13it/s]\n",
      "Epoch 7, Validation Loss: 0.0856\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0856\n",
      "Epoch 8/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.75it/s]\n",
      "Epoch 8, Training Loss: 0.0727\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 167.76it/s]\n",
      "Epoch 8, Validation Loss: 0.0573\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0573\n",
      "Epoch 9/10 (Training): 100%|██████████████████| 852/852 [00:15<00:00, 55.32it/s]\n",
      "Epoch 9, Training Loss: 0.0487\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 168.72it/s]\n",
      "Epoch 9, Validation Loss: 0.0425\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0425\n",
      "Epoch 10/10 (Training): 100%|█████████████████| 852/852 [00:15<00:00, 55.17it/s]\n",
      "Epoch 10, Training Loss: 0.0356\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 167.90it/s]\n",
      "Epoch 10, Validation Loss: 0.0340\n",
      "Best model saved at ./model/name/bilstm.model with validation loss: 0.0340\n",
      "\n",
      "real\t2m38.702s\n",
      "user\t2m40.164s\n",
      "sys\t0m2.961s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ဇေါင်း ကျ တု ခါရ် ကောင် ကောင် ကောင် ဇေါင်း ဇေါင်း တု ဇေါင်း ကောင် ဖု ဇေါင်း ဟွ ရှိတ် ကောင် ဖု ဇေါင်း ကောင် ဖု ဟွ ရှိတ် ဇေါင်း တု ခါရ် ဟွ ကောင် ဇေါင်း ကျ ဇေါင်း ဟွ ဖု ကျ ကောင် ဇေါင်း ကောင် ခါရ် ဖု ကောင် ကျ တု ကောင် တု ကျ ကောင် ဖု ဖု ကောင် တု\n",
      "Generated Text 2: ရဲ ကောင် ခါရ် ခါရ် ဟွ တု ကောင် ကျ ခါရ် တု ကောင် တု ကျ ကောင် ရှိတ် ကောင် ဇေါင်း ကောင် ဟွ ခါရ် ကောင် ကောင် ကောင် ကောင် ခါရ် ဖု ကျ ဇေါင်း ကောင် ဟွ ကျ ကောင် ဖု ကောင် ကောင် ကောင် ဇေါင်း ဖု ဇေါင်း ကျ ခါရ် ကောင် ကောင် ဇေါင်း တု ရှိတ် ဇေါင်း ဖု ဖု ရှိတ် ကျ\n",
      "Generated Text 3: ရဲ ရှိတ် ကောင် ဇေါင်း ကောင် ဟွ ကောင် ကျ ဇေါင်း ရှိတ် ကောင် တု ဇေါင်း ကောင် ခါရ် ကောင် ကောင် ကောင် ဖု ကောင် ကောင် ဖု ခါရ် ကောင် ကောင် ကောင် ခါရ် ကောင် တု ကောင် ကောင် ကောင် ကောင် ကောင် ခါရ် ကောင် ဇေါင်း ခါရ် ဇေါင်း တု တု ရှိတ် ဇေါင်း ခါရ် ကောင် ဖု ကောင် ဖု ဟွ တု ဟွ\n",
      "Generated Text 4: ရဲ ကောင် ဇေါင်း ဇေါင်း ဖု ခါရ် ကောင် ကောင် ကောင် ဇေါင်း ဖု ခါရ် ခါရ် ကောင် ကောင် ဟွ ဇေါင်း ကောင် ကောင် ကောင် ဇေါင်း ဖု ဇေါင်း ခါရ် တု ဖု တု ဟွ ဖု ဇေါင်း တု ကောင် ဖု တု ကောင် ဖု ဖု ကောင် ကောင် ခါရ် ကျ ဖု ကောင် ကောင် ကောင် ကောင် ကောင် ကောင် ကောင် ကောင် ကောင်\n",
      "Generated Text 5: ရဲ ဟွ ဖု ကောင် ဟွ ကောင် ဖု ခါရ် ကောင် ဇေါင်း ကောင် တု တု ကောင် ကျ ကျ ဇေါင်း ဖု ဟွ ဇေါင်း ဇေါင်း ကောင် ဇေါင်း ကောင် ရှိတ် ခါရ် ဇေါင်း ကောင် ကောင် ဇေါင်း ဖု ကျ ဇေါင်း ကောင် ခါရ် တု ခါရ် ဇေါင်း ခါရ် ကောင် ကောင် ခါရ် ကောင် ဇေါင်း ကောင် ကောင် ခါရ် ခါရ် ကောင် ကောင် ဇေါင်း\n",
      "Generated Text 6: ရဲ ကျ ဖု ခါရ် ကျ တု ဖု ကောင် ဖု ကောင် တု ကောင် ကောင် ကျ ဟွ ကောင် ဖု ကောင် ဖု ရှိတ် ကောင် ကောင် ဟွ ကျ ဇေါင်း တု ခါရ် ဖု ကျ ဟွ ဖု ခါရ် ကောင် ခါရ် တု တု ကောင် ခါရ် ကောင် ဇေါင်း ကောင် ဇေါင်း ဖု ခါရ် ကောင် ဇေါင်း ကောင် ကောင် ကျ တု တု\n",
      "Generated Text 7: ရဲ ကောင် ကျ ဇေါင်း ဇေါင်း ခါရ် ကျ တု ကျ ဇေါင်း ဖု ဖု ဖု ရှိတ် တု ကောင် ဖု ရှိတ် ကောင် ကောင် တု ရှိတ် ဇေါင်း ကျ ဖု တု ရှိတ် ကောင် ကျ ဇေါင်း ကောင် ဟွ ဇေါင်း တု ကောင် တု ဖု ကောင် ရှိတ် ကောင် ကျ တု ကောင် ဟွ ဖု ရှိတ် ကောင် ဇေါင်း ဇေါင်း ခါရ် ဇေါင်း\n",
      "Generated Text 8: ရဲ ကျ တု ကောင် ကောင် တု ကျ ကျ ကောင် ဟွ ကောင် ဖု ဇေါင်း ဇေါင်း ခါရ် ကောင် ဇေါင်း ခါရ် ဖု ကောင် ကောင် ကောင် ကောင် ကောင် ကောင် တု တု ခါရ် ဇေါင်း ကောင် ကောင် ခါရ် ကျ တု ဟွ တု ဇေါင်း ကောင် တု တု တု တု ကောင် ဖု ဖု ကောင် ဟွ ဇေါင်း ကောင် တု တု\n",
      "Generated Text 9: ရဲ ဖု ဇေါင်း ကောင် ခါရ် ကျ ကောင် ဇေါင်း ကျ ဇေါင်း ဟွ ကောင် ကောင် ကျ ဖု ကျ ဖု ကောင် ကောင် ကောင် ဟွ ရှိတ် ကောင် တု ဇေါင်း ဇေါင်း ဖု ကောင် ကောင် တု ရှိတ် ဖု ဇေါင်း ဖု ဖု ဇေါင်း ခါရ် ဇေါင်း ကောင် ဖု ဟွ ဖု ဇေါင်း ကျ ကျ ဟွ ဟွ ကျ တု ကောင် တု\n",
      "Generated Text 10: ရဲ ကောင် ကျ ကောင် တု ကောင် ရှိတ် ကောင် ဖု ဇေါင်း ကောင် ဇေါင်း ဖု ရှိတ် ကျ ဖု ကောင် ဖု ဇေါင်း ရှိတ် ဖု ကောင် ရှိတ် ဇေါင်း ကောင် ဟွ ရှိတ် ဇေါင်း ဟွ ကျ ကောင် ကောင် ကျ ကောင် ဇေါင်း ဇေါင်း ဇေါင်း ဇေါင်း ခါရ် ကျ ဇေါင်း ကောင် ဖု ဖု ဇေါင်း တု ဇေါင်း ဖု ဖု ကျ ကောင်\n",
      "\n",
      "real\t0m1.984s\n",
      "user\t0m4.811s\n",
      "sys\t0m2.263s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ရှဲလ်\n",
      "Generated texts saved to ./output/name/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.845s\n",
      "user\t0m4.681s\n",
      "sys\t0m2.253s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 38.91it/s]\n",
      "Average Perplexity on Test Data: 1.0300\n",
      "Average Cross-Entropy on Test Data: 0.0295\n",
      "\n",
      "real\t0m1.860s\n",
      "user\t0m4.697s\n",
      "sys\t0m2.176s\n",
      "Training Transformer language model:\n",
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 192.91it/s]\n",
      "Epoch 1, Training Loss: 0.2443\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 574.79it/s]\n",
      "Epoch 1, Validation Loss: 0.0436\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0436\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.24it/s]\n",
      "Epoch 2, Training Loss: 0.0291\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 604.84it/s]\n",
      "Epoch 2, Validation Loss: 0.0210\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0210\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 211.78it/s]\n",
      "Epoch 3, Training Loss: 0.0152\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 604.28it/s]\n",
      "Epoch 3, Validation Loss: 0.0135\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0135\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.36it/s]\n",
      "Epoch 4, Training Loss: 0.0096\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 600.49it/s]\n",
      "Epoch 4, Validation Loss: 0.0094\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0094\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 208.36it/s]\n",
      "Epoch 5, Training Loss: 0.0065\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 597.51it/s]\n",
      "Epoch 5, Validation Loss: 0.0070\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0070\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 209.76it/s]\n",
      "Epoch 6, Training Loss: 0.0046\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 574.78it/s]\n",
      "Epoch 6, Validation Loss: 0.0056\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0056\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 208.87it/s]\n",
      "Epoch 7, Training Loss: 0.0033\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 583.86it/s]\n",
      "Epoch 7, Validation Loss: 0.0047\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0047\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.89it/s]\n",
      "Epoch 8, Training Loss: 0.0024\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 585.85it/s]\n",
      "Epoch 8, Validation Loss: 0.0041\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0041\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 206.61it/s]\n",
      "Epoch 9, Training Loss: 0.0017\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 574.81it/s]\n",
      "Epoch 9, Validation Loss: 0.0038\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0038\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 210.25it/s]\n",
      "Epoch 10, Training Loss: 0.0011\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 599.05it/s]\n",
      "Epoch 10, Validation Loss: 0.0036\n",
      "Best model saved at ./model/name/transformer.model with validation loss: 0.0036\n",
      "\n",
      "real\t0m43.639s\n",
      "user\t0m46.074s\n",
      "sys\t0m2.346s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ မန်း မတ် စိုင်း လန့် သိန်း စမ်း ကြီး သင့် တုန် ဂူး သိန်း ယန်း ဆီ သွင် သွန်း ဖုံး သိန်း ကောန် တုန် သိန်း ရှင် ကေး သိန်း စင်္ကြာ ဂီ သိန်း လွန်း သိန်း အိန္တာ သိန်း ဒေါင်း သိန်း ပင် သိန်း ဖောင် ဝဏ္ဏ သိန်း ဝမ်း သိန်း ဗြ သိန်း ပေ သိန်း သဉ္ဇူ ယျာ သိန်း သတ္တိ ခွေ သိန်း ထု\n",
      "Generated Text 2: ရဲ က လမ့် ပုံ ဓိ ဒိ တုန် ဖို့ ဝဏ္ဏ ရှိုး သိန်း အိန္မာ သိန်း လှိုင်း သိန်း ဇယ် ရွတ် သိန်း ပယ် သိန်း ဠာ သိန်း လုံ သိန်း မှောင် သိန်း ဗြာ သိန်း လွန် သိန်း ပန် အဂ္ဂ တုန် ဆွေး ယျာ သိန်း တား သိန်း ဟံ သိန်း ကြောင် သိန်း စွန်း တုန် သိန်း အိန္ဒု တုန် သိန်း ချီ သိန်း ယဉ်း\n",
      "Generated Text 3: ရဲ ဇူ မှောင် သိန်း အိန္ဒြ ဂုန် သိန်း နူး ကြင်း သိန်း ရေ့ဒ်ဗ် ယျာ ကျုံး ဂျ သိန်း လောင်း ဂီ သိန်း ဂျမ်း တုန် သိန်း ဂတ် နိုင်း ဿဲ နိုင်း နောင် ကျီး တုန် အိန္တာ သိန်း တော် သိန်း ကျော ယျာ သိန်း ပု သိန်း ဂွိ သိန်း ဣန္ဒ တုန် သိန်း ဇူး ခါရ် သိန်း သော် ဟောမ်း သိန်း ယျ စက်ဖ် သိန်း\n",
      "Generated Text 4: ရဲ ဖော် သိန်း ဝယ် မိုက် တုန် ကောက် သိန်း ညွတ် ယျာ ရီ သင်္ချာ တုန် သိန်း နှင်မ် တုန် တုန် သိန်း မုန်း သိန်း ဒေါင့် ယျာ သိန်း ခဲ သိန်း ရှဲလ် တုန် သိန်း မဲန်း သိန်း ခြိမ့် ခန်း သိန်း ဟွမ်း သိန်း ဂျယ် သိန်း ယန်း သိန်း ဖေ မြင့် ဇေ သိန်း ယော် သိန်း ခတ် တုန် တုန် သိန်း အိုက် ဂီ\n",
      "Generated Text 5: ရဲ အ ဆောမ်း ဝဏ္ဏ ဂျာလ် တုန် ချောက် တုန် သိန်း ကဲ သိန်း ဒွန့် သိန်း ဟံ ခဲ သိန်း လျှပ် နိုင်း စော ခုံ တုန် သိန်း ထီး ဂီ သိန်း သမ္ဘူ သိန်း ဣန္တာ သိန်း ဟော ဝဏ္ဏ သိန်း လှိုဏ်း တုန် သိန်း လာလ် နိုင်း အင် ရွက် သိန်း ဒေါင်း တုန် သိန်း ကစ္စ သိန်း ဆုမ်း ဂီ သိန်း ဆဲ သိန်း ခူး\n",
      "Generated Text 6: ရဲ ချောင် သိန်း ရပ် တုန် နှောင်း ငြိမ့် ကြွေ သိန်း ကျိမ်း သိန်း မြော် သိန်း ချယ် ဟေး ယျာ သိန်း ကေ ရည် မြေ့ မိုင်း ဝဏ္ဏ တုန် စော ဘင်း သိန်း မှိုင် သိန်း ဂျွန် သိန်း ညွှန်း ဖီ ရောင် သိန်း လား သိန်း ရှင်း စိမ့် မို ဒိ တုန် သိန်း ယု အပ် သိန်း ခမ့် ယျာ သိန်း မန့် ရွတ် တုန်\n",
      "Generated Text 7: ရဲ ဒင် ရု သိန်း ကုန် လှဲန် သိန်း ချူ သိန်း သျှန် ထောင် တွမ်း သိန်း အားရ် သိန်း ဆူ သိန်း ဇမ်း နိုင်း သိန်း ဆုံး တုန် သိန်း အိုင် နိုင်း ချုံ သိန်း အိပ် ဝဏ္ဏ သိန်း ဆန္ဒ သိန်း ဒိန် တုန် သိန်း မွမ်း သိန်း ရိန် ချက်စ် ရွတ် သိန်း ဆာ သိန်း မား သိန်း စွဲ့ သိန်း ယတ် တုန် သိန်း ရွှန်း\n",
      "Generated Text 8: ရဲ သော် ဟံ ရှာ ဘိုင် ယျာ ဂျေ သိန်း ထွဋ် ခွမ် သိန်း အမ် သိန်း သောင်း ရယ်လ် သိန်း စဉ့် ဝဏ္ဏ ယျာ သိန်း သုန် တက္ခ ဝဏ္ဏ လည်း သိန်း ဂျယ်လ် သိန်း ဆွေး နိုင်း ငုံ ဆွမ်း သိန်း ဇေါင်း ဂီ ဝဏ္ဏ သိန်း နိုက် သိန်း ထူး စံ ဟန် မိန်း ယျာ သိန်း သိင်္ဃ ယျာ သိန်း ဗျက် တုန် သိန်း ဝါ\n",
      "Generated Text 9: ရဲ ကျဲရ် မျှား စိုး ပီ တ ကိမ် မောက် သိန်း ဝီ တား ဂီ သိန်း ဘောမ် သိန်း သဲ မို့စ် နိုင်း က အိစ် သိန်း မျှော် သိန်း ဒန့် ယျာ အဲ သိန်း လဲလ် သိန်း ဗီ နိုင်း သား သိန်း ဗျာလ် ရွတ် နိုင်း ခမ့် သိန်း တောက် သိန်း ဒင့် သိန်း လဲလ် သိန်း ယော် နိုင်း ဒယ်လ် တုန် သိန်း ဖု သိန်း\n",
      "Generated Text 10: ရဲ အောင် ကျုံ သိန်း ကျိုင်း နေး နိုင်း ကောင်း ရိန်း ဂီ ယျာ ကြံ သိန်း ထွဋ် ပွင့် ဖောင် ယျာ ဖီး တုန် သိန်း ဖော် ဂီ သိန်း ညောင် သိန်း ထူး ဟင်း တုန် သော့ သိန်း သော့ သိန်း လှိုင်း ထန်း သိန်း သွန်း သမ္မာ ဂီ သိန်း အဲ့ သိန်း ရှိုင်းန်း ဂီ သိန်း ပြိုင် ခန်း သိန်း လွန်း နိုင်း ချိန်း ဂီ\n",
      "\n",
      "real\t0m2.248s\n",
      "user\t0m5.121s\n",
      "sys\t0m2.151s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: ဖေ\n",
      "Generated texts saved to ./output/name/transformer_gen_texts.txt\n",
      "\n",
      "real\t0m1.786s\n",
      "user\t0m4.664s\n",
      "sys\t0m2.093s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 51.33it/s]\n",
      "Average Perplexity on Test Data: 1.0036\n",
      "Average Cross-Entropy on Test Data: 0.0036\n",
      "\n",
      "real\t0m1.585s\n",
      "user\t0m4.446s\n",
      "sys\t0m2.223s\n",
      "Training Bert language model:\n",
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 192.95it/s]\n",
      "Epoch 1, Training Loss: 0.2444\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 593.59it/s]\n",
      "Epoch 1, Validation Loss: 0.0435\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0435\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 208.62it/s]\n",
      "Epoch 2, Training Loss: 0.0287\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 586.46it/s]\n",
      "Epoch 2, Validation Loss: 0.0209\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0209\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.08it/s]\n",
      "Epoch 3, Training Loss: 0.0150\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 579.36it/s]\n",
      "Epoch 3, Validation Loss: 0.0134\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0134\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 203.96it/s]\n",
      "Epoch 4, Training Loss: 0.0094\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 578.43it/s]\n",
      "Epoch 4, Validation Loss: 0.0094\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0094\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 205.57it/s]\n",
      "Epoch 5, Training Loss: 0.0064\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.68it/s]\n",
      "Epoch 5, Validation Loss: 0.0070\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0070\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 208.92it/s]\n",
      "Epoch 6, Training Loss: 0.0045\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 588.98it/s]\n",
      "Epoch 6, Validation Loss: 0.0055\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0055\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 207.92it/s]\n",
      "Epoch 7, Training Loss: 0.0032\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 582.32it/s]\n",
      "Epoch 7, Validation Loss: 0.0047\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0047\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 207.14it/s]\n",
      "Epoch 8, Training Loss: 0.0023\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 582.54it/s]\n",
      "Epoch 8, Validation Loss: 0.0042\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0042\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 207.10it/s]\n",
      "Epoch 9, Training Loss: 0.0016\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.60it/s]\n",
      "Epoch 9, Validation Loss: 0.0039\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0039\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 206.27it/s]\n",
      "Epoch 10, Training Loss: 0.0011\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 588.93it/s]\n",
      "Epoch 10, Validation Loss: 0.0036\n",
      "Best model saved at ./model/name/bert.model with validation loss: 0.0036\n",
      "\n",
      "real\t0m44.045s\n",
      "user\t0m46.125s\n",
      "sys\t0m2.316s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ သူ ဂွိ ဝံ ဖိုး လွန် ပါယ် ဟမ်း ခွေး ဒိုး ဒိုး ဆွဲ ဝံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ခွေး ဝံ ဆွဲ ဆွဲ ဆွဲ ဒိုး ဆွဲ မာရ် ကံ့ ဆွဲ သျှမ်း ဆွဲ သဒ္ဓါ ဝံ ဆွဲ ဇင် လတ် ဂျွန်း ဆွဲ ဆွဲ ဆွဲ ဝံ ဝံ ဆွဲ ဆွဲ ဒိုး ဆွဲ ဆွဲ ဒိုး\n",
      "Generated Text 2: ရဲ ဇဉ် လဲလ် ဆွဲ အောန် ဆွဲ ဆွဲ ဆွဲ ခွေး ဝံ ဒိုး ဆွဲ ဆွဲ ဆွဲ ခွေး ဒိုး ဆွဲ ဒိုး ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ ဆွဲ ဒိုး ဆွဲ ဆွဲ ဝံ ဆွဲ ဒိုး ဆွဲ သျှမ်း ဆွဲ ဒိုး ခွေး ဆွဲ ဆွဲ ခွေး သျှမ်း ဆွဲ ဝံ ဆွဲ မာရ် ဆွဲ ဆွဲ ဒိုး မာရ် ဆွဲ သျှမ်း ဆွဲ ဆွဲ\n",
      "Generated Text 3: ရဲ ဘန့် ဆွဲ လဲင် ဂျွန်း မှူး ကောန် ဝံ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဝံ မာရ် ဒုံ သျှမ်း ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဒိုး ဆွဲ ဆွဲ ဒိုး ဆွဲ ဒိုး ဆွဲ ဝံ မာရ် ဆွဲ ဆွဲ ဆွဲ ဒိုး ဝံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဝံ ဝံ ဝံ\n",
      "Generated Text 4: ရဲ နေ အိဇ္ဇာ ဆွဲ အမ် ဂန္ဓ ဆွဲ ဆွဲ ဝံ ဆွဲ သျှမ်း ဒိုး ဆွဲ ဝံ ဆွဲ ခွေး မာရ် ဆွဲ ခွေး ဆွဲ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ သဒ္ဓါ ဆွဲ ဆွဲ မာရ် ဆည်း ဝံ ဆွဲ ဆွဲ ဇင် သိုင်း ဆွဲ သျှမ်း ဆွဲ သဒ္ဓါ ဆွဲ ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ သျှမ်း ဆွဲ မာရ်\n",
      "Generated Text 5: ရဲ ရ ချော ရွေ့ ဆွဲ ဇန် ဂျမ်း နိုက် သျှမ်း ဆွဲ ဝံ ဒိုး ဆွဲ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ခွေး သျှမ်း ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ခွေး ဆွဲ ဝံ ဆွဲ သျှမ်း ဝံ ဒိုး ဆွဲ မာရ် ဆွဲ မာရ် ဆွဲ ဝံ မာရ် ဆွဲ ဝံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဒိုး ဆွဲ ဝံ ဆွဲ\n",
      "Generated Text 6: ရဲ ကျ ညို ချာ ဆွဲ လွန် ညွှန်း ဒိုး ဆွဲ သျှမ်း အက္ခ ဒိုး ဆွဲ သျှမ်း ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဒိုး ဆွဲ မာရ် ဆွဲ ဆွဲ မာရ် ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဒိုး ယု ပျံ့ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဇင် သန့် ဝံ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ မာရ် ဆွဲ ဆွဲ ခွေး ဝံ\n",
      "Generated Text 7: ရဲ ကျ ကြံ ယန် ဆွဲ ဒင်း ဇင် ဖုန် မာရ် အိန္မာ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ မာရ် ဆွဲ ဒိုး ဆွဲ ခွေး ဆွဲ ဒိုး ဆွဲ ဆွဲ မာရ် ဆွဲ မာရ် ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဒိုး ဆွဲ ဒိုး ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ ဒိုး ဒိုး ဆွဲ ဆွဲ\n",
      "Generated Text 8: ရဲ ခီ သျှင်း ဇင်း ဆွဲ နယ် သျှမ်း မင် မာရ် ထွတ် ဆွဲ ဆွဲ ဒိုး ဆွဲ သျှမ်း သျှမ်း ဆွဲ ဆွဲ ဝံ ဝံ ဆွဲ ဒိုး ဆွဲ ဝံ ဝံ ဆွဲ ခွေး ဒိုး ဆွဲ ဝံ ဆွဲ ဒိုး မာရ် ဆွဲ မာရ် ဆွဲ ခွေး ဆွဲ ဆွဲ ဝံ ခွေး မာရ် မို့ ဆွဲ ဆွဲ ဆွဲ ဆွဲ သဒ္ဓါ ကြေး ဆွဲ သျှမ်း\n",
      "Generated Text 9: ရဲ ဒိမ့် ဒွတ် အိင်္ခ မာရ် ဂိမ်း ရော် ဆွဲ ဆွဲ သျှမ်း ကတ် မာရ် ဆွဲ ဝံ ဆွဲ ဒိုး ဆွဲ ဆွဲ သဒ္ဓါ သဒ္ဓါ ဆွဲ ဆွဲ သျှမ်း ဆွဲ ဆွဲ မာရ် ဆွဲ ဆွဲ ဆွဲ ဝံ ဆွဲ ဒိုး သျှမ်း ဆွဲ ဆွဲ ဒိုး ဆွဲ ဇင် ချာ ခွေး မာရ် ဆွဲ ဆွဲ ခွေး ဆွဲ ခွေး သဒ္ဓါ ဆွဲ ဆွဲ မာရ် ဝံ\n",
      "Generated Text 10: ရဲ သျှင်း ဂျ သို ဆွဲ သျှင် သျှံ ဆွဲ ဆွဲ ဝံ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဝံ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ သဒ္ဓါ ဆွဲ ဆွဲ ဆွဲ မာရ် ဆွဲ ဆွဲ ဆွဲ ဆွဲ ခွေး ခွေး ဝံ ဆွဲ သျှမ်း ဆွဲ သျှမ်း ဆွဲ သဒ္ဓါ ဝံ ဝံ ဆွဲ ဆွဲ သဒ္ဓါ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ ဆွဲ\n",
      "\n",
      "real\t0m2.257s\n",
      "user\t0m5.139s\n",
      "sys\t0m2.184s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: တြီ\n",
      "Generated texts saved to ./output/name/bert_gen_texts.txt\n",
      "\n",
      "real\t0m1.837s\n",
      "user\t0m4.689s\n",
      "sys\t0m2.199s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 48.74it/s]\n",
      "Average Perplexity on Test Data: 1.0036\n",
      "Average Cross-Entropy on Test Data: 0.0036\n",
      "\n",
      "real\t0m1.582s\n",
      "user\t0m4.487s\n",
      "sys\t0m2.158s\n",
      "Training Gpt language model:\n",
      "Epoch 1/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 196.27it/s]\n",
      "Epoch 1, Training Loss: 0.0716\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 513.36it/s]\n",
      "Epoch 1, Validation Loss: 0.0015\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0015\n",
      "Epoch 2/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 210.12it/s]\n",
      "Epoch 2, Training Loss: 0.0011\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 587.39it/s]\n",
      "Epoch 2, Validation Loss: 0.0004\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0004\n",
      "Epoch 3/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 211.59it/s]\n",
      "Epoch 3, Training Loss: 0.0004\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 575.58it/s]\n",
      "Epoch 3, Validation Loss: 0.0002\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0002\n",
      "Epoch 4/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.94it/s]\n",
      "Epoch 4, Training Loss: 0.0002\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 587.72it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|█████████████████| 852/852 [00:03<00:00, 213.73it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.06it/s]\n",
      "Epoch 5, Validation Loss: 0.0001\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0001\n",
      "Epoch 6/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.02it/s]\n",
      "Epoch 6, Training Loss: 0.0001\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 588.21it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|█████████████████| 852/852 [00:03<00:00, 214.23it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 589.06it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|█████████████████| 852/852 [00:03<00:00, 213.34it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 562.60it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|█████████████████| 852/852 [00:04<00:00, 212.21it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 32/32 [00:00<00:00, 584.54it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|████████████████| 852/852 [00:04<00:00, 212.01it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 32/32 [00:00<00:00, 590.05it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/name/gpt.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m42.945s\n",
      "user\t0m45.325s\n",
      "sys\t0m2.342s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: ရဲ ခေါင်း သို့ ဘယ်လ် ငယ် ကွန် ညီ တွယ် အိဇ္ဇာ အပ္ပ ပါရ် ပံ အိန္မာ ပြန်း ရယ် ကွပ် ခံ့ ဇော် ဗျက် ခွန် ဿီ ဘော် ရှာမ် ပွင့် မယ် မင်း ကဉ္စ ပီ ခက် ဇန္ဒား ဖု လျှပ် ဂဂ် ပိ စက် ရှဲ ဒိန်း ဝ ဂျယ်လ် ဝီ ကိန်း တီ ရက် ကိုင်း ဝါး ဓါန် ချူး ကိုလ် ခမ်း ဿ အိ\n",
      "Generated Text 2: ရဲ မြူး ပါယ် ခဏ် ဇုံ အက်စ် ငြိမ့် ဟမ် တံ့ ချွန်း မြင့့်် အစ္စ လူး ဆယ် စင်္ကြာ ကွီ မေ ညီ ယိန်း ဝင်း စင်္ကြာ ရှမ်း သျှမ် ရှီး ဂူး ပြည် သျှင် လဲန်း ဘေ ဇုန် ချင် ဝီ ဂျတ် ဗိုလ် ဒုလ် တောက် လက္ခ ရစ္စ ရှိန်း သော် ဂတ် စမ်း အီး သင့် မှောင် ဗွန် ရှိုး ဟံ ကြံ့ ဘာ အိဇ္ဇာ\n",
      "Generated Text 3: ရဲ ယှဉ် သစ် ဝင် သ အင် လျှပ် သိင်္ဃ ဒိုင်း နဲမ်း တွာ ဓု ဇယ် ခွာ မြေ အိန္နီ ခမ့် ခန် စန် ရှဲမ်း ဟူ ဝ လွင် တာ ဖော အယ် စု ကိုင်း နော် အပ် ဟွမ် နှစ် ရိန်း ကျိန်း ဏာ ဇို သန့် ဖြူ ဝဏ္ဏ သန္တာ ထိုက် ချာ ခေါင်း လှီး အို ရွှေ နှဲမ်း စဉ့် ကွာ ဝေ သစ်\n",
      "Generated Text 4: ရဲ ခြင်း ခယ် ဗန် ဒူး ရှယ် ကျော့် အိဇ္ဇာ ရို ဖုန်း ဒွပ် ကူ ဒိုင်း ရှဲလ် ယန် နမ် ဓ မွေ့ မုစ် အင်း ဆင့် ပြီ ပြီ နာမ် စွန်း ရိုး မျိုး ကြောင်း ကား ဂွိ ဓါန် ကျွန် ဗီ ဆက် အန့် မျှော် ဒု လှေ ယူး ညာ လွန် ယု နမ် လျာ ဇင့် ရိန်း ဒေ့ဗ် ပံ ဖြင့် ကျင့် တွာ\n",
      "Generated Text 5: ရဲ ဒစ် မျိုး နှင်း ဂို ဟမ် လင်း နောင့် ရှဲမ်း ထွဏ်း သိ မျက် အစ္စ ချော် ဆီး အိပ် ရိုး သွင့် ယှဉ် ငါး လှဲ ဣ ရော့ဒ် မှုန် ရိန်း ရေး ကောင်း ဒွန်း ဇ္ဈိ ခ ဒေါင် ဘောမ် အိန္ဒြေ ရံ နွဲ့ စူ မျိုး တက္ခ ကျိုင်း တန့် သိဏ်း ရင်း ဒိုး စဉ် စန္ဒီ ကိုက် ကိုလ် ဝင့် ဆမ် ထာရ် သိင်္ဂါ\n",
      "Generated Text 6: ရဲ ခက် ထု အက် တော ခွန် ဆီး ဆွမ် ကျီး အက္ခ ဋာ ထု [UNK] ခွါ ဝါး ဆောမ်း စူး လင်္ကာ ဧည့် ရှာမ် ရုတ်စ် မှောင် တက္ခ သဉ္ဇာ ဆွတ် ဗင် လွမ်း ဆွမ် ဘေး ဧည့် မှောင် ရေ နဲ ဖုန် ကယ် လှေး လင်္ကာ ကြင်း မောင်း ရပ် လဲစ် ခံ လျှား လော ဂင့် တေး သိပ္ပံ မြဲ ကျက် ရှင်း ည\n",
      "Generated Text 7: ရဲ ယူး လျံ အဲ့ သီး ဒိုင်း ရှောင် ကွယ် ညိမ့် ဦး ရှာ သန္တာ မုန်း ချယ် ပါးရ် ခွာ ခိုင် မုန် စီး ဆောမ်း ပူး ဘွဲ ဖေါ မွန်း ကျော့ ဆွမ်း ကျင့် လှီး အင်း မြန် ဘွား ခေါ်လ် သန်း အစ္စ စန် သဉ္ဇ ရှု ဖတ် ထိုက် ပယ်လ် လဲလ် ဇီ သင်္ချာ ဋေ ဂိုး မွေး နှင်မ် မှီ ယက် အံ့ ယော\n",
      "Generated Text 8: ရဲ ဿန်း ပေါ့ အုတ် သန္ဒြာ ဂေ ထက် ဂူ နန္ဒ ပြန်း ထင် ထွတ် ကျွမ်း ပြုံး ငြိမ်း ထက် ဟင် ကွက် ယို ဗေ ဘွယ် လှိုင်း ဝန် တုတ် ထွဋ် ဟဲ ရက် လွန်း ဇိတ် ဣန္ဒ အူရ် ဒွတ် ဟတ် ရှဲမ်း မ စွန် တီ အဲန် ကြော လွှင် ဆွန် သဉ္ဇာ ရိုင် နွမ်း ဖိုက် င ဒွေး ဘို ရှိတ် သီး တောက်\n",
      "Generated Text 9: ရဲ မွှေး စည်း ထဲ အုံး နာမ် နဲမ်း အောင် ဖန်း မဲလ် ရတ် ဗင် ကျေ ငြိမ့် မဲ့ ကို ရစ် ခြူး ထိ ကြုံ ရိုင်း စင် လျင် ယော် ဗဲလ် သိဏ်း ဒေါ ကျုံး ဆွတ် အိဏ် ယိန်း မွှေး ဘွား အေး မိုက် ဌေး ဘိ ဒန့် ရမ်း အမ် ပေါ် လက် ယံ တက္ခ သဉ္စာ ဂျန် ဌေး ဏ ချက်စ် ဂဲလ် ဖြိုး\n",
      "Generated Text 10: ရဲ အဲန် လိန် ကွန် ဟံ ညွန့် ဂျီ ရွှင် ပွန် ဒင် ဥ ဗီ စုံ ကာ ယာ ရွှေ့ တုံ မြေ့ ကြုံ ဂျွန်း လုံ ကျိမ်း လှိုင်း ဂျား မွှန်း ကျဲ မြိုင် ဇာရ် လို့ လှေ ယိမ့် သန့် ဒြာ ထိုက် တွေး မှာ အာ စိမ့် ဆွေး တည် ရှမ် ဇွဲ သဉ္ဇူ ပြည့် တုတ် ရွမ် ပိုး မြေ့ ဿာ ခွာ လာ\n",
      "\n",
      "real\t0m2.230s\n",
      "user\t0m5.127s\n",
      "sys\t0m2.188s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Random Prompt Generated: သု\n",
      "Generated texts saved to ./output/name/gpt_gen_texts.txt\n",
      "\n",
      "real\t0m1.782s\n",
      "user\t0m4.679s\n",
      "sys\t0m2.183s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 48.77it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.577s\n",
      "user\t0m4.485s\n",
      "sys\t0m2.158s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_name.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45fb58-9f9c-4c85-bf13-990ac3764df6",
   "metadata": {},
   "source": [
    "## GPU Usage Information\n",
    "\n",
    "လက်ရှိ ရေးထားတဲ့ Laphet LM Toolkit က Pytorch library ကို သုံးထားပါတယ်။ Neural network ကို အခြေခံတဲ့ language modeling လုပ်တာမို့လို့ GPU ပေါ်မှာ run မှ မြန်ပါလိမ့်မယ်။ တကယ်လို့ GPU မရှိရင် ဒါမှမဟုတ် GPU က စက်မှာရှိနေပေမဲ့ setup လုပ်ထားတဲ့ Python environment က အကြောင်းတခုခုကြောင့် GPU ကို ခေါ်မသုံးနိုင်ရင်တော့ CPU ပေါ်မှာပဲ Laphet က run ပေးသွားပါလိမ့်မယ်။  \n",
    "\n",
    "အထက်က Shell script ကို run နေစဉ်မှာ GPU usage ကို တချက် တချက် စစ်ကြည့်တော့ အောက်ပါအတိုင်းပါ။ ဆောက်နေတဲ့ မော်ဒယ်အမျိုးအစား(i.e. neural network architecture) နဲ့ ဒေတာပမာဏ အပေါ်မူတည်ပြီး GPU Usage က ကွာပါလိမ့်မယ်။ "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf2817ff-2653-4fcd-b2d1-bacfb4ea62c1",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Wed Jan 29 00:04:08 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 34%   64C    P2             243W / 480W |   1101MiB / 24564MiB |     76%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         27MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       25MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    178662      C   python                                      418MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Wed Jan 29 00:04:25 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 38%   71C    P2             384W / 480W |   1891MiB / 24564MiB |     95%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         27MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       25MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    178913      C   python                                     1208MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Wed Jan 29 00:07:09 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 51%   75C    P2             362W / 480W |   1249MiB / 24564MiB |     87%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          246MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         28MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome       113MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       46MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    179248      C   python                                      544MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919e3d2-2bd2-4ca1-83f3-5787faa41c40",
   "metadata": {},
   "source": [
    "## FYI\n",
    "\n",
    "ဒီ notebook ကို အစကနေ အဆုံးထိ သေချာဖတ်လာတယ် ဆိုရင်တော့ NLP, AI အနည်းဆုံးတော့ မြန်မာစာကို သုံးပြီး language model ဆောက်တာနဲ့ ပတ်သက်ပြီး စိတ်ဝင်စားတယ်လို့ ယူဆပါတယ်။ \n",
    "\n",
    "တခု သိစေချင်တာက Laphet LM Toolkit က NLP/AI fundamental education အတွက် ရည်ရွယ်ပြီး တပတ်လောက်နဲ့ အကြမ်းပြီးအောင် ရေးထားတာမို့လို့ production အတွက် မသုံးစေချင်ပါဘူး။ Language modeling နဲ့ ပတ်သက်ပြီး ပထမဆုံး လေ့လာတဲ့သူတွေအတွက် လွယ်ကူအောင် တတ်နိုင်သမျှ ရှင်းရှင်းလေးရေးထားတာမို့ပါ။ အဲဒါတောင်မှ တကယ်တမ်း ရေးထားတဲ့ Laphet LM code အကုန် နဲ့ background theory ကို ထဲထဲဝင်ဝင် နားလည်ဖို့ ဆိုရင် ကျောင်းသားပေါ်မူတည်ပြီး သုံးလ၊ လေးလ ကြာသွားနိုင်ပါတယ်။ အနည်းဆုံးတော့ မြန်မာကျောင်းသားတွေအတွက်က မြန်မာနာမည်တွေကို သုံးပြီးတော့ မော်ဒယ်ဆောက်ပြ၊ စာကြောင်း အသစ်တွေကို generate လုပ်ပြ၊ testing/evaluation လုပ်ပြထားတာမို့လို့ ဒီ example notebook ကို အခြေခံပြီး NLP/AI သုတေသန အတွက် အရေးကြီးတဲ့ language modeling ဆိုတာကို နည်းနည်းပါးပါး တီးမိခေါက်မိရှိသွားရင်ပဲ ဝမ်းသာပါတယ်လို့။  \n",
    "\n",
    "ရဲကျော်သူ  \n",
    "LU Lab., Myanmar  \n",
    "27 Jan 2025  \n",
    "Email: ykt.nlp.ai@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c9f08-0200-4c1b-88d1-d68065695b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
