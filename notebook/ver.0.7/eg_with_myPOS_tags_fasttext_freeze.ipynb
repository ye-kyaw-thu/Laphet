{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7b317b-b675-418a-b141-e50c931ec763",
   "metadata": {},
   "source": [
    "# Laphet (Version 0.7) with myPOS Tags (fasttext embedding, freeze)\n",
    "\n",
    "ဒီ notebook မှာတော့ fasttext embedding, freeze နဲ့ training, text generation and testing လုပ်ပြသွားပါမယ်။  \n",
    "အရင်ဆုံး myPOS ရဲ့ tag ဒေတာကို သုံးပြီး fasttext embedding model ဆောက်ပါမယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad40283-c884-4c8f-ab91-00c21191ddd7",
   "metadata": {},
   "source": [
    "## fasttext Model Building with Tags\n",
    "\n",
    "ကိုယ်ရဲ့ စက်ထဲမှာတော့ fasttext command ကို သုံးလို့ ရအောင် ကြို installation လုပ်ထားပါ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6efd07d-afce-46f0-b187-f50ec0232a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fasttext <command> <args>\n",
      "\n",
      "The commands supported by fasttext are:\n",
      "\n",
      "  supervised              train a supervised classifier\n",
      "  quantize                quantize a model to reduce the memory usage\n",
      "  test                    evaluate a supervised classifier\n",
      "  test-label              print labels with precision and recall scores\n",
      "  predict                 predict most likely labels\n",
      "  predict-prob            predict most likely labels with probabilities\n",
      "  skipgram                train a skipgram model\n",
      "  cbow                    train a cbow model\n",
      "  print-word-vectors      print word vectors given a trained model\n",
      "  print-sentence-vectors  print sentence vectors given a trained model\n",
      "  print-ngrams            print ngrams given a trained model and word\n",
      "  nn                      query for nearest neighbors\n",
      "  analogies               query for analogies\n",
      "  dump                    dump arguments,dictionary,input/output vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!~/tool/fastText-0.9.2/fasttext --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c706e-fd39-4721-8e78-f07974cb1364",
   "metadata": {},
   "source": [
    "myPOS training ဒေတာကို သုံးပြီး မော်ဒယ် ဆောက်ပါမယ်။  \n",
    "POS tag တွေမို့လို့ မြန်မာစာလုံးတွေလို့ subword ကိစ္စတွေကိုပါ မစဉ်းစားပဲ default setting တွေနဲ့ပဲသွားပါမယ်။  \n",
    "dimension ကိုတော့ 100 ပဲ ထားပါမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba289cf-2d09-4ad4-8c44-be9f69049678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a172883-789c-4339-871c-c380a0219a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_tag.txt  test_tag.txt  train_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/myPOS/tag/fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b325bf-e5ef-4e8d-a4d5-b453f3702da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/data/myPOS/tag/fasttext\n"
     ]
    }
   ],
   "source": [
    "%cd data/myPOS/tag/fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b84c1c0e-46a5-4747-9e5c-62a1b07c3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  16\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread: 2323651 lr:  0.000000 avg.loss:  2.659970 ETA:   0h 0m 0s\n",
      "\n",
      "real\t0m0.952s\n",
      "user\t0m1.652s\n",
      "sys\t0m0.511s\n"
     ]
    }
   ],
   "source": [
    "!time ~/tool/fastText-0.9.2/fasttext skipgram -input ./train_tag.txt -output mypos.tag.100 -dim 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214be639-7722-4d22-8fee-c21cf4d1be62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 763M Jan 28 18:47 ./mypos.tag.100.bin\n",
      "-rw-rw-r-- 1 ye ye  15K Jan 28 18:47 ./mypos.tag.100.vec\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./mypos.tag.100.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a278a1-198f-48b4-9c58-0a2229a812b4",
   "metadata": {},
   "source": [
    "## Info of fasttext Model\n",
    "\n",
    "fasttext model ဆောက်လိုက်ရင် output က အထက်မှာ မြင်ရတဲ့အတိုင်း binary ဖိုင်တစ်ဖိုင်နဲ့ vector ဖိုင်တစ်ဖိုင် စုစုပေါင်း နှစ်ဖိုင်ရလာပါလိမ့်မယ်။ vector ဖိုင်က text ဖိုင်မို့လို့ လေ့လာကြည့်လို့ ရပါတယ်။ myPOS tag အရေအတွက်အတိုင်းပဲ vocab က ရှိတာမို့လို့ အကုန်ရိုက်ထုတ်ကြည့်ကြရအောင်။    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b615987-fab2-48ee-8a58-265f934f05d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t16 100\n",
      "     2\tpart -0.0027827 0.17253 -0.077405 -0.15206 0.0050667 0.099576 -0.17478 -0.16456 -0.010718 0.053749 -0.058972 0.015828 -0.044261 0.001468 -0.02014 -0.0094892 -0.039988 -0.088841 0.053043 -0.0049294 -0.073834 0.12506 0.12769 0.057253 -0.018244 0.13777 0.096009 0.0075117 -0.089836 -0.051383 -0.074525 0.034393 -0.097127 0.01354 0.039181 0.038902 -0.016387 -0.057132 0.032407 0.030399 -0.18997 -0.015742 -0.027703 0.097702 -0.031372 0.02566 0.022331 -0.126 -0.095041 0.16848 -0.17758 0.01202 -0.080799 -0.084911 0.04629 0.15063 0.0064536 0.1056 0.12741 -0.030796 -0.14447 -0.00055489 0.025879 -0.10228 0.043743 -0.038949 -0.05012 -0.036213 -0.102 -0.094976 -0.0054511 0.028801 0.045814 -0.019863 -0.049478 -0.011332 0.095801 -0.11725 0.08028 -0.14976 -0.052907 -0.04801 0.15445 -0.043885 0.43745 -0.032207 -0.19298 0.17952 -0.23702 -0.0078286 -0.083832 0.045821 0.049841 0.097393 -0.058102 0.0027715 -0.071128 0.13154 0.012659 -0.049212 \n",
      "     3\tn -0.033405 0.13638 -0.024758 -0.12205 0.070657 0.08329 -0.095322 -0.10341 -0.18525 0.21451 -0.14775 -0.029931 -0.16838 -0.019771 -0.2987 -0.024603 -0.048272 -0.16505 0.032585 -0.0078818 -0.0025618 0.18921 0.16083 0.036212 0.0012991 0.10259 0.042042 0.001639 -0.038581 -0.08743 -0.080324 0.058424 -0.06754 0.081511 -0.025681 0.017241 -0.0044451 -0.10541 -0.063309 0.081767 -0.22891 -0.063167 0.031135 0.091047 0.12549 -0.16594 0.032247 0.00014128 -0.1302 0.11493 -0.111 -0.060934 -0.10413 -0.098492 -0.046811 0.095419 -0.067833 0.028171 0.16698 -0.11747 -0.025532 -0.010938 0.076135 -0.037259 0.021633 -0.050549 -0.16552 -0.065091 -0.20921 -0.13257 -0.036174 0.1234 0.040868 -0.058729 -0.068318 -0.045158 0.043811 -0.032027 0.10531 -0.05826 -0.058637 -0.010608 0.056286 -0.037639 0.22565 -0.10436 -0.050712 -0.01926 -0.10309 0.036391 -0.037342 0.04475 -0.010618 0.045 -0.058183 0.0034496 -0.088826 0.028311 0.041504 -0.1195 \n",
      "     4\tppm -0.0066014 0.10249 -0.045525 -0.073649 0.031833 0.051487 -0.077324 -0.069984 -0.090363 0.13293 -0.088224 0.005747 -0.096495 -0.0039185 -0.14496 -0.023674 -0.036439 -0.1434 0.055752 -0.0096331 -0.065541 0.18399 0.18218 0.052731 -0.0096563 0.15213 0.10071 0.00053472 -0.10421 -0.093824 -0.11175 0.067267 -0.11769 0.073497 0.013086 0.052045 0.0026106 -0.087765 -0.012672 0.047942 -0.19243 -0.057854 0.0087238 0.080484 0.064464 -0.1123 0.035043 -0.051311 -0.10979 0.15234 -0.1339 -0.051617 -0.096982 -0.094073 -0.0019183 0.10563 -0.033377 0.057295 0.13697 -0.088304 -0.11316 -0.016078 0.085357 -0.074449 0.0070606 -0.041604 -0.14159 -0.061129 -0.15424 -0.10911 -0.0067063 0.083767 0.053521 -0.033371 -0.06439 -0.02103 0.069651 -0.062392 0.11747 -0.089222 -0.041338 -0.024881 0.096224 -0.032934 0.35218 -0.082636 -0.1245 0.092929 -0.10853 -0.00069368 -0.044559 0.019356 0.021099 0.05277 -0.016811 0.0074863 -0.068397 0.1064 -0.008627 -0.013229 \n",
      "     5\tv -0.029612 0.15627 0.0081315 -0.17998 -0.037678 0.081159 -0.15304 -0.19706 -0.10699 0.17936 -0.12312 0.019469 -0.18228 -0.086964 -0.17731 0.0038255 -0.062573 -0.114 0.11348 -0.0028073 -0.17321 0.21895 0.21894 0.091799 -0.020527 0.21817 0.14935 0.024947 -0.16222 -0.074395 -0.10198 0.038099 -0.09669 -0.024501 0.066442 0.038573 -0.039604 -0.021373 0.044127 -0.0075396 -0.10072 0.018729 -0.010274 0.048175 -0.0227 -0.010789 0.0072037 -0.11373 -0.11396 0.16407 -0.16736 -0.0049478 -0.087736 -0.097021 0.026414 0.15474 -0.0038304 0.067072 0.083624 -0.018171 -0.14548 -0.027064 0.060654 -0.094657 0.058608 -0.04681 -0.14907 -0.097661 -0.2206 -0.1423 -0.020976 0.092609 0.07678 -0.012885 -0.057668 0.0027991 0.071236 -0.08703 0.071527 -0.11071 -0.020322 -0.01601 0.087462 -0.0191 0.20853 -0.041442 -0.081024 0.077912 -0.059973 -0.0040369 -0.020379 0.0021642 0.017493 0.046814 -0.014436 0.0062278 -0.042786 0.062153 -0.016177 0.0063015 \n",
      "     6\tpunc -0.020472 -0.099195 0.078119 0.04097 -0.065099 -0.0106 0.063599 -0.026656 0.015997 -0.035474 0.041906 0.021984 0.0098559 -0.10233 0.057051 0.014734 -0.087539 -0.1696 0.091592 0.012016 -0.13423 0.30717 0.28754 0.045907 0.0077524 0.25792 0.12436 -0.010933 -0.13903 -0.072865 -0.10328 0.089518 -0.16204 0.060457 0.048911 0.082944 -0.082075 -0.097682 -0.024506 0.074369 -0.13931 -0.07486 0.0020157 0.044709 0.035975 -0.08633 0.037271 -0.022036 -0.20792 0.29181 -0.28787 -0.05817 -0.11886 -0.084538 0.010534 0.15718 -0.063389 0.094175 0.1508 -0.027417 -0.201 -0.065509 0.12314 -0.061144 0.013544 -0.031969 -0.062309 -0.079489 -0.10699 -0.059332 -0.010287 0.042412 0.060391 -0.00768 -0.079338 -0.013169 0.063615 -0.12585 0.092502 -0.15091 -0.010268 0.019806 0.163 -0.026803 0.36372 -0.14056 -0.12179 0.11145 -0.070284 0.021823 -0.035683 0.042475 0.023912 0.049552 -0.086841 0.021564 -0.037359 0.040563 0.042037 -0.036495 \n",
      "     7\t</s> 0.18359 0.36672 -0.033284 -0.31465 0.14219 0.22588 -0.40628 -0.3182 -0.080942 0.1469 -0.00077749 0.099285 -0.24105 -0.068515 -0.24273 0.075764 -0.13916 -0.058512 0.19469 -0.11051 -0.20912 -0.070186 0.019832 -0.0082186 0.037758 0.097409 -0.00069495 -0.087801 -0.14563 0.027163 0.032071 0.017951 -0.011911 -0.22515 0.17966 0.062699 -0.15002 -0.0036526 -0.058768 0.083652 -0.15431 -0.017625 -0.13952 0.10194 0.10931 0.0070421 -0.085462 -0.30637 0.10875 -0.077061 0.094144 -0.014823 -0.08881 0.16784 0.14788 0.049086 0.061227 0.037572 0.080956 -0.1216 -0.22249 -0.082622 0.17585 -0.12486 0.10412 0.065559 -0.16515 -0.092879 -0.1174 0.10457 -0.04887 0.11092 0.23592 -0.18043 -0.079222 -0.071969 0.18628 -0.22207 0.019543 -0.21084 0.034435 0.10871 0.13135 0.20628 0.45916 -0.18858 -0.28127 0.11079 -0.067213 0.12895 0.048107 -0.065784 -0.13145 -0.017256 0.058925 -0.0031629 -0.13807 0.0055867 -0.12716 -0.067484 \n",
      "     8\tpron -0.011166 0.23041 0.0032886 -0.27873 -0.058617 0.15877 -0.26552 -0.31605 -0.012341 0.072834 -0.071181 0.049487 -0.13255 -0.10862 -0.017444 0.012832 -0.10143 -0.14937 0.11799 0.01254 -0.22577 0.28758 0.3054 0.09878 -0.030303 0.22328 0.15733 0.012218 -0.13574 -0.013381 -0.084349 0.0025936 -0.1223 -0.03756 0.070443 0.060921 -0.062218 -0.040199 0.066158 0.023274 -0.03957 0.01129 -0.037724 0.039305 -0.1198 0.14669 0.0074371 -0.08541 -0.099487 0.20203 -0.2369 0.06311 -0.029373 -0.030615 0.098348 0.14371 0.049038 0.11647 0.0051976 0.11275 -0.16499 0.0016343 -0.045225 -0.081173 0.077964 -0.014945 0.073337 -0.01519 -0.0099161 -0.048992 -0.015671 -0.049001 0.025734 0.061328 -0.014375 0.028365 0.031789 -0.1331 -0.070738 -0.12736 -0.0031341 -0.039632 0.17087 -0.050962 0.28118 0.067767 -0.17346 0.25297 -0.34675 0.032827 -0.10048 0.065958 0.0595 0.12467 -0.10957 -0.00053789 -0.045115 0.15064 -0.02073 -0.033622 \n",
      "     9\tconj 0.010779 0.18209 -0.095264 -0.13208 0.052852 0.07524 -0.15966 -0.12308 -0.084847 0.15879 -0.10729 0.0085202 -0.11295 0.023508 -0.16318 -0.012403 -0.015815 -0.048285 0.047873 -0.028895 -0.014577 0.016908 0.014356 0.03238 -0.0077706 0.11891 0.073181 0.0047919 -0.097295 -0.080132 -0.068056 0.06043 -0.059689 -0.001082 0.049595 0.019536 0.00045958 -0.032225 0.017709 0.012841 -0.17753 -0.0062436 -0.026972 0.081514 0.036657 -0.058194 0.008761 -0.14617 -0.065411 0.10311 -0.088398 -0.034922 -0.11882 -0.1108 0.0041819 0.13971 -0.024003 0.069461 0.18896 -0.13776 -0.15189 -0.024668 0.11699 -0.12731 0.032054 -0.056043 -0.21321 -0.096722 -0.23919 -0.14487 -0.0070373 0.12617 0.085845 -0.059204 -0.064575 -0.025636 0.12889 -0.08662 0.15898 -0.13338 -0.057921 -0.016503 0.11417 -0.010465 0.45044 -0.13899 -0.15873 0.093544 -0.042989 -0.031203 -0.016723 -0.01098 0.0063584 0.044273 0.0028058 -0.0042623 -0.076717 0.070825 0.010615 -0.02165 \n",
      "    10\tadj -0.053174 0.12144 0.0015781 -0.13739 -0.012077 0.055093 -0.099257 -0.13157 -0.11831 0.18143 -0.14336 -0.0041961 -0.15903 -0.039408 -0.17565 -0.022156 -0.03147 -0.11811 0.056323 0.0071446 -0.09616 0.19682 0.19072 0.07639 -0.023754 0.17973 0.14585 0.025546 -0.12302 -0.083255 -0.12277 0.042313 -0.095678 0.034484 0.017303 0.037551 0.0071265 -0.046548 0.035772 0.0061479 -0.13264 -0.01446 0.01018 0.063365 0.012606 -0.056034 0.029257 -0.061206 -0.11823 0.15148 -0.13981 -0.031765 -0.098954 -0.11872 -0.0081522 0.12077 -0.029419 0.053729 0.11662 -0.05995 -0.094963 -0.006081 0.062319 -0.076928 0.042361 -0.061367 -0.17256 -0.092246 -0.23082 -0.17104 -0.026981 0.10492 0.050256 0.00032341 -0.055304 -0.0034526 0.062752 -0.060391 0.10956 -0.088859 -0.043909 -0.037053 0.10383 -0.069128 0.26679 -0.05823 -0.087794 0.085872 -0.10174 -0.0048367 -0.041915 0.024426 0.02201 0.067185 -0.044721 -0.006332 -0.06619 0.08252 0.026187 -0.048796 \n",
      "    11\tadv -0.06307 0.13506 0.0079782 -0.19934 -0.093076 0.065826 -0.13823 -0.20352 -0.017278 0.076288 -0.093022 -0.00052654 -0.12926 -0.093703 -0.046737 -0.00093158 -0.034561 -0.03966 0.078216 0.036636 -0.20095 0.23946 0.2273 0.11865 -0.040119 0.205 0.17088 0.055503 -0.14357 -0.045157 -0.090598 -0.0093075 -0.10341 -0.038796 0.061974 0.024166 -0.026685 0.0067794 0.11241 -0.043828 -0.092427 0.030294 -0.016659 0.04862 -0.096425 0.075427 0.025398 -0.097544 -0.12494 0.214 -0.23589 0.041731 -0.045258 -0.10956 0.055445 0.18273 0.02345 0.12389 0.068101 0.053012 -0.14168 0.0042385 -0.023473 -0.093498 0.058023 -0.055151 -0.039233 -0.052556 -0.15144 -0.15875 -0.010964 0.024771 0.032921 0.043523 -0.041028 0.026019 0.076847 -0.133 0.059864 -0.16746 -0.049694 -0.065326 0.18242 -0.087821 0.41704 -0.0064635 -0.17688 0.21597 -0.19777 -0.030298 -0.080508 0.044329 0.078162 0.095445 -0.062877 0.0084087 -0.02409 0.13498 -0.0033512 0.020656 \n",
      "    12\tnum -0.014809 0.099661 -0.075504 0.0065529 0.23854 0.12856 -0.043875 -0.024135 -0.30379 0.27047 -0.16395 -0.067097 -0.07837 0.043383 -0.43556 -0.029152 -0.092381 -0.31238 0.030745 -0.024358 0.099431 0.36806 0.26354 0.011115 0.048573 0.14154 -0.04983 -0.040118 0.045508 -0.2011 -0.076561 0.18177 -0.089919 0.21905 -0.069837 0.021995 -0.052687 -0.27884 -0.2563 0.22752 -0.65029 -0.20681 0.099859 0.24588 0.34053 -0.44425 0.056164 0.034066 -0.17671 0.10329 -0.12235 -0.062672 -0.111 -0.071615 -0.12076 0.072658 -0.13264 -0.0051457 0.25828 -0.17778 0.097861 -0.044808 0.097696 0.075036 -0.071764 -0.053611 -0.11555 -0.018571 -0.14729 -0.039016 -0.011427 0.13098 -0.041999 -0.17918 -0.044151 -0.084976 -0.015583 0.067836 0.030152 0.04096 -0.051716 0.012807 -0.075065 0.10763 0.094683 -0.047641 -0.02188 -0.20211 0.11634 0.01823 -0.027226 0.073688 0.027487 -0.060212 0.0054701 0.11922 0.0029978 -0.016177 -0.028604 -0.04484 \n",
      "    13\ttn -0.040216 0.096772 -0.032869 -0.07984 0.0038702 0.032323 -0.037766 -0.039701 -0.086857 0.10251 -0.088117 -0.01658 -0.090997 -0.003259 -0.12746 -0.034152 -0.036689 -0.14816 0.042716 0.00092475 -0.075349 0.18921 0.19485 0.071325 -0.021222 0.1225 0.11184 0.008911 -0.10235 -0.085257 -0.12991 0.045043 -0.12504 0.062688 -0.0073192 0.04373 0.022539 -0.081017 0.0082705 0.03377 -0.19752 -0.034809 0.018475 0.10104 0.044582 -0.08989 0.044103 -0.069356 -0.16537 0.18771 -0.16605 -0.059739 -0.12952 -0.13056 -0.0029714 0.14636 -0.037431 0.076915 0.15274 -0.090178 -0.09871 -0.0010458 0.069356 -0.082153 0.045682 -0.046938 -0.17823 -0.063229 -0.21516 -0.16891 -0.041247 0.10543 0.065272 -0.043916 -0.095153 -0.042579 0.066541 -0.070345 0.14912 -0.10368 -0.066511 -0.046395 0.082839 -0.064262 0.24521 -0.044925 -0.075506 0.049111 -0.17534 0.020092 -0.060869 0.045538 0.019426 0.074724 -0.028838 -0.010291 -0.079841 0.098666 0.0047553 -0.061582 \n",
      "    14\tfw 0.12402 0.40593 -0.37628 -0.14333 0.14269 0.21087 -0.31588 -0.13169 0.014807 0.054225 -0.0012742 0.12919 0.07961 0.17508 0.065218 -0.10008 -0.065235 -0.42316 -0.018426 -0.10654 0.2523 -0.062122 0.066258 -0.097475 0.021436 0.032844 0.0039761 -0.16761 -0.019276 -0.076319 -0.18591 0.24049 -0.19343 0.41251 -0.15821 0.17568 0.13886 -0.28544 -0.13961 0.25779 -0.062504 -0.19742 0.014377 0.029574 0.1822 -0.21463 0.095725 0.12431 -0.18075 0.28291 -0.10072 -0.39849 -0.39209 -0.25391 -0.13884 -0.046776 -0.14008 -0.13834 0.18974 -0.3668 -0.13329 0.011364 0.31745 -0.093132 -0.031543 -0.015068 -0.20849 -0.024811 0.040699 0.002603 0.042753 0.054779 0.065885 0.056243 -0.090695 -0.063253 0.096476 0.028118 0.39938 -0.016954 -0.052598 0.042622 0.065984 -0.22712 0.23357 -0.36638 0.0452 0.016988 -0.66405 0.11764 -0.11364 0.043205 -0.1387 0.253 -0.24957 -0.22715 -0.32423 0.096681 0.242 -0.41715 \n",
      "    15\tint 0.017314 0.22272 0.033045 -0.29554 -0.069183 0.18383 -0.29276 -0.35689 0.044168 -0.0044261 -0.0091703 0.060837 -0.11479 -0.13971 0.043108 0.05202 -0.13522 -0.14368 0.17355 0.0010678 -0.32559 0.33462 0.35682 0.12329 -0.013933 0.22838 0.1457 0.0082476 -0.14633 0.014547 -0.042781 -0.021574 -0.12408 -0.11786 0.12629 0.069358 -0.12404 -0.031712 0.063587 0.023661 -0.13089 0.0074661 -0.05929 0.086719 -0.10554 0.15125 -0.013344 -0.15567 -0.11507 0.23872 -0.28176 0.088697 -0.037511 0.013478 0.14199 0.19022 0.071318 0.14208 0.013638 0.12435 -0.18976 -0.022593 -0.046268 -0.079188 0.090517 0.011078 0.15852 0.007406 0.072706 0.036735 -0.017287 -0.095692 0.049808 0.01675 -0.012321 0.023458 0.046516 -0.18755 -0.1439 -0.17079 0.021638 -0.02008 0.15316 0.028661 0.24866 0.10807 -0.2118 0.24204 -0.23742 0.048099 -0.075875 0.052539 0.051389 0.064031 -0.069607 0.0365 0.0069283 0.10002 -0.06987 0.010404 \n",
      "    16\tabb -0.10772 0.0042786 0.016961 0.050485 0.13806 -0.043534 0.093487 0.084105 -0.30441 0.34782 -0.27549 -0.11192 -0.21794 0.064261 -0.49394 -0.074091 0.012415 -0.12101 -0.027274 0.028681 0.093166 0.17617 0.12012 0.009744 0.0044509 0.076209 0.06413 0.021249 -0.041799 -0.19508 -0.15093 0.10859 -0.045377 0.14355 -0.088486 0.010651 0.059798 -0.12837 -0.11633 0.054683 -0.45361 -0.13546 0.10272 0.17542 0.25911 -0.41529 0.079668 -0.034065 -0.14601 0.052339 -0.0003314 -0.090779 -0.15319 -0.11972 -0.099029 0.072153 -0.14324 -0.0046997 0.34219 -0.33753 0.052926 0.018655 0.18239 -0.0096171 -0.014136 -0.084788 -0.40855 -0.13485 -0.40509 -0.21437 -0.060651 0.26054 0.07912 -0.15853 -0.10156 -0.066296 0.12861 0.010127 0.32872 -0.072191 -0.094741 -0.010157 0.059932 -0.034787 0.38621 -0.23625 -0.06523 -0.11813 0.11582 -0.0062587 0.021498 0.01292 -0.022507 -0.00032071 0.040466 -0.006533 -0.14106 0.021462 0.081468 -0.18375 \n",
      "    17\tsb 0.13204 0.43253 -0.37483 -0.13399 0.3033 0.35499 -0.37608 -0.2585 -0.15486 0.18867 -0.065546 0.12936 0.10303 0.091727 -0.14974 -0.043963 -0.19715 -0.60386 -0.0014239 -0.12912 0.3686 0.23196 0.20568 -0.1429 0.10618 0.23498 -0.12008 -0.21444 0.099595 -0.18304 -0.089811 0.43252 -0.16224 0.51779 -0.15726 0.16575 -0.055703 -0.48609 -0.42369 0.49044 -0.45757 -0.37668 0.10335 0.12966 0.43066 -0.52735 0.11135 0.26824 -0.33807 0.42835 -0.2914 -0.43559 -0.51609 -0.27612 -0.26447 0.0097413 -0.26275 -0.24353 0.32411 -0.42768 -0.0034668 -0.079066 0.41065 0.081653 -0.11481 -0.052652 -0.07881 -0.011854 0.12075 0.17106 0.099161 0.035716 -0.06733 -0.029305 0.0034265 -0.073091 0.047187 0.13539 0.25679 0.075814 -0.0099448 0.17318 -0.061237 -0.042568 -0.062284 -0.46087 0.15472 -0.28317 -0.44403 0.11692 -0.084105 0.099155 -0.1376 0.15633 -0.32059 -0.12051 -0.22747 -0.1018 0.2691 -0.48716 \n"
     ]
    }
   ],
   "source": [
    "!cat -n ./mypos.tag.100.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bdcfa-31e4-454f-8891-d15e0a098f04",
   "metadata": {},
   "source": [
    "fasttext model ရှိသွားပြီမို့လို့ language model တွေကို Lahet LM toolkit နဲ့ ဆောက်တဲ့အခါမှာ embedding feature အနေနဲ့ သုံးလို့ ရသွားပါပြီ။  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e827465-c67f-4fdb-98ef-941ff4ae6121",
   "metadata": {},
   "source": [
    "ဆောက်ခဲ့တဲ့ fasttext model ကို ကော်ပီကူးခဲ့။  \n",
    "\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ cp mypos.tag.100.bin ../../../../fasttext-model/\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ ls ../../../../fasttext-model/\n",
    "cc.my.300.bin  myfasttext_v1.bin  myfasttext_v1.vec  mypos.tag.100.bin  note.txt\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235bae0-afb1-4f64-b8c6-10ed74499f3f",
   "metadata": {},
   "source": [
    "## Updating Bash Shell Script\n",
    "\n",
    "Laphet (version 0.6) တုန်းက သုံးခဲ့တဲ့ shell script ကို --embedding_method, --fasttext_model စတဲ့ command line argument အသစ်တွေ ထပ်ဖြည့်ပြီး update လုပ်ခဲ့ပါတယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56ebc5a-5d89-4ab8-83c6-826723a13c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib\n"
     ]
    }
   ],
   "source": [
    "%cd ~/exp/name-lm/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7482932c-8779-4117-accd-3d1f71d12466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ye/exp/name-lm/lib'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30ebed54-d395-4c3e-be4f-4526a31b5429",
   "metadata": {},
   "source": [
    "အရင်ဆုံး MLP model ကို train မှာမို့လို့ ကျန်တဲ့ task တွေကို comment ပိတ်ထားလိုက်တယ်။  \n",
    "\n",
    "task mlp  \n",
    "#task bilstm  \n",
    "#task transformer  \n",
    "#task bert  \n",
    "#task gpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4617746a-5344-489a-8f6c-3d505747e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.ftfz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_ftfz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.ftfz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ce940-c76f-423d-b66f-abcef4dc1abe",
   "metadata": {},
   "source": [
    "## Training, Testing MLP LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "963e4a81-cfde-4bc4-b200-de454cc85444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mlp language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 185.16it/s]\n",
      "Epoch 1, Training Loss: 1.0539\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 520.13it/s]\n",
      "Epoch 1, Validation Loss: 0.5764\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5764\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 185.69it/s]\n",
      "Epoch 2, Training Loss: 0.5742\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 507.11it/s]\n",
      "Epoch 2, Validation Loss: 0.5732\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5732\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 193.54it/s]\n",
      "Epoch 3, Training Loss: 0.5730\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 530.93it/s]\n",
      "Epoch 3, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 196.26it/s]\n",
      "Epoch 4, Training Loss: 0.5729\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 529.22it/s]\n",
      "Epoch 4, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 191.57it/s]\n",
      "Epoch 5, Training Loss: 0.5729\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 530.48it/s]\n",
      "Epoch 5, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 188.66it/s]\n",
      "Epoch 6, Training Loss: 0.5729\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 531.75it/s]\n",
      "Epoch 6, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 187.48it/s]\n",
      "Epoch 7, Training Loss: 0.5729\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 520.11it/s]\n",
      "Epoch 7, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 197.59it/s]\n",
      "Epoch 8, Training Loss: 0.5729\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 541.25it/s]\n",
      "Epoch 8, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 198.46it/s]\n",
      "Epoch 9, Training Loss: 0.5729\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 541.91it/s]\n",
      "Epoch 9, Validation Loss: 0.5729\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:06<00:00, 196.52it/s]\n",
      "Epoch 10, Training Loss: 0.5729\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 547.63it/s]\n",
      "Epoch 10, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.ftfz.model with validation loss: 0.5729\n",
      "\n",
      "real\t1m8.559s\n",
      "user\t1m13.215s\n",
      "sys\t0m2.466s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n sb part part part tn sb sb [UNK] ppm ppm abb pron int int num punc part num [PAD] num ppm sb ppm fw adv [PAD] sb part [UNK] [UNK] fw fw num fw num v v part part v [PAD] fw conj [UNK] int [UNK] fw fw sb fw\n",
      "Generated Text 2: n num pron abb adv punc ppm adv punc v int part abb [UNK] punc ppm punc adv part [UNK] adj int n sb part ppm tn pron [UNK] conj adj n abb adj adj conj [UNK] [PAD] adv ppm ppm sb punc sb abb ppm fw num adj [UNK] pron\n",
      "Generated Text 3: n int part adv tn [PAD] [PAD] pron v n v v adj punc ppm adj abb pron num ppm sb fw part sb adj v adv [UNK] [PAD] sb fw punc fw [UNK] n [PAD] abb pron adj num ppm [UNK] n punc ppm [PAD] fw [PAD] conj adj tn\n",
      "Generated Text 4: n conj part v tn adv pron pron [UNK] tn pron adj adv int fw conj fw ppm adj fw ppm fw v [UNK] num pron adj adj [PAD] punc punc [UNK] adv [UNK] abb conj fw conj part ppm num part [PAD] int adj pron [PAD] adv adj abb punc\n",
      "Generated Text 5: n n adj conj adv adj adv num n adv abb [UNK] v v v v [UNK] [PAD] num adv punc int v fw sb adv [UNK] fw int pron adv v int fw fw pron abb ppm fw adj pron [PAD] int conj [PAD] n num punc abb abb pron\n",
      "Generated Text 6: n [UNK] abb pron [PAD] tn adj num ppm fw [UNK] fw conj num sb int sb num [PAD] part [PAD] adj [UNK] n adv fw num sb [UNK] conj adj v part [UNK] tn n punc part pron ppm fw punc tn [PAD] pron [PAD] num sb int conj ppm\n",
      "Generated Text 7: n part adj [UNK] num n n num [UNK] tn punc [UNK] num abb fw pron punc n conj fw punc [UNK] [UNK] v part conj int adv punc abb [UNK] adv [PAD] [PAD] int n abb conj ppm [PAD] tn num fw adv [UNK] [UNK] fw num conj adv sb\n",
      "Generated Text 8: n ppm v abb sb int pron adv n num punc num tn int [PAD] adv [UNK] v int [PAD] tn n part adj tn fw v abb ppm ppm conj punc ppm ppm tn int ppm adj int sb adv part n sb pron pron num tn num sb fw\n",
      "Generated Text 9: n punc fw int abb [PAD] fw fw part [PAD] n punc part adv punc pron pron [PAD] n tn v abb [PAD] v conj v ppm conj adv ppm v [UNK] part sb adj ppm ppm conj fw num n abb ppm fw int v int v fw int tn\n",
      "Generated Text 10: n part [PAD] [PAD] fw [UNK] int n int punc pron [UNK] num [UNK] [UNK] punc adj tn n num num sb adj conj sb n v adj ppm conj pron pron pron adj abb int part ppm n adj int fw pron n v num v conj punc fw v\n",
      "\n",
      "real\t0m2.366s\n",
      "user\t0m5.065s\n",
      "sys\t0m2.383s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/mlp_ftfz_gen_texts.txt\n",
      "\n",
      "real\t0m1.850s\n",
      "user\t0m4.558s\n",
      "sys\t0m2.372s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.27it/s]\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "real\t0m1.805s\n",
      "user\t0m4.487s\n",
      "sys\t0m2.346s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed6cf6-92be-4918-8d25-67ee2c27c53b",
   "metadata": {},
   "source": [
    "## GPU Usage Infor for MLP LM Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dd0bfbc-3d0b-4fa5-a1aa-b616f74aa5ff",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:45:10 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   53C    P2             146W / 480W |   1084MiB / 24564MiB |     47%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       59MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    161347      C   python                                      350MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:45:21 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   54C    P2             149W / 480W |   1084MiB / 24564MiB |     47%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       59MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      162MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    161347      C   python                                      350MiB |\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c68c9-e611-4568-bbb9-179c96abd915",
   "metadata": {},
   "source": [
    "Model file size ကို စစ်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af9db3bc-5cb2-4535-b8a9-d014b621b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 246K Jan 28 19:46 ./model/tag/mlp.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 19:44 ./model/tag/mlp.ftfz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/mlp.ftfz*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37634e8d-20ba-472f-a36c-687f454a8331",
   "metadata": {},
   "source": [
    "vocab ဖိုင်ကိုလည်း လေ့လာကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53ba1684-fd73-4fc9-b1a8-668725debb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conj\t0\n",
      "abb\t1\n",
      "n\t2\n",
      "[PAD]\t3\n",
      "int\t4\n",
      "tn\t5\n",
      "part\t6\n",
      "num\t7\n",
      "ppm\t8\n",
      "pron\t9\n",
      "sb\t10\n",
      "adj\t11\n",
      "adv\t12\n",
      "fw\t13\n",
      "v\t14\n",
      "[UNK]\t15\n",
      "punc\t16\n"
     ]
    }
   ],
   "source": [
    "!cat ./model/tag/mlp.ftfz.model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c48145-ecda-4ad8-9f9d-ce6ebde86658",
   "metadata": {},
   "source": [
    "start_tags.txt ဖိုင်ကိုလည်း သုံးပြီး MLP LM model နဲ့ text generation လုပ်ထားတာမို့ အရင်ဆုံး start_tags.txt ဖိုင်ကို လေ့လာပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51672683-536d-4208-a65c-a31d43d754ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbe1d9-1433-454c-8291-a35249b9d5c0",
   "metadata": {},
   "source": [
    "MLP model က ထုတ်ပေးတဲ့ generated tags တွေက အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "afad3538-504d-4866-a8ce-92d5fdb70121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron adj ppm\n",
      "pron part punc\n",
      "pron n conj\n",
      "pron sb adj\n",
      "pron v v\n",
      "n [PAD] [UNK]\n",
      "n abb part\n",
      "n v punc\n",
      "n sb conj\n",
      "n v n\n",
      "adj [UNK] adj\n",
      "adj fw ppm\n",
      "adj pron adv\n",
      "adj v part\n",
      "adj n pron\n",
      "v [UNK] pron\n",
      "v punc fw\n",
      "v part adj\n",
      "v n conj\n",
      "v n adj\n",
      "pron part fw adj\n",
      "pron part num [PAD]\n",
      "pron part sb conj\n",
      "pron part [UNK] [PAD]\n",
      "pron part n conj\n",
      "pron ppm int [PAD]\n",
      "pron ppm fw tn\n",
      "pron ppm tn pron\n",
      "pron ppm ppm num\n",
      "pron ppm abb n\n",
      "n v ppm pron\n",
      "n v num num\n",
      "n v v int\n",
      "n v num num\n",
      "n v punc adv\n",
      "n n pron tn\n",
      "n n part [PAD]\n",
      "n n adv conj\n",
      "n n fw adj\n",
      "n n ppm part\n",
      "v part v sb\n",
      "v part n abb\n",
      "v part tn v\n",
      "v part [UNK] [UNK]\n",
      "v part part fw\n",
      "n part num adv\n",
      "n part pron conj\n",
      "n part fw num\n",
      "n part fw num\n",
      "n part conj fw\n",
      "pron pron num num\n",
      "pron pron punc adv\n",
      "pron pron int tn\n",
      "pron pron sb tn\n",
      "pron pron part adj\n",
      "pron ppm num [UNK]\n",
      "pron ppm num fw\n",
      "pron ppm abb int\n",
      "pron ppm num tn\n",
      "pron ppm part punc\n",
      "n tn punc [UNK]\n",
      "n tn conj adv\n",
      "n tn int pron\n",
      "n tn ppm n\n",
      "n tn [PAD] pron\n",
      "adj v part n adv\n",
      "adj v part ppm ppm\n",
      "adj v part int abb\n",
      "adj v part ppm ppm\n",
      "adj v part part ppm\n",
      "n n n adj tn\n",
      "n n n [PAD] v\n",
      "n n n tn sb\n",
      "n n n [PAD] num\n",
      "n n n adv punc\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/mlp_ftfz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a366f23-ece0-4247-8958-c509ed143084",
   "metadata": {},
   "source": [
    "## Updating Shell Script for Running Only Bi-LSTM LM\n",
    "\n",
    "\\#task mlp  \n",
    "task bilstm  \n",
    "\\#task transformer  \n",
    "\\#task bert  \n",
    "\\#task gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2401d-3867-4621-9777-fc01ab191bb3",
   "metadata": {},
   "source": [
    "## Training, Testing for Bi-LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea5470b3-65de-482d-861b-b233e9f2a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.96it/s]\n",
      "Epoch 1, Training Loss: 0.1998\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 173.55it/s]\n",
      "Epoch 1, Validation Loss: 0.0021\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0021\n",
      "Epoch 2/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.59it/s]\n",
      "Epoch 2, Training Loss: 0.0017\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 179.68it/s]\n",
      "Epoch 2, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0002\n",
      "Epoch 3/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.72it/s]\n",
      "Epoch 3, Training Loss: 0.0004\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 180.36it/s]\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0001\n",
      "Epoch 4/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.98it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.44it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.48it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 168.27it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.81it/s]\n",
      "Epoch 6, Training Loss: 0.0001\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 177.98it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.16it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 174.27it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.78it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 178.82it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.ftfz.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 60.27it/s]\n",
      "Epoch 9, Training Loss: 0.0001\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 176.90it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 59.56it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 171.00it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "\n",
      "real\t3m35.927s\n",
      "user\t3m39.785s\n",
      "sys\t0m3.035s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 2: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 3: n [UNK] n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 4: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 5: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 6: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 7: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 8: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 9: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 10: n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "\n",
      "real\t0m2.237s\n",
      "user\t0m4.902s\n",
      "sys\t0m2.422s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_ftfz_gen_texts.txt\n",
      "\n",
      "real\t0m2.065s\n",
      "user\t0m4.693s\n",
      "sys\t0m2.425s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 41.46it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m2.096s\n",
      "user\t0m4.527s\n",
      "sys\t0m2.449s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00255c2b-dee5-4d36-bf72-303315b97571",
   "metadata": {},
   "source": [
    "## GPU Usage Info for Bi-LSTM  \n",
    "\n",
    "Bi-LSTM LM model ကတော့ GPU ကို တော်တော်လေး သုံးရပါတယ်။ အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "634f8223-0c33-40d1-9a4c-9042a60b0040",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:48:39 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 51%   77C    P2             387W / 480W |   1958MiB / 24564MiB |     94%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       53MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    161668      C   python                                     1232MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:49:02 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 51%   77C    P2             377W / 480W |   1958MiB / 24564MiB |     94%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       53MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    161668      C   python                                     1232MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b4d5be0-6a7b-40f8-8950-b108ee158725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 82M Jan 28 19:49 ./model/tag/bilstm.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye 110 Jan 28 19:46 ./model/tag/bilstm.ftfz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/bilstm.ftfz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "259026c1-338f-42af-9e96-69916bdbea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron ppm num\n",
      "pron [PAD] part\n",
      "pron pron tn\n",
      "pron [PAD] int\n",
      "pron adj [UNK]\n",
      "n n n\n",
      "n n n\n",
      "n n n\n",
      "n n n\n",
      "n n n\n",
      "adj tn [PAD]\n",
      "adj part part\n",
      "adj sb tn\n",
      "adj sb abb\n",
      "adj [UNK] ppm\n",
      "v [UNK] v\n",
      "v [UNK] v\n",
      "v [UNK] v\n",
      "v [UNK] v\n",
      "v [UNK] v\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron part part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "n v [UNK] v\n",
      "n v [UNK] v\n",
      "n v [UNK] v\n",
      "n v [UNK] v\n",
      "n v [UNK] v\n",
      "n n n n\n",
      "n n n n\n",
      "n n n n\n",
      "n n n n\n",
      "n n n n\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "v part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "n part part part\n",
      "pron pron abb conj\n",
      "pron pron adj conj\n",
      "pron pron num num\n",
      "pron pron ppm num\n",
      "pron pron pron pron\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "pron ppm part part\n",
      "n tn sb [PAD]\n",
      "n tn sb [PAD]\n",
      "n tn [PAD] ppm\n",
      "n tn [PAD] ppm\n",
      "n tn pron conj\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "adj v part part part\n",
      "n n n n n\n",
      "n n n n n\n",
      "n n n n n\n",
      "n n n n n\n",
      "n n n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/bilstm_ftfz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9ddb3c1-e5f6-44ad-b8f7-3d403be8a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/bilstm.ftfz.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559eb48-b216-40a6-963a-b46f25fcd794",
   "metadata": {},
   "source": [
    "Test data နဲ့ PPL, corss-entropy score တွေက ကောင်းပေမဲ့ long sequence တွေအတွက် generation က ရလဒ်မကောင်းဘူး။ parameter ကစားတာနဲ့ coding ကို ပြန်စစ်ဖို့လိုအပ်တယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bc3e4-b087-4b07-aaad-23f1ed75a629",
   "metadata": {},
   "source": [
    "## Training, Testing with Transformer based LM\n",
    "\n",
    "\\# Run tasks for each model type in the specified order  \n",
    "\\#task mlp  \n",
    "\\#task bilstm  \n",
    "task transformer  \n",
    "\\#task bert  \n",
    "\\#task gpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "820c877a-48f7-4d58-b584-0e7587bcc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 219.82it/s]\n",
      "Epoch 1, Training Loss: 0.4247\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 652.39it/s]\n",
      "Epoch 1, Validation Loss: 0.0504\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0504\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 240.57it/s]\n",
      "Epoch 2, Training Loss: 0.0466\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 647.50it/s]\n",
      "Epoch 2, Validation Loss: 0.0113\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0113\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 234.53it/s]\n",
      "Epoch 3, Training Loss: 0.0175\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 648.49it/s]\n",
      "Epoch 3, Validation Loss: 0.0040\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0040\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 241.97it/s]\n",
      "Epoch 4, Training Loss: 0.0092\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 663.62it/s]\n",
      "Epoch 4, Validation Loss: 0.0018\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0018\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 246.43it/s]\n",
      "Epoch 5, Training Loss: 0.0055\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 636.88it/s]\n",
      "Epoch 5, Validation Loss: 0.0008\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0008\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 247.39it/s]\n",
      "Epoch 6, Training Loss: 0.0037\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 659.23it/s]\n",
      "Epoch 6, Validation Loss: 0.0006\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0006\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 251.87it/s]\n",
      "Epoch 7, Training Loss: 0.0027\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 647.53it/s]\n",
      "Epoch 7, Validation Loss: 0.0003\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0003\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 242.40it/s]\n",
      "Epoch 8, Training Loss: 0.0021\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 657.13it/s]\n",
      "Epoch 8, Validation Loss: 0.0003\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0003\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 240.21it/s]\n",
      "Epoch 9, Training Loss: 0.0018\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 656.60it/s]\n",
      "Epoch 9, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/transformer.ftfz.model with validation loss: 0.0002\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 247.03it/s]\n",
      "Epoch 10, Training Loss: 0.0015\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 656.24it/s]\n",
      "Epoch 10, Validation Loss: 0.0004\n",
      "\n",
      "real\t0m55.135s\n",
      "user\t0m59.694s\n",
      "sys\t0m2.492s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n abb num conj int adj punc pron tn v num int n pron pron part [PAD] fw pron sb conj sb abb part adj sb conj adv part sb part tn v v sb [PAD] pron adj sb [UNK] num conj conj conj tn v adv int conj adj abb\n",
      "Generated Text 2: n int ppm sb num sb adj abb adv part punc part adj v sb v int num ppm conj int ppm part tn v punc tn v adv fw v [UNK] adj part part v num part adv conj part [UNK] conj [UNK] punc tn v punc conj abb [PAD]\n",
      "Generated Text 3: n ppm part [PAD] int conj int [PAD] n part sb tn v ppm n int adj sb [UNK] num adj n num ppm fw ppm adv fw n n num [UNK] [PAD] n n pron fw pron ppm ppm [UNK] adv [PAD] conj pron pron tn v ppm adv v\n",
      "Generated Text 4: n part adj num punc [PAD] fw num sb n sb part num tn v abb num int [PAD] num adj punc fw punc abb v [UNK] fw int sb conj sb part part conj pron int [PAD] ppm adj [PAD] [UNK] pron adj [UNK] abb [PAD] conj fw int v\n",
      "Generated Text 5: n ppm n num sb part abb pron ppm tn tn v adj v pron abb [UNK] v ppm punc num adv n ppm conj ppm [PAD] fw tn v num conj fw part [PAD] adv tn v adj sb adv punc pron num v adv n [PAD] int abb [PAD]\n",
      "Generated Text 6: n part n part [UNK] [UNK] adv int pron part part conj int conj ppm tn [PAD] ppm ppm adj num abb tn int pron sb conj [UNK] v n num n tn v ppm tn v int int part [PAD] [UNK] conj part abb [PAD] num [UNK] int adj adj\n",
      "Generated Text 7: n ppm abb [PAD] abb tn adj abb [UNK] v v n fw punc punc adv fw [PAD] n [UNK] v num part ppm n sb num [PAD] adj adj adv num n v n [UNK] adj adj sb ppm v [PAD] punc ppm [UNK] tn v num int tn fw\n",
      "Generated Text 8: n pron abb v [UNK] [UNK] tn part num int fw sb part pron adv sb punc punc [UNK] num v adv pron [UNK] adv n [PAD] ppm pron fw ppm [PAD] n fw int adv conj sb abb adj [UNK] [UNK] sb pron adj conj int [PAD] conj fw tn\n",
      "Generated Text 9: n v sb [UNK] abb adj sb part pron int sb adv fw pron conj part adj sb abb part pron [UNK] abb [PAD] int adj adj int n punc v sb part conj v v sb n adv abb v sb v tn v tn v sb num ppm adv\n",
      "Generated Text 10: n [PAD] fw num pron [PAD] part punc v punc abb adv [UNK] fw punc conj [UNK] ppm adj [UNK] abb punc sb abb int ppm ppm v adj adj sb ppm n int n conj conj fw tn v tn v punc [UNK] n v v n conj v conj\n",
      "\n",
      "real\t0m2.562s\n",
      "user\t0m5.202s\n",
      "sys\t0m2.434s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/transformer_ftfz_gen_texts.txt\n",
      "\n",
      "real\t0m2.051s\n",
      "user\t0m4.742s\n",
      "sys\t0m2.381s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 48.13it/s]\n",
      "Average Perplexity on Test Data: 1.0001\n",
      "Average Cross-Entropy on Test Data: 0.0001\n",
      "\n",
      "real\t0m1.830s\n",
      "user\t0m4.500s\n",
      "sys\t0m2.391s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aed1d3-6a84-44f2-b869-9d75644b45a9",
   "metadata": {},
   "source": [
    "## GPU Usage of Transformer LM Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ac52591-14cb-4d6b-86d8-01d05020ae67",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:54:44 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   53C    P2             232W / 480W |   1162MiB / 24564MiB |     59%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          253MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         70MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       39MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      156MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    162311      C   python                                      446MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ nvidia-smi\n",
    "Tue Jan 28 19:55:08 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   60C    P2             244W / 480W |   1155MiB / 24564MiB |     60%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       39MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      156MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    162311      C   python                                      448MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/data/myPOS/tag/fasttext$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f406bba-cd4d-4ec2-9ad0-e8436af0b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 19:55 ./model/tag/transformer.ftfz.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 28 19:54 ./model/tag/transformer.ftfz.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/transformer.ftfz.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a71a0328-273d-4e6a-927a-a9606fd0445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron [UNK] fw\n",
      "pron [UNK] fw\n",
      "pron adj part\n",
      "pron v sb\n",
      "pron part pron\n",
      "n pron v\n",
      "n num pron\n",
      "n ppm tn\n",
      "n num abb\n",
      "n adv v\n",
      "adj sb sb\n",
      "adj adv fw\n",
      "adj num abb\n",
      "adj sb ppm\n",
      "adj punc ppm\n",
      "v pron n\n",
      "v num pron\n",
      "v [PAD] fw\n",
      "v part part\n",
      "v int ppm\n",
      "pron part int num\n",
      "pron part punc punc\n",
      "pron part tn v\n",
      "pron part [UNK] sb\n",
      "pron part sb ppm\n",
      "pron ppm pron pron\n",
      "pron ppm pron fw\n",
      "pron ppm v pron\n",
      "pron ppm abb [UNK]\n",
      "pron ppm adv v\n",
      "n v int v\n",
      "n v num adj\n",
      "n v punc v\n",
      "n v tn v\n",
      "n v tn v\n",
      "n n tn v\n",
      "n n part ppm\n",
      "n n int abb\n",
      "n n abb punc\n",
      "n n adj fw\n",
      "v part ppm v\n",
      "v part [PAD] adv\n",
      "v part sb sb\n",
      "v part n n\n",
      "v part v adv\n",
      "n part abb ppm\n",
      "n part n ppm\n",
      "n part [PAD] punc\n",
      "n part pron num\n",
      "n part ppm sb\n",
      "pron pron punc adv\n",
      "pron pron abb [PAD]\n",
      "pron pron ppm adj\n",
      "pron pron fw n\n",
      "pron pron [UNK] [PAD]\n",
      "pron ppm part num\n",
      "pron ppm num abb\n",
      "pron ppm ppm num\n",
      "pron ppm [UNK] fw\n",
      "pron ppm abb conj\n",
      "n tn [PAD] adv\n",
      "n tn num tn\n",
      "n tn conj num\n",
      "n tn punc pron\n",
      "n tn ppm [UNK]\n",
      "adj v part num n\n",
      "adj v part adj adj\n",
      "adj v part pron n\n",
      "adj v part tn part\n",
      "adj v part int conj\n",
      "n n n [UNK] [UNK]\n",
      "n n n num conj\n",
      "n n n conj [PAD]\n",
      "n n n part punc\n",
      "n n n abb [PAD]\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/transformer_ftfz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd87a3-356a-4ece-a96d-2dd72b8735b7",
   "metadata": {},
   "source": [
    "## Training, Testing with BERT LM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92d2ee00-f5df-427b-b035-b24fe42a1d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.ftfz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_ftfz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.ftfz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e7111c3-87da-445b-8878-21d788471111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 217.46it/s]\n",
      "Epoch 1, Training Loss: 0.4136\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 631.32it/s]\n",
      "Epoch 1, Validation Loss: 0.0470\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0470\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 241.12it/s]\n",
      "Epoch 2, Training Loss: 0.0440\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 596.76it/s]\n",
      "Epoch 2, Validation Loss: 0.0111\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0111\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 235.93it/s]\n",
      "Epoch 3, Training Loss: 0.0180\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 645.60it/s]\n",
      "Epoch 3, Validation Loss: 0.0042\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0042\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 233.18it/s]\n",
      "Epoch 4, Training Loss: 0.0095\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 633.07it/s]\n",
      "Epoch 4, Validation Loss: 0.0017\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0017\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 234.27it/s]\n",
      "Epoch 5, Training Loss: 0.0058\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 623.73it/s]\n",
      "Epoch 5, Validation Loss: 0.0009\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0009\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 241.28it/s]\n",
      "Epoch 6, Training Loss: 0.0040\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 571.71it/s]\n",
      "Epoch 6, Validation Loss: 0.0006\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0006\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 232.87it/s]\n",
      "Epoch 7, Training Loss: 0.0030\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 612.70it/s]\n",
      "Epoch 7, Validation Loss: 0.0005\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0005\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 238.16it/s]\n",
      "Epoch 8, Training Loss: 0.0023\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 539.18it/s]\n",
      "Epoch 8, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.ftfz.model with validation loss: 0.0004\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 234.21it/s]\n",
      "Epoch 9, Training Loss: 0.0019\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 566.95it/s]\n",
      "Epoch 9, Validation Loss: 0.0017\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 239.56it/s]\n",
      "Epoch 10, Training Loss: 0.0016\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 562.07it/s]\n",
      "Epoch 10, Validation Loss: 0.0007\n",
      "\n",
      "real\t0m56.612s\n",
      "user\t1m1.221s\n",
      "sys\t0m2.496s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n adv punc adj n ppm n [PAD] punc part part tn punc pron pron [PAD] num pron [PAD] adj fw v conj n punc [PAD] v pron v punc adv ppm adj tn pron adj punc [UNK] [PAD] punc num part int num part n abb tn n v ppm\n",
      "Generated Text 2: n [PAD] adj part abb adj part [PAD] ppm [UNK] adj v num sb adj v ppm punc part pron adj num abb adv pron adv [PAD] v int int punc v fw adv adv conj sb adv ppm part num abb abb punc fw int abb punc adj n [UNK]\n",
      "Generated Text 3: n int [UNK] int abb conj part v [UNK] [UNK] adj ppm part conj adv num fw sb [PAD] adj adv tn punc abb num v abb adv adj pron adj int abb adv conj [UNK] int abb v ppm fw punc ppm adj adj punc conj part part adj part\n",
      "Generated Text 4: n [PAD] adj pron pron ppm adj num [UNK] ppm adj n fw num pron adv tn pron pron int part fw [PAD] conj adv sb sb [PAD] fw adj punc punc part v [UNK] sb punc int num adj adv sb [PAD] conj int adv punc abb pron fw int\n",
      "Generated Text 5: n tn pron abb punc [UNK] sb num [PAD] sb abb ppm num pron adj [UNK] n num punc pron v pron int adv [UNK] v [PAD] part sb pron num tn punc conj int [UNK] adv fw conj tn [PAD] conj sb pron sb fw pron adv adj n abb\n",
      "Generated Text 6: n part conj num abb adv ppm part part num abb n sb conj num num tn pron ppm [PAD] num adj abb int punc [PAD] conj adv adj conj num [UNK] int conj [PAD] fw abb punc punc abb ppm adv ppm adj adj punc [PAD] adv num n sb\n",
      "Generated Text 7: n [PAD] adj int [PAD] v fw ppm fw sb pron int abb [PAD] conj abb v fw tn punc fw abb [PAD] punc conj [UNK] adv pron tn tn punc n num abb punc tn punc num n fw sb fw adj punc adj punc fw adj [UNK] part sb\n",
      "Generated Text 8: n tn pron v [UNK] punc [PAD] abb n part conj ppm abb part sb punc sb [PAD] fw pron conj int tn fw [UNK] fw pron v adj n tn fw conj part fw ppm ppm ppm part pron conj abb tn punc [UNK] pron abb tn abb punc n\n",
      "Generated Text 9: n [PAD] adj part tn [UNK] sb [UNK] abb num [PAD] tn punc abb conj n sb conj fw ppm part num fw n conj fw [UNK] sb part abb punc [UNK] adj adv int num num adj n fw pron [UNK] v abb int adj punc abb part adj tn\n",
      "Generated Text 10: n part sb adj punc [UNK] sb conj adv adv fw [PAD] punc part sb ppm fw tn [UNK] adv num [PAD] tn fw punc conj n abb punc adv v fw [PAD] fw v v pron fw ppm [PAD] part part pron adj adj punc num abb pron conj punc\n",
      "\n",
      "real\t0m2.596s\n",
      "user\t0m5.254s\n",
      "sys\t0m2.410s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/bert_ftfz_gen_texts.txt\n",
      "\n",
      "real\t0m2.070s\n",
      "user\t0m4.771s\n",
      "sys\t0m2.378s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 48.35it/s]\n",
      "Average Perplexity on Test Data: 1.0003\n",
      "Average Cross-Entropy on Test Data: 0.0003\n",
      "\n",
      "real\t0m1.840s\n",
      "user\t0m4.486s\n",
      "sys\t0m2.417s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0da8b-3095-4250-9eae-d0041c4e9d8c",
   "metadata": {},
   "source": [
    "## GPU Usage of BERT LM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10b98743-9a26-4f41-b377-6591a752144d",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 19:58:29 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   52C    P2             230W / 480W |   1153MiB / 24564MiB |     60%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        121MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       37MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    162728      C   python                                      446MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 19:59:11 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   62C    P2             237W / 480W |   1173MiB / 24564MiB |     59%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        120MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       55MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    162728      C   python                                      448MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d80dab-56a6-4131-9aba-d0071850cab2",
   "metadata": {},
   "source": [
    "## Training, Testing with GPT based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ab06b26-a2b4-49f5-a6b4-12e16a548a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Updated for Laphet LM Toolkit Version 0.7\n",
      "# Last updated: 28 Jan 2025\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.ftfz.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_ftfz_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.ftfz.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_name=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 \\\n",
      "      --lr 0.0001 --embedding_method fasttext_freeze \\\n",
      "      --fasttext_model ./fasttext-model/mypos.tag.100.bin --embed_dim 100;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"n\" --no_of_generation 10 \\\n",
      "      --embedding_method fasttext_freeze\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_name --no_of_generation 5 --output $output_file \\\n",
      "      --embedding_method fasttext_freeze;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 --embedding_method fasttext_freeze 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc071baa-8c0a-4413-af2f-1b7ed4436f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gpt language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 256.75it/s]\n",
      "Epoch 1, Training Loss: 0.1020\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 736.50it/s]\n",
      "Epoch 1, Validation Loss: 0.0029\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0029\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 260.75it/s]\n",
      "Epoch 2, Training Loss: 0.0021\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 671.59it/s]\n",
      "Epoch 2, Validation Loss: 0.0005\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0005\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 273.07it/s]\n",
      "Epoch 3, Training Loss: 0.0005\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 584.42it/s]\n",
      "Epoch 3, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0002\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 267.46it/s]\n",
      "Epoch 4, Training Loss: 0.0003\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 739.81it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 271.05it/s]\n",
      "Epoch 5, Training Loss: 0.0002\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 676.07it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 275.28it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 792.49it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 272.68it/s]\n",
      "Epoch 7, Training Loss: 0.0001\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 761.21it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 285.46it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 777.28it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:04<00:00, 288.02it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 773.93it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:04<00:00, 285.76it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 806.70it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.ftfz.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m48.913s\n",
      "user\t0m53.687s\n",
      "sys\t0m2.501s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: n int fw n adj adj pron punc [UNK] part conj fw conj adj punc tn num part pron sb abb adj adj abb [UNK] fw ppm part [UNK] conj [UNK] fw tn abb part [UNK] int adj punc punc [UNK] abb ppm conj part [UNK] adj adv punc fw adv\n",
      "Generated Text 2: n adv n num num sb conj [PAD] fw tn adv n int pron v num num part n v adj [PAD] [PAD] num tn part adv part abb [PAD] abb adv [PAD] fw fw adj num punc punc adj int part num v v part part conj ppm v abb\n",
      "Generated Text 3: n ppm tn part conj adj sb punc adv pron int v n fw v num abb [PAD] num tn v sb ppm n pron int punc int [PAD] punc int sb punc adj conj adj abb adj fw pron int v sb part conj part fw conj sb n conj\n",
      "Generated Text 4: n punc adv sb num [UNK] part pron [PAD] abb ppm [UNK] part num fw abb fw tn conj sb ppm conj ppm adv pron v fw [PAD] punc v [UNK] abb fw num sb fw v pron abb v adv int adv tn tn conj int punc conj pron [PAD]\n",
      "Generated Text 5: n conj fw part pron fw n [UNK] conj [UNK] fw [PAD] [PAD] part abb n int n sb ppm tn [PAD] v num abb n conj sb ppm int ppm pron tn conj int num [UNK] num v [UNK] ppm sb ppm int punc tn abb fw adj punc ppm\n",
      "Generated Text 6: n sb adv num conj int abb pron fw num abb n [UNK] fw adv n abb tn [UNK] [PAD] sb conj [UNK] ppm pron conj n int punc [UNK] abb n int num tn abb conj pron tn punc sb pron n abb num punc tn tn num sb int\n",
      "Generated Text 7: n int ppm n adv tn num v [PAD] part int ppm fw num tn [PAD] sb v ppm [PAD] fw pron conj sb n [PAD] punc abb abb conj int sb num num fw [UNK] adj adj abb part punc conj conj sb [PAD] fw pron adv [PAD] punc pron\n",
      "Generated Text 8: n fw abb conj tn conj sb adv sb tn tn int [UNK] part fw [PAD] int ppm [PAD] adj sb sb part int conj fw v fw tn conj [UNK] adv num int adj n pron v ppm n adj [UNK] ppm pron v ppm abb punc sb abb tn\n",
      "Generated Text 9: n pron pron abb num num abb part ppm int part adj int punc fw tn pron adv fw abb n n ppm tn fw tn [UNK] int adv [UNK] pron num int [PAD] num adv [UNK] int sb pron [UNK] [UNK] num [UNK] punc ppm n num sb [PAD] abb\n",
      "Generated Text 10: n num adj n abb fw pron adv conj tn adv int punc adj int fw [PAD] adv v tn num punc num pron part adj punc adv punc part num v fw n sb sb punc adj adv n ppm int part v v pron ppm tn pron sb ppm\n",
      "\n",
      "real\t0m2.426s\n",
      "user\t0m5.073s\n",
      "sys\t0m2.411s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:228: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/gpt_ftfz_gen_texts.txt\n",
      "\n",
      "real\t0m2.047s\n",
      "user\t0m4.712s\n",
      "sys\t0m2.420s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:429: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 50.03it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.811s\n",
      "user\t0m4.563s\n",
      "sys\t0m2.323s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag_ftfz.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a590dcb-3d93-4d02-8218-533834a9895f",
   "metadata": {},
   "source": [
    "## GPU Usage of GPT LM Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03ba9ec7-3db2-4403-ae19-2b62cfd71ec6",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:01:32 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   54C    P2             240W / 480W |   1145MiB / 24564MiB |     52%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        121MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       54MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    163153      C   python                                      420MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Tue Jan 28 20:01:41 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "| 31%   56C    P2             243W / 480W |   1145MiB / 24564MiB |     68%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          191MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                        121MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        87MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       54MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      160MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A    163153      C   python                                      420MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff75cf-2443-4755-8a63-cdf640e38299",
   "metadata": {},
   "source": [
    "train လုပ်ပြီး ရလာတဲ့ model ဖိုင်ကိုလေ့လာကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af90e25b-26a6-4a71-a6b1-b15339c4d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 2.3M Jan 28 20:02 ./model/tag/gpt.ftfz.model\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/gpt.ftfz.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956ca24-02dd-4b8b-a0ce-2196f087ce7b",
   "metadata": {},
   "source": [
    "start_tags.txt ဖိုင်နဲ့ prompt လုပ်ပြီး generate လုပ်ရလာတဲ့ tag sequence တွေကို လေ့လာကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6c720ce-ec23-4c1b-bc24-7a7a1719ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron ppm part\n",
      "pron [PAD] abb\n",
      "pron int num\n",
      "pron num num\n",
      "pron fw fw\n",
      "n adj [PAD]\n",
      "n part adj\n",
      "n abb num\n",
      "n adv adj\n",
      "n [UNK] pron\n",
      "adj adv pron\n",
      "adj [UNK] fw\n",
      "adj tn ppm\n",
      "adj adj fw\n",
      "adj fw punc\n",
      "v fw punc\n",
      "v pron part\n",
      "v conj pron\n",
      "v [PAD] pron\n",
      "v punc adj\n",
      "pron part adj n\n",
      "pron part ppm [UNK]\n",
      "pron part adj fw\n",
      "pron part fw conj\n",
      "pron part v v\n",
      "pron ppm adv [PAD]\n",
      "pron ppm num punc\n",
      "pron ppm conj sb\n",
      "pron ppm tn adj\n",
      "pron ppm abb num\n",
      "n v abb conj\n",
      "n v adv ppm\n",
      "n v ppm [UNK]\n",
      "n v conj adj\n",
      "n v tn sb\n",
      "n n adj fw\n",
      "n n part num\n",
      "n n v fw\n",
      "n n sb part\n",
      "n n v n\n",
      "v part pron ppm\n",
      "v part conj [UNK]\n",
      "v part num abb\n",
      "v part sb n\n",
      "v part int v\n",
      "n part part adj\n",
      "n part int num\n",
      "n part [UNK] pron\n",
      "n part pron int\n",
      "n part sb abb\n",
      "pron pron v fw\n",
      "pron pron n conj\n",
      "pron pron n tn\n",
      "pron pron int part\n",
      "pron pron adj [PAD]\n",
      "pron ppm [PAD] [PAD]\n",
      "pron ppm part abb\n",
      "pron ppm num tn\n",
      "pron ppm adj [UNK]\n",
      "pron ppm sb fw\n",
      "n tn ppm punc\n",
      "n tn ppm [UNK]\n",
      "n tn adv ppm\n",
      "n tn abb part\n",
      "n tn [PAD] adj\n",
      "adj v part fw conj\n",
      "adj v part tn fw\n",
      "adj v part fw conj\n",
      "adj v part n conj\n",
      "adj v part conj [UNK]\n",
      "n n n punc part\n",
      "n n n ppm pron\n",
      "n n n adv [UNK]\n",
      "n n n num conj\n",
      "n n n abb abb\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/tag/gpt_ftfz_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799000d2-687f-4384-82cd-1114e06dc138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
