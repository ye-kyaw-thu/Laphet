{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c0062c-cc95-4aea-8ab2-5af5d6be5c76",
   "metadata": {},
   "source": [
    "# Laphet with myPOS Tags\n",
    "\n",
    "Language model ကို NLP task အမျိုးမျိုးအတွက် အသုံးပြုကြပါတယ်။ ဒီ notebook မှာတော့ Laphet LM Toolkit ကို သုံးပြီး မြန်မာစာကြောင်းတွေရဲ့ Part-of-Speech prediction/generation ကို လက်တွေ့ လုပ်ပြပါမယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a99ae-e036-4178-9527-c22a00f008d9",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "Link: [https://github.com/ye-kyaw-thu/myPOS](https://github.com/ye-kyaw-thu/myPOS)  \n",
    "\n",
    "Preprocessing အနေနဲ့ myPOS dataset ထဲက tag တွေကိုပဲ ဆွဲထုတ်ယူထားပါတယ်။  \n",
    "tag တွေ သို့မဟုတ် word တွေကို ဆွဲထုတ်ဖို့အတွက် သုံးခဲ့တဲ့ perl code က ဒီလင့်ကနေ ရယူနိုင်ပါတယ်။  \n",
    "\n",
    "[https://github.com/ye-kyaw-thu/myPOS/blob/master/corpus-draft-ver-1.0/mk-wordtag.pl](https://github.com/ye-kyaw-thu/myPOS/blob/master/corpus-draft-ver-1.0/mk-wordtag.pl)  \n",
    "\n",
    "Laphet repository မှာလည်း ကော်ပီကူးထည့်ပေးထားပါမယ်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ac6096-31c8-44f4-9359-03e5317f39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 103K Jan 26 18:03 ./data/myPOS/tag/dev_tag.txt\n",
      "-rw-rw-r-- 1 ye ye   95 Jan 24 18:42 ./data/myPOS/tag/start_tags.txt\n",
      "-rw-rw-r-- 1 ye ye  48K Jan 24 17:43 ./data/myPOS/tag/test_tag.txt\n",
      "-rw-rw-r-- 1 ye ye 1.9M Jan 26 18:02 ./data/myPOS/tag/train_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./data/myPOS/tag/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a79b8e-9208-4aac-a656-fd0ffd9f6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  40000  522381 1908736 ./data/myPOS/tag/train_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/train_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67119a87-addf-4893-a706-9a7977587db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2196  28668 104680 ./data/myPOS/tag/dev_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/dev_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def2eb0c-f680-4962-8d10-e0aa0f19b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 13468 48966 ./data/myPOS/tag/test_tag.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/test_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0e4303-dbe9-44c7-82fc-c818f663d86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 28 95 ./data/myPOS/tag/start_tags.txt\n"
     ]
    }
   ],
   "source": [
    "!wc ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22463-547e-4fd7-bb67-cbd960d6dd2c",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca670b8c-1453-40cc-a4e4-dc8de7b60a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num n v n ppm n num part v ppm punc\n",
      "n part ppm v v part v part part n v n conj punc n conj n n ppm v part part n n part v part n conj n n v ppm punc\n",
      "adj n ppm v part n part v ppm punc\n",
      "n v part ppm v v part part n n part ppm v n part ppm n ppm adv v part v part part part punc\n",
      "pron ppm pron n part punc\n",
      "num n n n n punc num part punc n n punc n n punc\n",
      "pron n tn part v part part ppm punc\n",
      "pron part ppm pron n ppm n ppm v part v part ppm punc\n",
      "n v part adj n ppm pron part part punc\n",
      "adj n ppm n punc n punc n part n part v ppm punc\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myPOS/tag/train_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad874dd-3a10-4123-a4a3-503bd9d7eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron tn part part v ppm punc\n",
      "n n n n part ppm adv v part part ppm punc\n",
      "conj n part v conj adv v part part punc\n",
      "pron pron v part n v part n v part part punc\n",
      "n v part ppm n v ppm punc\n",
      "v part ppm punc\n",
      "v v part v part punc\n",
      "pron ppm n n ppm v part part part part punc\n",
      "pron pron part ppm n tn part part ppm n v part v part punc\n",
      "pron part ppm adj n part punc\n"
     ]
    }
   ],
   "source": [
    "!tail ./data/myPOS/tag/dev_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d93d71-71b1-48cb-8212-ba61e7d0e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn n ppm n tn part punc\n",
      "n ppm pron pron ppm v part ppm punc\n",
      "pron n v v part punc\n",
      "n v part v ppm part punc\n",
      "n adv v part punc tn tn n part v part conj v part ppm part punc\n",
      "v part conj n ppm v part v ppm punc\n",
      "n ppm v ppm punc\n",
      "n v conj n v part part part punc\n",
      "pron v part ppm tn part part v part ppm punc\n",
      "n part ppm v conj n part ppm n n ppm n v ppm punc\n"
     ]
    }
   ],
   "source": [
    "!head ./data/myPOS/tag/test_tag.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3507386b-d237-4d92-826e-5f151d8ab459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4133f-0f4e-45eb-84c1-278357af1824",
   "metadata": {},
   "source": [
    "start_tags.txt ဖိုင်မှာ start word (တကယ်ကတော့ ဒီနေရာမှာ start tag ပါ) အနေနဲ့ ပေးတဲ့အခါမှာ တစ်လုံးထက်မက ပိုပေးထားတာကို ဂရုပြုပါ။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05150a-b986-40de-9215-0f0e49bbfed4",
   "metadata": {},
   "source": [
    "## Bash Shell Script\n",
    "\n",
    "Experiment အကုန်၊ ဆိုလိုတာက MLP, Bi-LSTM, Transformer, BERT, GPT မော်ဒယ်အကုန်နဲ့ experiment လုပ်မယ် ဆိုရင်တော့ အောက်ပါ shel script နဲ့ run ပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f90235c-fadc-4b1c-9b55-3196c258f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 --lr 0.0001;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"ရဲ\" --no_of_generation 10;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "task bilstm\n",
      "task transformer\n",
      "task bert\n",
      "task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a38641-d5bd-4d4f-bb5d-b17651ad0995",
   "metadata": {},
   "source": [
    "ဒီ notebook မှာ အကုန်လုံးကို တန်းစီပြီး တခါတည်း run ချလိုက်ရင် လိုက်ကြည့်ရခက်နေမှာစိုးလို့ မော်ဒယ် တစ်ခုချင်းစီကို သပ်သပ်စီ run ပြသွားပါမယ်။ အဲဒီလို လုပ်ဖို့အတွက် ဆိုရင် task line တွေကို comment အပိတ်အဖွင့်လုပ်ပြီး ကစားပေးရပါလိမ့်မယ်။  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391264e6-403d-4d4c-bf67-717f9162c783",
   "metadata": {},
   "source": [
    "## Bash Script for MLP  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3e367c-8c74-4454-b4c3-de7c544e8aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 --lr 0.0001;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"ရဲ\" --no_of_generation 10;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "task mlp\n",
      "#task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a884e-014a-4bc4-bf79-37b784bbf813",
   "metadata": {},
   "source": [
    "## MLP based LM Training, Text Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4899e390-58c3-428d-876e-a7b37fa12b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mlp language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 133.75it/s]\n",
      "Epoch 1, Training Loss: 0.6058\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 380.24it/s]\n",
      "Epoch 1, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 141.09it/s]\n",
      "Epoch 2, Training Loss: 0.5729\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 531.65it/s]\n",
      "Epoch 2, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 140.36it/s]\n",
      "Epoch 3, Training Loss: 0.5729\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 389.54it/s]\n",
      "Epoch 3, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 143.20it/s]\n",
      "Epoch 4, Training Loss: 0.5729\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 522.41it/s]\n",
      "Epoch 4, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 136.85it/s]\n",
      "Epoch 5, Training Loss: 0.5729\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 525.91it/s]\n",
      "Epoch 5, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:09<00:00, 138.81it/s]\n",
      "Epoch 6, Training Loss: 0.5729\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 518.08it/s]\n",
      "Epoch 6, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 141.18it/s]\n",
      "Epoch 7, Training Loss: 0.5729\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 524.14it/s]\n",
      "Epoch 7, Validation Loss: 0.5729\n",
      "Best model saved at ./model/tag/mlp.model with validation loss: 0.5729\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 139.93it/s]\n",
      "Epoch 8, Training Loss: 0.5729\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 516.36it/s]\n",
      "Epoch 8, Validation Loss: 0.5729\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:08<00:00, 142.86it/s]\n",
      "Epoch 9, Training Loss: 0.5729\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 505.28it/s]\n",
      "Epoch 9, Validation Loss: 0.5729\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:08<00:00, 143.49it/s]\n",
      "Epoch 10, Training Loss: 0.5729\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 507.06it/s]\n",
      "Epoch 10, Validation Loss: 0.5729\n",
      "\n",
      "real\t1m32.203s\n",
      "user\t1m36.430s\n",
      "sys\t0m0.394s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: [UNK] adj punc sb [PAD] int adv tn ppm int n v ppm pron conj fw punc [UNK] v conj fw [UNK] [PAD] n punc n punc ppm ppm num pron punc num num v punc sb [UNK] ppm v punc n abb abb abb [PAD] fw adv num n adj\n",
      "Generated Text 2: [UNK] fw conj ppm fw [UNK] adv punc adj tn int adv [UNK] punc part conj part adj int ppm sb n [PAD] adj fw adj v part int pron adv adj num n conj sb abb n ppm adv conj pron fw pron int num [UNK] punc int fw [PAD]\n",
      "Generated Text 3: [UNK] adv pron punc abb part adj n sb pron [PAD] pron v sb conj abb abb [PAD] int ppm [UNK] abb pron abb adv fw punc punc tn conj punc int abb punc fw fw punc tn abb num [PAD] sb conj [PAD] sb pron n conj n int punc\n",
      "Generated Text 4: [UNK] n punc [PAD] punc fw int adj [UNK] conj sb part part tn [PAD] v abb adj punc adj tn fw [PAD] int part part num int ppm [PAD] adj n sb n sb fw fw n num int pron adj ppm ppm adj n adv adv [PAD] adj conj\n",
      "Generated Text 5: [UNK] tn adv tn adj int tn adv punc abb fw abb punc num adv punc fw int int abb pron ppm ppm pron pron conj v v part part adj part adj int int part abb sb pron sb tn tn adj fw [PAD] fw abb num num adj ppm\n",
      "Generated Text 6: [UNK] pron conj pron pron part v v adj int [PAD] part tn [UNK] v fw part part adv int n [PAD] punc abb conj n ppm int sb [PAD] tn tn [UNK] adj ppm adv punc part num adv conj adj abb adv [PAD] part int adj [UNK] tn part\n",
      "Generated Text 7: [UNK] adj pron ppm abb punc ppm [PAD] part part fw abb [PAD] ppm [PAD] fw adj part punc tn int abb ppm part adj tn abb n sb fw int tn sb conj adj [PAD] sb v v int conj ppm fw part v num conj part conj n [UNK]\n",
      "Generated Text 8: [UNK] punc ppm ppm v conj part int punc n num v adv part fw pron pron punc adv conj [UNK] sb adv adv pron sb adj adj conj fw v conj conj fw [PAD] conj abb adv punc int num num num n tn pron ppm n ppm adv v\n",
      "Generated Text 9: [UNK] v [PAD] int fw adv num tn abb conj adj n adj adj conj adj [UNK] abb part pron [PAD] num ppm v ppm part sb n n fw conj part sb pron adj sb punc adj ppm pron adv conj n sb pron v num pron v fw ppm\n",
      "Generated Text 10: [UNK] abb punc sb int conj conj adj fw fw num sb fw ppm [UNK] pron v num abb int ppm fw adj abb fw fw v tn part conj tn punc tn n adj part sb [PAD] abb ppm int [UNK] sb adv [PAD] [PAD] conj adv ppm adj [UNK]\n",
      "\n",
      "real\t0m1.819s\n",
      "user\t0m4.137s\n",
      "sys\t0m0.217s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/mlp_gen_texts.txt\n",
      "\n",
      "real\t0m1.362s\n",
      "user\t0m3.565s\n",
      "sys\t0m0.226s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.56it/s]\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "real\t0m1.243s\n",
      "user\t0m3.583s\n",
      "sys\t0m0.181s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669b284-9730-4efe-b029-f1bad43fe83d",
   "metadata": {},
   "source": [
    "## GPU Usage for MLP-based Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c52eeb0-5f6c-4e2d-b0b0-4f4a9cb57dad",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Mon Jan 27 04:52:47 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   61C    P2             175W / 480W |   1014MiB / 24564MiB |     57%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          232MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        23MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       46MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      163MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            23MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A     51309      C   python                                      370MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b8420-4148-4811-af8f-69ebc2bb0a81",
   "metadata": {},
   "source": [
    "## Checking Model, Output and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6768e6f0-0c95-4ea9-85b8-dad484e3e2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── mlp.model\n",
      "└── mlp.model.vocab\n",
      "\n",
      "1 directory, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a916e89-8547-4111-a936-f75796dfe2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 568K Jan 27 04:53 ./model/tag/mlp.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 27 04:52 ./model/tag/mlp.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d931e682-6fdf-4948-9697-8fe279dde845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17  34 110 ./model/tag/mlp.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!wc ./model/tag/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3314b552-4802-40d6-a8c8-e92084a301e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\t0\n",
      "ppm\t1\n",
      "adv\t2\n",
      "part\t3\n",
      "conj\t4\n",
      "fw\t5\n",
      "[PAD]\t6\n",
      "tn\t7\n",
      "pron\t8\n",
      "num\t9\n",
      "adj\t10\n",
      "[UNK]\t11\n",
      "int\t12\n",
      "punc\t13\n",
      "abb\t14\n",
      "n\t15\n",
      "sb\t16\n"
     ]
    }
   ],
   "source": [
    "!cat ./model/tag/mlp.model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c06f6-ae3e-48ac-936d-dd9ccbeaf8ed",
   "metadata": {},
   "source": [
    "## Tag Generation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c8956e1-cb28-4f0f-9e66-906d2bccef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pron\n",
      "n\n",
      "adj\n",
      "v\n",
      "pron part\n",
      "pron ppm\n",
      "n v\n",
      "n n\n",
      "v part\n",
      "n part\n",
      "pron pron\n",
      "pron ppm\n",
      "n tn\n",
      "adj v part\n",
      "n n n\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/myPOS/tag/start_tags.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318459eb-f8a4-419e-8f81-1122eb871323",
   "metadata": {},
   "source": [
    "အထက်ပါ input ဖိုင်ထဲက start tag တွေကို အခြေခံပြီး MLP-based LM က generated လုပ်ပြီးထွက်လာတဲ့ output က အောက်ပါအတိုင်းပါ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea57522f-115c-4d2f-b549-fa239e4fff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron conj v\n",
      "     2\tpron conj sb\n",
      "     3\tpron int fw\n",
      "     4\tpron [UNK] abb\n",
      "     5\tpron sb sb\n",
      "     6\tn v conj\n",
      "     7\tn n punc\n",
      "     8\tn abb v\n",
      "     9\tn abb abb\n",
      "    10\tn n pron\n",
      "    11\tadj v adv\n",
      "    12\tadj sb v\n",
      "    13\tadj adv [PAD]\n",
      "    14\tadj adj adv\n",
      "    15\tadj punc abb\n",
      "    16\tv num sb\n",
      "    17\tv tn punc\n",
      "    18\tv num n\n",
      "    19\tv tn abb\n",
      "    20\tv part punc\n",
      "    21\tpron part [UNK] v\n",
      "    22\tpron part fw int\n",
      "    23\tpron part num conj\n",
      "    24\tpron part conj punc\n",
      "    25\tpron part tn punc\n",
      "    26\tpron ppm sb conj\n",
      "    27\tpron ppm part fw\n",
      "    28\tpron ppm v adv\n",
      "    29\tpron ppm conj ppm\n",
      "    30\tpron ppm fw adj\n",
      "    31\tn v tn num\n",
      "    32\tn v conj v\n",
      "    33\tn v tn abb\n",
      "    34\tn v [UNK] num\n",
      "    35\tn v adv punc\n",
      "    36\tn n v [PAD]\n",
      "    37\tn n int [UNK]\n",
      "    38\tn n [PAD] v\n",
      "    39\tn n abb tn\n",
      "    40\tn n adv fw\n",
      "    41\tv part tn pron\n",
      "    42\tv part pron v\n",
      "    43\tv part part [PAD]\n",
      "    44\tv part pron fw\n",
      "    45\tv part tn [UNK]\n",
      "    46\tn part abb fw\n",
      "    47\tn part v part\n",
      "    48\tn part [PAD] pron\n",
      "    49\tn part punc adj\n",
      "    50\tn part int [PAD]\n",
      "    51\tpron pron num ppm\n",
      "    52\tpron pron pron [PAD]\n",
      "    53\tpron pron ppm [UNK]\n",
      "    54\tpron pron adv abb\n",
      "    55\tpron pron num ppm\n",
      "    56\tpron ppm v num\n",
      "    57\tpron ppm punc [UNK]\n",
      "    58\tpron ppm sb conj\n",
      "    59\tpron ppm [PAD] fw\n",
      "    60\tpron ppm ppm [PAD]\n",
      "    61\tn tn tn conj\n",
      "    62\tn tn adj [PAD]\n",
      "    63\tn tn v tn\n",
      "    64\tn tn v n\n",
      "    65\tn tn adj adv\n",
      "    66\tadj v part adv adv\n",
      "    67\tadj v part ppm sb\n",
      "    68\tadj v part n conj\n",
      "    69\tadj v part fw tn\n",
      "    70\tadj v part ppm adj\n",
      "    71\tn n n num tn\n",
      "    72\tn n n v v\n",
      "    73\tn n n adj part\n",
      "    74\tn n n sb [PAD]\n",
      "    75\tn n n int [UNK]\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/mlp_gen_texts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d35ec2-8b86-423f-8937-3832db3a76c0",
   "metadata": {},
   "source": [
    "tag generation ကို ဥပမာ အလုံး ၃၀ ထားကြည့်ပြီး run ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022656ef-3280-44d9-9c05-3073997ef709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: pron sb num adj tn [PAD] ppm adj part sb fw part [PAD] pron fw abb punc v pron ppm abb [UNK] sb v [UNK] punc ppm tn int adv tn\n",
      "Generated Text 2: n v punc ppm abb abb tn part adv abb [UNK] [PAD] num tn conj abb num sb tn adj fw [UNK] n n n conj [UNK] ppm [UNK] adj adv\n",
      "Generated Text 3: adj adv tn [PAD] sb [UNK] num abb adv int tn conj sb [UNK] [PAD] conj pron [UNK] [PAD] num fw abb n tn sb num conj ppm pron [UNK] conj\n",
      "Generated Text 4: v num abb part tn v adj [UNK] [UNK] adj int sb adv pron adj [UNK] adv ppm n punc adv part fw punc n adv [UNK] [PAD] n fw adj\n",
      "Generated Text 5: pron part [UNK] [PAD] [PAD] n int conj num tn pron pron abb adj punc abb num v int part [UNK] ppm adj sb v punc adv adv pron num punc [PAD]\n",
      "Generated Text 6: pron ppm pron [UNK] fw pron adj fw sb num n adv adv pron [UNK] adj abb adj ppm pron sb [UNK] adj ppm abb fw [UNK] part punc ppm conj part\n",
      "Generated Text 7: n v tn sb sb pron int adv int [UNK] punc ppm pron [UNK] pron sb n punc int abb punc [PAD] tn int int pron adj abb punc fw num conj\n",
      "Generated Text 8: n n adj v adj tn [UNK] tn int [PAD] sb v int num abb [UNK] v tn num [PAD] adv num sb punc sb part punc [PAD] n num adj conj\n",
      "Generated Text 9: v part part ppm abb sb [PAD] punc sb [PAD] ppm adv int [UNK] conj pron sb fw part ppm ppm part num part num abb [UNK] ppm part abb pron int\n",
      "Generated Text 10: n part conj v num int sb adj sb conj pron punc sb n part punc v n num pron n adj punc conj sb [UNK] part fw num [PAD] pron [UNK]\n",
      "Generated Text 11: pron pron v int [PAD] tn sb v [UNK] num adj ppm tn ppm pron pron abb adv num v punc v int num adv v n pron num int v abb\n",
      "Generated Text 12: pron ppm fw [PAD] punc v v ppm fw [UNK] v punc pron ppm pron ppm int num adv tn ppm v pron adj int sb [PAD] sb ppm v num part\n",
      "Generated Text 13: n tn num abb pron punc [PAD] pron tn num conj punc adj int num abb punc ppm v tn adj sb adv abb int punc pron n adj pron abb sb\n",
      "Generated Text 14: adj v part sb punc n fw v n int [UNK] punc [PAD] ppm sb [UNK] num v [PAD] ppm [UNK] adj n v sb punc [UNK] [UNK] part int ppm abb punc\n",
      "Generated Text 15: n n n fw adj v pron v [UNK] pron part ppm sb adj ppm v num [UNK] pron int part v sb adj v pron [PAD] ppm fw n fw adj v\n",
      "\n",
      "real\t0m1.624s\n",
      "user\t0m3.909s\n",
      "sys\t0m0.218s\n"
     ]
    }
   ],
   "source": [
    "!time python laphet.py --model_type mlp --generate --model ./model/tag/mlp.model \\\n",
    "--seq_len 30 --input ./data/myPOS/tag/start_tags.txt --no_of_generation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf77edd-a74b-471f-ad68-c85466fad41b",
   "metadata": {},
   "source": [
    "Test data နဲ့ evaluation လုပ်ထားတဲ့ ရလဒ်ကို ကြည့်ကြည့်ရအောင်။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dc2010d-f756-4115-a39c-6a3017121176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/mlp.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8dd2a-114b-415d-92c3-60fb9a36118f",
   "metadata": {},
   "source": [
    "## Bi-LSTM based LM Building, Tag Generation and Testing\n",
    "\n",
    "bash shell script ကို အောက်ပါအတိုင်း updated လုပ်ခဲ့။  \n",
    "\n",
    "# Run tasks for each model type in the specified order\n",
    "#task mlp  \n",
    "task bilstm  \n",
    "#task transformer  \n",
    "#task bert  \n",
    "#task gpt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d93d40-0856-461f-8c03-c64d1dc5210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.23it/s]\n",
      "Epoch 1, Training Loss: 0.1715\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 166.28it/s]\n",
      "Epoch 1, Validation Loss: 0.0017\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0017\n",
      "Epoch 2/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.38it/s]\n",
      "Epoch 2, Training Loss: 0.0012\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 174.90it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.66it/s]\n",
      "Epoch 3, Training Loss: 0.0002\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 172.65it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 58.61it/s]\n",
      "Epoch 4, Training Loss: 0.0002\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 173.73it/s]\n",
      "Epoch 4, Validation Loss: 0.0001\n",
      "Epoch 5/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.91it/s]\n",
      "Epoch 5, Training Loss: 0.0001\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 173.28it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.42it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 176.42it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.32it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 165.72it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|████████████████| 1250/1250 [00:21<00:00, 59.29it/s]\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 168.15it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|████████████████| 1250/1250 [00:20<00:00, 59.52it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 167.00it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|███████████████| 1250/1250 [00:20<00:00, 59.66it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 169.42it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "\n",
      "real\t3m37.680s\n",
      "user\t3m41.118s\n",
      "sys\t0m0.894s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: [UNK] adj n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 2: [UNK] adj adj int conj [UNK] [UNK] int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int\n",
      "Generated Text 3: [UNK] [UNK] adj abb fw ppm v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v\n",
      "Generated Text 4: [UNK] [UNK] adj adj int conj [UNK] v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v v\n",
      "Generated Text 5: [UNK] num punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc punc\n",
      "Generated Text 6: [UNK] n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 7: [UNK] adj adj int conj [UNK] [UNK] int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int int\n",
      "Generated Text 8: [UNK] conj part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part\n",
      "Generated Text 9: [UNK] n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\n",
      "Generated Text 10: [UNK] part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part\n",
      "\n",
      "real\t0m1.644s\n",
      "user\t0m3.905s\n",
      "sys\t0m0.277s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.532s\n",
      "user\t0m3.736s\n",
      "sys\t0m0.265s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 42.21it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.494s\n",
      "user\t0m3.771s\n",
      "sys\t0m0.259s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60da73-f0d3-462f-aa99-1d2c0a11c5fb",
   "metadata": {},
   "source": [
    "## Updated the Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f65cbdb-c2d0-4e9b-852e-d3b71e67d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 10 --epochs 10 --batch_size 32 --lr 0.0001;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 5 --prompt \"ရဲ\" --no_of_generation 10;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "task bilstm\n",
      "#task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c862-f14e-450e-a06e-40f0b8b9e256",
   "metadata": {},
   "source": [
    "## Training Again for Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59dd0c06-fdbb-4e1f-be3d-a09ed60a11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bilstm language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 164.23it/s]\n",
      "Epoch 1, Training Loss: 0.2847\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 600.32it/s]\n",
      "Epoch 1, Validation Loss: 0.0010\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0010\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 171.63it/s]\n",
      "Epoch 2, Training Loss: 0.0013\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 564.93it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 172.75it/s]\n",
      "Epoch 3, Training Loss: 0.0004\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 553.96it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 172.09it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 593.10it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 171.47it/s]\n",
      "Epoch 5, Training Loss: 0.0003\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 571.57it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 169.01it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 595.06it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 170.79it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 602.25it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 171.32it/s]\n",
      "Epoch 8, Training Loss: 0.0001\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 622.54it/s]\n",
      "Epoch 8, Validation Loss: 0.0002\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:07<00:00, 170.63it/s]\n",
      "Epoch 9, Training Loss: 0.0001\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 599.96it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:07<00:00, 172.94it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 620.49it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bilstm.model with validation loss: 0.0000\n",
      "\n",
      "real\t1m17.042s\n",
      "user\t1m20.716s\n",
      "sys\t0m0.914s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated Text 1: [UNK] sb [UNK] ppm ppm ppm\n",
      "Generated Text 2: [UNK] sb [PAD] [PAD] [PAD] [PAD]\n",
      "Generated Text 3: [UNK] ppm ppm ppm ppm ppm\n",
      "Generated Text 4: [UNK] v v v v v\n",
      "Generated Text 5: [UNK] part part part part part\n",
      "Generated Text 6: [UNK] n n n n n\n",
      "Generated Text 7: [UNK] abb n n n n\n",
      "Generated Text 8: [UNK] [UNK] [PAD] [PAD] [PAD] [PAD]\n",
      "Generated Text 9: [UNK] n n n n n\n",
      "Generated Text 10: [UNK] sb adj adj adj adj\n",
      "\n",
      "real\t0m1.472s\n",
      "user\t0m3.730s\n",
      "sys\t0m0.274s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "All logits are -Inf after filtering! Fallback to raw logits.\n",
      "Generated texts saved to ./output/tag/bilstm_gen_texts.txt\n",
      "\n",
      "real\t0m1.501s\n",
      "user\t0m3.755s\n",
      "sys\t0m0.276s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 42.85it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.485s\n",
      "user\t0m3.771s\n",
      "sys\t0m0.237s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c3478-720b-47fc-8699-91289c1ccd51",
   "metadata": {},
   "source": [
    "## Updated the Bash Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8cd87db-b31d-4df6-b132-c385df49ae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Create the output and log directories if they don't exist\n",
      "mkdir -p model/tag/\n",
      "mkdir -p output/tag/\n",
      "mkdir -p log/tag/\n",
      "\n",
      "# Function to train, generate text, and test a language model\n",
      "task() {\n",
      "  local model_type=$1\n",
      "  local model_file=\"./model/tag/${model_type}.model\"\n",
      "  local output_file=\"./output/tag/${model_type}_gen_texts.txt\"\n",
      "  local log_file=\"./log/tag/${model_type}.log\"\n",
      "  local train_data=\"./data/myPOS/tag/train_tag.txt\"\n",
      "  local dev_data=\"./data/myPOS/tag/dev_tag.txt\"\n",
      "  local test_data=\"./data/myPOS/tag/test_tag.txt\"\n",
      "  local start_tag=\"./data/myPOS/tag/start_tags.txt\"\n",
      "\n",
      "  {\n",
      "    echo \"Training ${model_type^} language model:\";\n",
      "    time python -u laphet.py --model_type $model_type --train --data $train_data \\\n",
      "      --dev_file $dev_data --model $model_file --seq_len 50 --epochs 10 --batch_size 32 --lr 0.0001;\n",
      "\n",
      "    echo \"Text generation:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 50 --prompt \"ရဲ\" --no_of_generation 10;\n",
      "\n",
      "    echo \"Batch text generation from file:\";\n",
      "    time python -u laphet.py --model_type $model_type --generate --model $model_file \\\n",
      "      --seq_len 2 --input $start_tag --no_of_generation 5 --output $output_file;\n",
      "\n",
      "    echo \"Testing:\";\n",
      "    time python -u laphet.py --model_type $model_type --test --model $model_file \\\n",
      "      --test_file $test_data --seq_len 50 --batch_size 64 2>&1;\n",
      "  } | tee \"$log_file\"\n",
      "}\n",
      "\n",
      "# Run tasks for each model type in the specified order\n",
      "#task mlp\n",
      "#task bilstm\n",
      "task transformer\n",
      "#task bert\n",
      "#task gpt\n",
      "\n",
      "echo \"All tasks completed!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc392aad-9f63-4349-84ef-9be68da5ce2a",
   "metadata": {},
   "source": [
    "## Transformer based LM Training, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ff4ccd8-97ec-46da-b89c-37bf68cbac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 209.48it/s]\n",
      "Epoch 1, Training Loss: 0.0363\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 612.30it/s]\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0004\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 222.14it/s]\n",
      "Epoch 2, Training Loss: 0.0004\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 620.60it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 220.49it/s]\n",
      "Epoch 3, Training Loss: 0.0002\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 622.85it/s]\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0001\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 220.08it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 545.96it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 218.43it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 610.74it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 222.69it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 624.26it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 225.58it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 603.70it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 219.62it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 607.95it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 224.08it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 617.13it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 224.86it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 621.82it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/transformer.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m59.489s\n",
      "user\t1m3.565s\n",
      "sys\t0m0.410s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: [UNK] tn n punc conj num int [UNK] [PAD] part fw num tn conj int pron conj [UNK] adv pron [PAD] conj pron n n conj int v conj abb adj pron [UNK] ppm num v pron fw ppm abb tn num [PAD] num n [PAD] adj v fw ppm abb\n",
      "Generated Text 2: [UNK] n v fw pron part int [PAD] fw sb punc sb n int conj adj abb ppm [PAD] conj [UNK] [PAD] part fw conj tn n [UNK] [PAD] int abb pron n adv ppm fw conj pron v fw adv part num [UNK] tn fw fw [UNK] adv num adv\n",
      "Generated Text 3: [UNK] ppm n num sb ppm fw part ppm [UNK] ppm tn part fw ppm fw adj abb part adv int abb pron adj adv num int v adv adv abb part part fw sb v abb abb abb conj part int n [PAD] part abb punc conj part [PAD] adv\n",
      "Generated Text 4: [UNK] tn conj fw sb tn tn int adj sb adj n v [UNK] v tn int adj [PAD] v abb adj num [UNK] adv [PAD] adj adj ppm abb part tn fw adj pron ppm n part adv part n v num sb pron punc conj abb int part [UNK]\n",
      "Generated Text 5: [UNK] [PAD] punc int [UNK] v [PAD] pron num adv [PAD] abb v adj [UNK] adv ppm adv tn tn adv n [UNK] adv [UNK] adv adj [PAD] num [UNK] [PAD] n part conj n int pron tn tn fw pron fw ppm [PAD] [UNK] n adj ppm adj punc n\n",
      "Generated Text 6: [UNK] n pron int n adv adj punc punc punc part ppm tn sb sb abb v adj part num [PAD] pron fw part [UNK] adv sb v adj abb adj punc ppm sb punc adv tn int int adv sb int num n n pron conj int int ppm adv\n",
      "Generated Text 7: [UNK] n conj punc part tn sb [PAD] sb pron sb pron tn abb punc part [PAD] abb tn ppm sb n part fw [PAD] sb adj abb conj [PAD] abb conj conj int adj [UNK] adv [UNK] adv int v part [UNK] [PAD] tn adv n num adv n part\n",
      "Generated Text 8: [UNK] [PAD] [PAD] n punc v punc pron n fw int int sb punc [PAD] int int conj conj conj [PAD] adv adv v sb tn adv adj tn n [UNK] ppm fw adj tn n punc punc abb [PAD] pron v part sb v tn adj conj pron [PAD] [PAD]\n",
      "Generated Text 9: [UNK] [PAD] sb conj sb part n abb int tn abb punc num fw adj fw abb punc tn [UNK] adv sb [UNK] punc adj sb sb adj ppm adj n conj [UNK] adv v adv tn sb punc conj abb n adj punc part [UNK] adv fw [PAD] adj adv\n",
      "Generated Text 10: [UNK] ppm n int part [PAD] adj n part int num abb part adv sb int adj pron [UNK] tn part part adv sb conj num ppm adv part punc tn adv tn int punc part n fw punc part sb [PAD] n n [PAD] [UNK] adv v tn tn ppm\n",
      "\n",
      "real\t0m1.881s\n",
      "user\t0m4.189s\n",
      "sys\t0m0.226s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/transformer_gen_texts.txt\n",
      "\n",
      "real\t0m1.478s\n",
      "user\t0m3.538s\n",
      "sys\t0m0.233s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.31it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.258s\n",
      "user\t0m3.592s\n",
      "sys\t0m0.196s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c4920-5f10-4ddc-9f4b-616f4b133c8d",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69896d13-97cb-4329-89a5-0d78826d590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3449f69-2711-418b-a146-4649f57c33b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 27 05:34 ./model/tag/transformer.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 27 05:33 ./model/tag/transformer.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/transformer.model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b1cfaab-14be-4cbd-9ef6-cf0636fff851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./output/tag/\u001b[0m\n",
      "├── bilstm_gen_texts.txt\n",
      "├── mlp_gen_texts.txt\n",
      "└── transformer_gen_texts.txt\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./output/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df2f4cad-0083-4ed4-bcd1-de5b980cbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron part num\n",
      "     2\tpron ppm punc\n",
      "     3\tpron punc tn\n",
      "     4\tpron abb abb\n",
      "     5\tpron fw adj\n",
      "     6\tn [PAD] tn\n",
      "     7\tn adj conj\n",
      "     8\tn adj adv\n",
      "     9\tn sb part\n",
      "    10\tn sb v\n",
      "    11\tadj num n\n",
      "    12\tadj ppm adv\n",
      "    13\tadj v tn\n",
      "    14\tadj part num\n",
      "    15\tadj num part\n",
      "    16\tv n sb\n",
      "    17\tv fw sb\n",
      "    18\tv part fw\n",
      "    19\tv part [PAD]\n",
      "    20\tv conj adv\n",
      "    21\tpron part tn num\n",
      "    22\tpron part fw pron\n",
      "    23\tpron part tn adv\n",
      "    24\tpron part adv int\n",
      "    25\tpron part int pron\n",
      "    26\tpron ppm adv adj\n",
      "    27\tpron ppm v part\n",
      "    28\tpron ppm [UNK] tn\n",
      "    29\tpron ppm num [UNK]\n",
      "    30\tpron ppm fw v\n",
      "    31\tn v v pron\n",
      "    32\tn v adv v\n",
      "    33\tn v sb [UNK]\n",
      "    34\tn v adj tn\n",
      "    35\tn v num part\n",
      "    36\tn n num fw\n",
      "    37\tn n [UNK] [PAD]\n",
      "    38\tn n v int\n",
      "    39\tn n v int\n",
      "    40\tn n abb ppm\n",
      "    41\tv part n adj\n",
      "    42\tv part adj abb\n",
      "    43\tv part fw part\n",
      "    44\tv part punc sb\n",
      "    45\tv part tn n\n",
      "    46\tn part conj v\n",
      "    47\tn part v abb\n",
      "    48\tn part [UNK] tn\n",
      "    49\tn part ppm ppm\n",
      "    50\tn part tn [UNK]\n",
      "    51\tpron pron adj tn\n",
      "    52\tpron pron [UNK] n\n",
      "    53\tpron pron fw part\n",
      "    54\tpron pron fw part\n",
      "    55\tpron pron fw n\n",
      "    56\tpron ppm [UNK] tn\n",
      "    57\tpron ppm conj abb\n",
      "    58\tpron ppm sb adj\n",
      "    59\tpron ppm tn conj\n",
      "    60\tpron ppm conj pron\n",
      "    61\tn tn part fw\n",
      "    62\tn tn n ppm\n",
      "    63\tn tn pron tn\n",
      "    64\tn tn pron punc\n",
      "    65\tn tn pron ppm\n",
      "    66\tadj v part [UNK] [PAD]\n",
      "    67\tadj v part sb ppm\n",
      "    68\tadj v part conj adv\n",
      "    69\tadj v part ppm pron\n",
      "    70\tadj v part abb ppm\n",
      "    71\tn n n fw int\n",
      "    72\tn n n v v\n",
      "    73\tn n n conj fw\n",
      "    74\tn n n n int\n",
      "    75\tn n n num tn\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/transformer_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "682e7e46-c302-417c-a932-f1d7b9b338b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/transformer.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558d18e-3570-40a3-9ce8-defa115ef2db",
   "metadata": {},
   "source": [
    "## Updated Bash Shell Script\n",
    "\n",
    "#task mlp  \n",
    "#task bilstm  \n",
    "#task transformer  \n",
    "task bert  \n",
    "#task gpt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf4764-aa6f-4605-adde-3adfdbb3b4fb",
   "metadata": {},
   "source": [
    "## BERT based LM Building, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17572252-1f42-4c0f-9d2a-7fa32e5f5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:06<00:00, 206.16it/s]\n",
      "Epoch 1, Training Loss: 0.0274\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 638.96it/s]\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0004\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 223.56it/s]\n",
      "Epoch 2, Training Loss: 0.0004\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 639.68it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 228.74it/s]\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 567.21it/s]\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 223.12it/s]\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 599.35it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 223.27it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 606.46it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 221.51it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 614.11it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 222.92it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 634.06it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 221.53it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 631.14it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 217.43it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 525.29it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 219.12it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 624.73it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m59.506s\n",
      "user\t1m3.593s\n",
      "sys\t0m0.430s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: [UNK] adv v [UNK] int n [PAD] ppm adv conj ppm ppm [UNK] conj fw v punc n sb adv int int [UNK] int part n num sb [PAD] adj num n sb conj ppm ppm [PAD] pron tn [PAD] adv conj part sb pron num adv fw fw sb int\n",
      "Generated Text 2: [UNK] conj adv fw int sb n part fw [UNK] int punc sb [UNK] sb tn [PAD] fw tn sb pron [UNK] adj num fw ppm tn ppm ppm n conj tn abb pron part adj num [PAD] [UNK] int pron ppm abb v ppm pron pron conj sb part tn\n",
      "Generated Text 3: [UNK] adv conj fw punc pron [UNK] [UNK] [UNK] [UNK] int pron part int adv v [PAD] abb sb fw punc tn pron sb sb part conj sb adj part sb num [PAD] adj fw sb [UNK] abb ppm abb n [UNK] sb abb n punc punc pron part abb n\n",
      "Generated Text 4: [UNK] sb int [PAD] v adj conj num part int num n adv int fw fw tn num [PAD] num tn [UNK] adj [PAD] punc fw part conj pron v sb adv adj fw num [PAD] [UNK] sb fw num fw fw part pron conj sb tn int v adv part\n",
      "Generated Text 5: [UNK] sb conj [UNK] num fw fw fw conj conj int punc ppm int [UNK] punc abb v ppm adv tn sb ppm [UNK] [UNK] int punc tn sb [UNK] punc sb int part conj conj sb adj abb n adv fw ppm adj fw tn tn fw tn int fw\n",
      "Generated Text 6: [UNK] adv [UNK] num adj abb fw int [PAD] sb [PAD] [UNK] sb ppm int ppm part num conj num ppm pron adj fw num abb int punc sb ppm sb fw int ppm [UNK] n v num punc adv [UNK] num [PAD] n conj abb part num pron part [PAD]\n",
      "Generated Text 7: [UNK] sb abb abb tn int punc tn pron abb [PAD] [PAD] sb ppm ppm conj int conj ppm sb adj pron [UNK] int ppm num tn int num adv n fw num abb part tn int abb [UNK] sb punc [PAD] sb v num adv sb ppm pron tn num\n",
      "Generated Text 8: [UNK] punc [PAD] punc conj [PAD] tn adj punc adv v adv [PAD] tn part [UNK] num tn ppm fw punc tn [PAD] int part part adj [PAD] adv ppm [UNK] int adv n adj fw abb ppm [UNK] int abb adv [PAD] adj adv tn n tn abb adj tn\n",
      "Generated Text 9: [UNK] int [UNK] num int int v num n adj punc sb int v num tn conj pron ppm punc ppm n num sb fw adj ppm v sb [PAD] v [UNK] punc [PAD] fw punc tn [PAD] sb abb pron ppm part sb fw fw n n conj abb num\n",
      "Generated Text 10: [UNK] int abb fw ppm fw abb [PAD] sb n adj abb adv punc part conj adv adj [PAD] conj abb punc ppm pron sb v fw part abb pron pron tn abb abb num num [UNK] int conj n [UNK] sb [PAD] num [PAD] sb sb punc [PAD] v pron\n",
      "\n",
      "real\t0m1.920s\n",
      "user\t0m4.235s\n",
      "sys\t0m0.220s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/bert_gen_texts.txt\n",
      "\n",
      "real\t0m1.473s\n",
      "user\t0m3.746s\n",
      "sys\t0m0.239s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.50it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.239s\n",
      "user\t0m3.526s\n",
      "sys\t0m0.220s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c92b9-bfb4-4df9-b1a5-f580d4795cad",
   "metadata": {},
   "source": [
    "## GPU Usage of BERT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5091b5d-f51e-45b8-9847-3c6edb50e94d",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib$ nvidia-smi\n",
    "Mon Jan 27 05:46:38 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   62C    P2             338W / 480W |   1123MiB / 24564MiB |     85%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          237MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        23MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       34MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      163MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A     54424      C   python                                      486MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e2bbb-b038-472f-b55d-90a804b08ecb",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags and Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbde7245-f57b-4491-8b90-3387ffb70237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bert.model\n",
      "├── bert.model.vocab\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82c62115-f57e-454b-9655-58916c04b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 27 05:47 ./model/tag/bert.model\n",
      "-rw-rw-r-- 1 ye ye  85M Jan 27 05:30 ./model/tag/bilstm.model\n",
      "-rw-rw-r-- 1 ye ye 568K Jan 27 04:53 ./model/tag/mlp.model\n",
      "-rw-rw-r-- 1 ye ye 8.2M Jan 27 05:34 ./model/tag/transformer.model\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/*.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62c7d45a-6efe-4eb0-8001-e78f965ec7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron adj fw\n",
      "     2\tpron [UNK] punc\n",
      "     3\tpron int adj\n",
      "     4\tpron conj ppm\n",
      "     5\tpron n int\n",
      "     6\tn abb conj\n",
      "     7\tn conj adv\n",
      "     8\tn num n\n",
      "     9\tn fw adj\n",
      "    10\tn int num\n",
      "    11\tadj n abb\n",
      "    12\tadj adj [PAD]\n",
      "    13\tadj pron adv\n",
      "    14\tadj fw int\n",
      "    15\tadj adj [UNK]\n",
      "    16\tv conj int\n",
      "    17\tv fw adv\n",
      "    18\tv punc part\n",
      "    19\tv [PAD] fw\n",
      "    20\tv n tn\n",
      "    21\tpron part abb abb\n",
      "    22\tpron part adj [PAD]\n",
      "    23\tpron part adj fw\n",
      "    24\tpron part num sb\n",
      "    25\tpron part num punc\n",
      "    26\tpron ppm fw int\n",
      "    27\tpron ppm adj abb\n",
      "    28\tpron ppm tn ppm\n",
      "    29\tpron ppm abb conj\n",
      "    30\tpron ppm pron [UNK]\n",
      "    31\tn v adj part\n",
      "    32\tn v v abb\n",
      "    33\tn v tn [PAD]\n",
      "    34\tn v int int\n",
      "    35\tn v tn num\n",
      "    36\tn n [PAD] n\n",
      "    37\tn n adj [PAD]\n",
      "    38\tn n part adj\n",
      "    39\tn n int part\n",
      "    40\tn n conj num\n",
      "    41\tv part sb fw\n",
      "    42\tv part adv part\n",
      "    43\tv part ppm num\n",
      "    44\tv part adj sb\n",
      "    45\tv part [PAD] fw\n",
      "    46\tn part n pron\n",
      "    47\tn part v ppm\n",
      "    48\tn part fw adj\n",
      "    49\tn part n sb\n",
      "    50\tn part v n\n",
      "    51\tpron pron n num\n",
      "    52\tpron pron num [UNK]\n",
      "    53\tpron pron [PAD] conj\n",
      "    54\tpron pron conj fw\n",
      "    55\tpron pron [UNK] num\n",
      "    56\tpron ppm adj int\n",
      "    57\tpron ppm [PAD] [PAD]\n",
      "    58\tpron ppm adv abb\n",
      "    59\tpron ppm num abb\n",
      "    60\tpron ppm conj [PAD]\n",
      "    61\tn tn adv punc\n",
      "    62\tn tn adv pron\n",
      "    63\tn tn tn tn\n",
      "    64\tn tn int fw\n",
      "    65\tn tn [UNK] punc\n",
      "    66\tadj v part fw sb\n",
      "    67\tadj v part num n\n",
      "    68\tadj v part ppm abb\n",
      "    69\tadj v part tn [PAD]\n",
      "    70\tadj v part pron ppm\n",
      "    71\tn n n sb abb\n",
      "    72\tn n n num v\n",
      "    73\tn n n [PAD] adj\n",
      "    74\tn n n conj part\n",
      "    75\tn n n fw pron\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/bert_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f775901d-aecb-4754-bfaf-920fa13b0bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bert language model:\n",
      "Epoch 1, Training Loss: 0.0274\n",
      "Epoch 1, Validation Loss: 0.0004\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0004\n",
      "Epoch 2, Training Loss: 0.0004\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0001\n",
      "Epoch 4, Training Loss: 0.0001\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/bert.model with validation loss: 0.0000\n",
      "Text generation:\n",
      "Generated Text 1: [UNK] adv v [UNK] int n [PAD] ppm adv conj ppm ppm [UNK] conj fw v punc n sb adv int int [UNK] int part n num sb [PAD] adj num n sb conj ppm ppm [PAD] pron tn [PAD] adv conj part sb pron num adv fw fw sb int\n",
      "Generated Text 2: [UNK] conj adv fw int sb n part fw [UNK] int punc sb [UNK] sb tn [PAD] fw tn sb pron [UNK] adj num fw ppm tn ppm ppm n conj tn abb pron part adj num [PAD] [UNK] int pron ppm abb v ppm pron pron conj sb part tn\n",
      "Generated Text 3: [UNK] adv conj fw punc pron [UNK] [UNK] [UNK] [UNK] int pron part int adv v [PAD] abb sb fw punc tn pron sb sb part conj sb adj part sb num [PAD] adj fw sb [UNK] abb ppm abb n [UNK] sb abb n punc punc pron part abb n\n",
      "Generated Text 4: [UNK] sb int [PAD] v adj conj num part int num n adv int fw fw tn num [PAD] num tn [UNK] adj [PAD] punc fw part conj pron v sb adv adj fw num [PAD] [UNK] sb fw num fw fw part pron conj sb tn int v adv part\n",
      "Generated Text 5: [UNK] sb conj [UNK] num fw fw fw conj conj int punc ppm int [UNK] punc abb v ppm adv tn sb ppm [UNK] [UNK] int punc tn sb [UNK] punc sb int part conj conj sb adj abb n adv fw ppm adj fw tn tn fw tn int fw\n",
      "Generated Text 6: [UNK] adv [UNK] num adj abb fw int [PAD] sb [PAD] [UNK] sb ppm int ppm part num conj num ppm pron adj fw num abb int punc sb ppm sb fw int ppm [UNK] n v num punc adv [UNK] num [PAD] n conj abb part num pron part [PAD]\n",
      "Generated Text 7: [UNK] sb abb abb tn int punc tn pron abb [PAD] [PAD] sb ppm ppm conj int conj ppm sb adj pron [UNK] int ppm num tn int num adv n fw num abb part tn int abb [UNK] sb punc [PAD] sb v num adv sb ppm pron tn num\n",
      "Generated Text 8: [UNK] punc [PAD] punc conj [PAD] tn adj punc adv v adv [PAD] tn part [UNK] num tn ppm fw punc tn [PAD] int part part adj [PAD] adv ppm [UNK] int adv n adj fw abb ppm [UNK] int abb adv [PAD] adj adv tn n tn abb adj tn\n",
      "Generated Text 9: [UNK] int [UNK] num int int v num n adj punc sb int v num tn conj pron ppm punc ppm n num sb fw adj ppm v sb [PAD] v [UNK] punc [PAD] fw punc tn [PAD] sb abb pron ppm part sb fw fw n n conj abb num\n",
      "Generated Text 10: [UNK] int abb fw ppm fw abb [PAD] sb n adj abb adv punc part conj adv adj [PAD] conj abb punc ppm pron sb v fw part abb pron pron tn abb abb num num [UNK] int conj n [UNK] sb [PAD] num [PAD] sb sb punc [PAD] v pron\n",
      "Batch text generation from file:\n",
      "Generated texts saved to ./output/tag/bert_gen_texts.txt\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 52.50it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!cat ./log/tag/bert.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ff02c-a596-4b73-861c-23214b01e0c7",
   "metadata": {},
   "source": [
    "## Updated Bash Shell Script\n",
    "\n",
    "#task mlp  \n",
    "#task bilstm  \n",
    "#task transformer  \n",
    "#task bert  \n",
    "task gpt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493f811-e859-471f-a25d-96a0e8e31ca1",
   "metadata": {},
   "source": [
    "## GPT based LM Training, Tag Generation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24e2ece1-76cd-4940-a54f-f629fbd84ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gpt language model:\n",
      "Epoch 1/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 214.73it/s]\n",
      "Epoch 1, Training Loss: 0.0148\n",
      "Epoch 1/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 557.06it/s]\n",
      "Epoch 1, Validation Loss: 0.0002\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0002\n",
      "Epoch 2/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 231.47it/s]\n",
      "Epoch 2, Training Loss: 0.0002\n",
      "Epoch 2/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 606.25it/s]\n",
      "Epoch 2, Validation Loss: 0.0001\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0001\n",
      "Epoch 3/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 226.48it/s]\n",
      "Epoch 3, Training Loss: 0.0001\n",
      "Epoch 3/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 595.10it/s]\n",
      "Epoch 3, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 4/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 228.16it/s]\n",
      "Epoch 4, Training Loss: 0.0000\n",
      "Epoch 4/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 508.75it/s]\n",
      "Epoch 4, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 5/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 225.47it/s]\n",
      "Epoch 5, Training Loss: 0.0000\n",
      "Epoch 5/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 575.05it/s]\n",
      "Epoch 5, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 6/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 229.42it/s]\n",
      "Epoch 6, Training Loss: 0.0000\n",
      "Epoch 6/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 589.37it/s]\n",
      "Epoch 6, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 7/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 230.42it/s]\n",
      "Epoch 7, Training Loss: 0.0000\n",
      "Epoch 7/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 604.17it/s]\n",
      "Epoch 7, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 8/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 230.24it/s]\n",
      "Epoch 8, Training Loss: 0.0000\n",
      "Epoch 8/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 618.02it/s]\n",
      "Epoch 8, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 9/10 (Training): 100%|███████████████| 1250/1250 [00:05<00:00, 237.05it/s]\n",
      "Epoch 9, Training Loss: 0.0000\n",
      "Epoch 9/10 (Validation): 100%|█████████████████| 69/69 [00:00<00:00, 621.39it/s]\n",
      "Epoch 9, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "Epoch 10/10 (Training): 100%|██████████████| 1250/1250 [00:05<00:00, 232.33it/s]\n",
      "Epoch 10, Training Loss: 0.0000\n",
      "Epoch 10/10 (Validation): 100%|████████████████| 69/69 [00:00<00:00, 531.80it/s]\n",
      "Epoch 10, Validation Loss: 0.0000\n",
      "Best model saved at ./model/tag/gpt.model with validation loss: 0.0000\n",
      "\n",
      "real\t0m57.600s\n",
      "user\t1m1.761s\n",
      "sys\t0m0.403s\n",
      "Text generation:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated Text 1: [UNK] n abb punc [PAD] ppm part pron punc adv v v punc n punc adv punc tn sb num n sb punc [PAD] conj int tn num abb num sb punc pron ppm [PAD] abb tn tn pron v [PAD] num adj int conj [PAD] sb [UNK] ppm n [PAD]\n",
      "Generated Text 2: [UNK] fw adv ppm adj adv [PAD] part int num pron conj conj part [UNK] num int pron adv adj fw adv abb tn sb [PAD] [PAD] ppm [UNK] int adj conj ppm [UNK] abb sb pron num fw tn int punc n abb conj int tn tn n fw int\n",
      "Generated Text 3: [UNK] ppm [PAD] tn part int adv tn part sb tn num part [UNK] ppm punc part abb punc int [PAD] pron adj sb pron num adv [PAD] part num adv part part fw part n [PAD] part tn n adv num int n ppm num punc [PAD] punc [PAD] v\n",
      "Generated Text 4: [UNK] n adv fw abb n n int adv num part ppm pron v n adj adv fw tn ppm fw sb adv part [PAD] [UNK] adv adv n [PAD] part int [UNK] punc adv num adv adv conj ppm tn v ppm v n adj sb num n num [UNK]\n",
      "Generated Text 5: [UNK] abb abb punc punc adj pron n sb v int fw [UNK] conj [UNK] int [UNK] abb int fw n [PAD] sb pron punc int adj fw abb ppm tn tn int sb part pron fw n abb int pron adj ppm int num adj sb tn abb [UNK] v\n",
      "Generated Text 6: [UNK] pron [UNK] num fw n n [UNK] adj v abb adj punc v part conj conj int adj adj v tn tn punc punc pron conj pron ppm punc v n fw abb int [UNK] punc int conj adv abb [PAD] abb adj ppm sb int v [UNK] int tn\n",
      "Generated Text 7: [UNK] int tn ppm conj v num part abb num tn v conj [PAD] part [PAD] part fw [PAD] punc fw tn punc n punc adj adj sb punc v pron int adv abb [PAD] [PAD] sb pron punc adj abb n v adv num part tn num punc adj num\n",
      "Generated Text 8: [UNK] part adj tn n [PAD] ppm conj n part adv adj conj [PAD] punc ppm pron num [UNK] part conj abb adj n conj conj part v conj pron adv [UNK] adj int [PAD] n v adj fw tn num v conj [UNK] adj ppm sb punc [PAD] [PAD] punc\n",
      "Generated Text 9: [UNK] conj tn int v int sb num [PAD] fw sb part adj conj sb ppm conj sb [PAD] int num n adv part num punc int sb int num [PAD] num pron adj pron sb punc fw fw adv adv ppm int [UNK] ppm sb v v fw conj conj\n",
      "Generated Text 10: [UNK] adj ppm tn pron n n int int pron [UNK] abb conj n conj abb adv ppm [PAD] ppm adj punc tn sb v [PAD] abb [PAD] [UNK] adv int adj [PAD] pron adj tn [UNK] int part v fw pron [UNK] [UNK] sb pron part tn int [UNK] num\n",
      "\n",
      "real\t0m1.913s\n",
      "user\t0m4.220s\n",
      "sys\t0m0.230s\n",
      "Batch text generation from file:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Generated texts saved to ./output/tag/gpt_gen_texts.txt\n",
      "\n",
      "real\t0m1.463s\n",
      "user\t0m3.749s\n",
      "sys\t0m0.242s\n",
      "Testing:\n",
      "/home/ye/exp/name-lm/lib/laphet.py:216: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model)\n",
      "Testing: 100%|██████████| 16/16 [00:00<00:00, 50.02it/s]\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "real\t0m1.266s\n",
      "user\t0m3.581s\n",
      "sys\t0m0.197s\n",
      "All tasks completed!\n"
     ]
    }
   ],
   "source": [
    "!./train_test_tag.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc7140-f50b-4465-901b-470b5eb906c6",
   "metadata": {},
   "source": [
    "## GPU Usage of GPT LM Modeling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "688e3229-3518-4b0b-95ed-97ba2bac014a",
   "metadata": {},
   "source": [
    "(torch_py3.10) (base) ye@lst-hpc3090:~/exp/name-lm/lib/model/tag$ nvidia-smi\n",
    "Mon Jan 27 05:50:34 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090 Ti     Off | 00000000:01:00.0  On |                  Off |\n",
    "|  0%   59C    P2             344W / 480W |   1144MiB / 24564MiB |     87%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     32486      G   /usr/lib/xorg/Xorg                          245MiB |\n",
    "|    0   N/A  N/A     32759      G   /usr/bin/gnome-shell                         69MiB |\n",
    "|    0   N/A  N/A     33282      G   /usr/libexec/xdg-desktop-portal-gnome        23MiB |\n",
    "|    0   N/A  N/A     33671      G   ...56,262144 --variations-seed-version       45MiB |\n",
    "|    0   N/A  N/A     34918      G   ...irefox/5647/usr/lib/firefox/firefox      163MiB |\n",
    "|    0   N/A  N/A     36812      G   /usr/bin/gnome-control-center                20MiB |\n",
    "|    0   N/A  N/A     45813      G   /usr/bin/nautilus                            22MiB |\n",
    "|    0   N/A  N/A     45863      G   /usr/bin/gnome-text-editor                   34MiB |\n",
    "|    0   N/A  N/A     54719      C   python                                      488MiB |\n",
    "+---------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc6796-7af5-41be-b047-8e3b6d1eba0f",
   "metadata": {},
   "source": [
    "## Checking Model, Generated Tags, Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45a19d4c-a06c-4889-afa3-60df0a57fbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./model/tag/\u001b[0m\n",
      "├── bert.model\n",
      "├── bert.model.vocab\n",
      "├── bilstm.model\n",
      "├── bilstm.model.vocab\n",
      "├── gpt.model\n",
      "├── gpt.model.vocab\n",
      "├── mlp.model\n",
      "├── mlp.model.vocab\n",
      "├── transformer.model\n",
      "└── transformer.model.vocab\n",
      "\n",
      "1 directory, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./model/tag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09ea5d18-604c-4f4e-848a-7dcc62c4558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ye ye 8.2M Jan 27 05:51 ./model/tag/gpt.model\n",
      "-rw-rw-r-- 1 ye ye  110 Jan 27 05:50 ./model/tag/gpt.model.vocab\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./model/tag/gpt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "540b70bc-65e5-4c71-9cc7-5160e4281d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tpron adj [UNK]\n",
      "     2\tpron abb tn\n",
      "     3\tpron punc [PAD]\n",
      "     4\tpron [UNK] part\n",
      "     5\tpron fw int\n",
      "     6\tn adj punc\n",
      "     7\tn punc pron\n",
      "     8\tn tn part\n",
      "     9\tn adj punc\n",
      "    10\tn pron n\n",
      "    11\tadj part adv\n",
      "    12\tadj num adv\n",
      "    13\tadj int sb\n",
      "    14\tadj punc conj\n",
      "    15\tadj adj abb\n",
      "    16\tv int n\n",
      "    17\tv tn [PAD]\n",
      "    18\tv ppm adj\n",
      "    19\tv conj ppm\n",
      "    20\tv pron conj\n",
      "    21\tpron part adj adv\n",
      "    22\tpron part abb pron\n",
      "    23\tpron part sb [UNK]\n",
      "    24\tpron part sb [UNK]\n",
      "    25\tpron part tn sb\n",
      "    26\tpron ppm part sb\n",
      "    27\tpron ppm n ppm\n",
      "    28\tpron ppm conj fw\n",
      "    29\tpron ppm v v\n",
      "    30\tpron ppm [UNK] conj\n",
      "    31\tn v part tn\n",
      "    32\tn v n abb\n",
      "    33\tn v [PAD] abb\n",
      "    34\tn v [PAD] int\n",
      "    35\tn v fw n\n",
      "    36\tn n sb tn\n",
      "    37\tn n ppm part\n",
      "    38\tn n conj n\n",
      "    39\tn n part abb\n",
      "    40\tn n adv fw\n",
      "    41\tv part pron sb\n",
      "    42\tv part [UNK] sb\n",
      "    43\tv part punc part\n",
      "    44\tv part part tn\n",
      "    45\tv part tn conj\n",
      "    46\tn part [UNK] v\n",
      "    47\tn part conj ppm\n",
      "    48\tn part num num\n",
      "    49\tn part adj punc\n",
      "    50\tn part punc ppm\n",
      "    51\tpron pron adj abb\n",
      "    52\tpron pron fw tn\n",
      "    53\tpron pron adj adv\n",
      "    54\tpron pron adj tn\n",
      "    55\tpron pron pron num\n",
      "    56\tpron ppm tn part\n",
      "    57\tpron ppm fw adj\n",
      "    58\tpron ppm n adv\n",
      "    59\tpron ppm v adv\n",
      "    60\tpron ppm n int\n",
      "    61\tn tn ppm int\n",
      "    62\tn tn sb adv\n",
      "    63\tn tn pron num\n",
      "    64\tn tn fw abb\n",
      "    65\tn tn v pron\n",
      "    66\tadj v part [UNK] pron\n",
      "    67\tadj v part sb adv\n",
      "    68\tadj v part ppm adv\n",
      "    69\tadj v part fw adj\n",
      "    70\tadj v part pron n\n",
      "    71\tn n n adv abb\n",
      "    72\tn n n n fw\n",
      "    73\tn n n sb punc\n",
      "    74\tn n n adj ppm\n",
      "    75\tn n n ppm n\n"
     ]
    }
   ],
   "source": [
    "!cat -n ./output/tag/gpt_gen_texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74036318-7915-4fa4-94a8-2044b64bb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./log/tag/bert.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/bilstm.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/gpt.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n",
      "\n",
      "==> ./log/tag/mlp.log <==\n",
      "Average Perplexity on Test Data: 1.1039\n",
      "Average Cross-Entropy on Test Data: 0.0988\n",
      "\n",
      "==> ./log/tag/transformer.log <==\n",
      "Average Perplexity on Test Data: 1.0000\n",
      "Average Cross-Entropy on Test Data: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 ./log/tag/*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69fe68-f0d2-4dd8-a9eb-c17d2b9105ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
